<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#e74c3c"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="A Python implementation of Bayesian optimization from scratch, covering Gaussian process regression, acquisition functions (EI, UCB, PI), and optimization …"><meta name=author content="yuhi-sa"><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20260223_bayesian_optimization/1/><meta property="og:type" content="article"><meta property="og:title" content="Bayesian Optimization: Fundamentals and Python Implementation"><meta property="og:description" content="A Python implementation of Bayesian optimization from scratch, covering Gaussian process regression, acquisition functions (EI, UCB, PI), and optimization …"><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20260223_bayesian_optimization/1/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2026-02-23T10:00:00+09:00"><meta property="article:modified_time" content="2026-02-23T16:03:15+09:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Optimization"><meta property="article:tag" content="Bayesian Statistics"><meta property="article:tag" content="Python"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Bayesian Optimization: Fundamentals and Python Implementation"><meta name=twitter:description content="A Python implementation of Bayesian optimization from scratch, covering Gaussian process regression, acquisition functions (EI, UCB, PI), and optimization …"><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Bayesian Optimization: Fundamentals and Python Implementation | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://pagead2.googlesyndication.com crossorigin><link rel=dns-prefetch href=https://pagead2.googlesyndication.com><link rel=icon href=https://yuhi-sa.github.io/favicon.ico><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20260223_bayesian_optimization/1/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20260223_bayesian_optimization/1/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/en/posts/20260223_bayesian_optimization/1/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Bayesian Optimization: Fundamentals and Python Implementation","description":"A Python implementation of Bayesian optimization from scratch, covering Gaussian process regression, acquisition functions (EI, UCB, PI), and optimization demos.","author":{"@type":"Person","name":"yuhi-sa","url":"https:\/\/yuhi-sa.github.io\/"},"publisher":{"@type":"Organization","name":"tomato blog","logo":{"@type":"ImageObject","url":"https:\/\/yuhi-sa.github.io\/ogp.jpeg"},"url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2026-02-23T10:00:00\u002b09:00","dateModified":"2026-02-23T16:03:15\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20260223_bayesian_optimization\/1\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20260223_bayesian_optimization\/1\/","wordCount":1339,"keywords":["Machine Learning","Optimization","Bayesian Statistics","Python"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT7M"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"tomato blog","item":"https:\/\/yuhi-sa.github.io\/en\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/yuhi-sa.github.io\/en\/posts\/"},{"@type":"ListItem","position":3,"name":"Bayesian Optimization: Fundamentals and Python Implementation"}]}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.72a177faa7b12de55dcc39c4ab6b6392116718d5e2735dab0214511354ecb973.css integrity="sha256-cqF3+qexLeVdzDnEq2tjkhFnGNXic12rAhRRE1TsuXM=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.403a82b29e94ee6e7a39204b5082cdf8b4966cfb44115390da7d8867a5006acd.css integrity="sha256-QDqCsp6U7m56OSBLUILN+LSWbPtEEVOQ2n2IZ6UAas0=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>window.MathJax={tex:{inlineMath:[["\\(","\\)"]],displayMath:[["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script data-ad-client=ca-pub-9558545098866170 async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Bayesian Optimization: Fundamentals and Python Implementation</h1><p class="lead article-description" itemprop=description>A Python implementation of Bayesian optimization from scratch, covering Gaussian process regression, acquisition functions (EI, UCB, PI), and optimization demos.</p><div class=article-meta><time datetime=2026-02-23T10:00:00+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
February 23, 2026
</time><time datetime=2026-02-23T16:03:15+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 23, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
7 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/machine-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Machine Learning
</a><a href=/en/tags/optimization/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Optimization
</a><a href=/en/tags/bayesian-statistics/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Bayesian Statistics
</a><a href=/en/tags/python/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Python</a></div></header><div class=article-content itemprop=articleBody><h2 id=what-is-bayesian-optimization>What is Bayesian Optimization?</h2><p>Bayesian optimization is a method for global optimization of <strong>expensive-to-evaluate black-box functions</strong>. It targets problems of the form:</p>\[
\mathbf{x}^* = \arg\min_{\mathbf{x} \in \mathcal{X}} f(\mathbf{x}) \tag{1}
\]<p>where \(f\) has no analytical gradient and each evaluation incurs a significant cost in time or resources. Common applications include hyperparameter tuning for machine learning models, experimental design, and materials discovery.</p><p>Bayesian optimization consists of two components:</p><ol><li><strong>Surrogate model</strong>: A probabilistic model that approximates the objective function (typically a Gaussian process)</li><li><strong>Acquisition function</strong>: A criterion that determines which point to evaluate next</li></ol><p>By building a surrogate from a small set of observations and sequentially selecting points that maximize the acquisition function, Bayesian optimization explores the search space efficiently.</p><h2 id=gaussian-process-regression>Gaussian Process Regression</h2><h3 id=kernel-functions>Kernel Functions</h3><p>A Gaussian process (GP) defines a prior distribution over functions. A GP is fully specified by a mean function \(m(\mathbf{x})\) and a kernel function \(k(\mathbf{x}, \mathbf{x}')\):</p>\[
f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}')) \tag{2}
\]<p>The most widely used kernel is the RBF (Radial Basis Function) kernel, also known as the squared exponential kernel:</p>\[
k(\mathbf{x}, \mathbf{x}') = \sigma_f^2 \exp\left(-\frac{\|\mathbf{x} - \mathbf{x}'\|^2}{2l^2}\right) \tag{3}
\]<p>where \(\sigma_f^2\) is the output variance and \(l\) is the length-scale parameter. Larger values of \(l\) produce smoother functions.</p><h3 id=posterior-distribution>Posterior Distribution</h3><p>Given \(n\) observations \(\mathcal{D} = \{(\mathbf{x}_i, y_i)\}_{i=1}^n\) with observation noise \(\sigma_n^2\), the predictive distribution at a new input \(\mathbf{x}_*\) is:</p>\[
\mu(\mathbf{x}_*) = \mathbf{k}_*^T (\mathbf{K} + \sigma_n^2 \mathbf{I})^{-1} \mathbf{y} \tag{4}
\]\[
\sigma^2(\mathbf{x}_*) = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T (\mathbf{K} + \sigma_n^2 \mathbf{I})^{-1} \mathbf{k}_* \tag{5}
\]<p>where:</p><ul><li>\(\mathbf{K}\) is the \(n \times n\) kernel matrix with \(K_{ij} = k(\mathbf{x}_i, \mathbf{x}_j)\)</li><li>\(\mathbf{k}_* = [k(\mathbf{x}_1, \mathbf{x}_*), \ldots, k(\mathbf{x}_n, \mathbf{x}_*)]^T\)</li><li>\(\mathbf{y} = [y_1, \ldots, y_n]^T\)</li></ul><p>Equation (4) gives the predictive mean, and Equation (5) gives the predictive uncertainty. The variance \(\sigma^2\) shrinks in regions dense with data and grows in regions with sparse observations.</p><h2 id=acquisition-functions>Acquisition Functions</h2><p>Acquisition functions decide <strong>where to sample next</strong>. They balance exploitation (regions with low predicted mean) and exploration (regions with high uncertainty).</p><h3 id=expected-improvement-ei>Expected Improvement (EI)</h3><p>EI maximizes the expected improvement over the current best observed value \(f(\mathbf{x}^+)\):</p>\[
\text{EI}(\mathbf{x}) = \mathbb{E}[\max(f(\mathbf{x}^+) - f(\mathbf{x}), 0)] \tag{6}
\]<p>Under the GP predictive distribution, EI admits a closed-form expression:</p>\[
\text{EI}(\mathbf{x}) = (\mu(\mathbf{x}^+) - \mu(\mathbf{x}) - \xi) \Phi(Z) + \sigma(\mathbf{x}) \phi(Z) \tag{7}
\]\[
Z = \frac{\mu(\mathbf{x}^+) - \mu(\mathbf{x}) - \xi}{\sigma(\mathbf{x})} \tag{8}
\]<p>where \(\Phi\) and \(\phi\) are the CDF and PDF of the standard normal distribution, and \(\xi \geq 0\) is a parameter that encourages exploration. For minimization, \(f(\mathbf{x}^+)\) is the current minimum observation.</p><h3 id=upper-confidence-bound-ucb>Upper Confidence Bound (UCB)</h3><p>For minimization, we use the Lower Confidence Bound variant:</p>\[
\text{UCB}(\mathbf{x}) = \mu(\mathbf{x}) - \kappa \sigma(\mathbf{x}) \tag{9}
\]<p>The parameter \(\kappa > 0\) controls the exploration&ndash;exploitation trade-off. Larger \(\kappa\) favors exploration.</p><h3 id=probability-of-improvement-pi>Probability of Improvement (PI)</h3><p>PI maximizes the probability of improving upon the current best value:</p>\[
\text{PI}(\mathbf{x}) = \Phi\left(\frac{\mu(\mathbf{x}^+) - \mu(\mathbf{x}) - \xi}{\sigma(\mathbf{x})}\right) \tag{10}
\]<p>PI is simple to compute but ignores the magnitude of improvement, which can lead to premature convergence to local optima.</p><p>The figure below compares how the three acquisition functions suggest different exploration strategies for the same GP surrogate.</p><p><img src=/posts/20260223_bayesian_optimization/acquisition_functions_comparison.png alt="Acquisition function comparison (EI, UCB, PI)"></p><h2 id=python-implementation>Python Implementation</h2><h3 id=gaussian-process-class>Gaussian Process Class</h3><p>We implement GP regression from scratch using NumPy and SciPy.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>norm</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GaussianProcess</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>length_scale</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>signal_var</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span> <span class=n>noise_var</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>l</span> <span class=o>=</span> <span class=n>length_scale</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sf2</span> <span class=o>=</span> <span class=n>signal_var</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>sn2</span> <span class=o>=</span> <span class=n>noise_var</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>X_train</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>y_train</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>K_inv</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>kernel</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X1</span><span class=p>,</span> <span class=n>X2</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;RBF kernel (Eq. 3)&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>dist_sq</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>X1</span><span class=o>**</span><span class=mi>2</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span> \
</span></span><span class=line><span class=cl>                  <span class=o>-</span> <span class=mf>2.0</span> <span class=o>*</span> <span class=n>X1</span> <span class=o>@</span> <span class=n>X2</span><span class=o>.</span><span class=n>T</span> \
</span></span><span class=line><span class=cl>                  <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>X2</span><span class=o>**</span><span class=mi>2</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>sf2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=mf>0.5</span> <span class=o>*</span> <span class=n>dist_sq</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>l</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>fit</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Fit the GP to observed data&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>X_train</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>y_train</span> <span class=o>=</span> <span class=n>y</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>K</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>kernel</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>X</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>sn2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>eye</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>X</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>K_inv</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>K</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X_new</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;Predictive mean and variance at new points (Eqs. 4, 5)&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>k_star</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>kernel</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>X_train</span><span class=p>,</span> <span class=n>X_new</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mu</span> <span class=o>=</span> <span class=n>k_star</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>K_inv</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>y_train</span>
</span></span><span class=line><span class=cl>        <span class=n>var</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>sf2</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>diag</span><span class=p>(</span><span class=n>k_star</span><span class=o>.</span><span class=n>T</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>K_inv</span> <span class=o>@</span> <span class=n>k_star</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>var</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=n>var</span><span class=p>,</span> <span class=mf>1e-10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>mu</span><span class=p>,</span> <span class=n>var</span>
</span></span></code></pre></div><h3 id=acquisition-functions-1>Acquisition Functions</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>expected_improvement</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>var</span><span class=p>,</span> <span class=n>y_best</span><span class=p>,</span> <span class=n>xi</span><span class=o>=</span><span class=mf>0.01</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Expected Improvement (Eqs. 7, 8)&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>sigma</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>var</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>np</span><span class=o>.</span><span class=n>errstate</span><span class=p>(</span><span class=n>divide</span><span class=o>=</span><span class=s2>&#34;ignore&#34;</span><span class=p>,</span> <span class=n>invalid</span><span class=o>=</span><span class=s2>&#34;ignore&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>Z</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_best</span> <span class=o>-</span> <span class=n>mu</span> <span class=o>-</span> <span class=n>xi</span><span class=p>)</span> <span class=o>/</span> <span class=n>sigma</span>
</span></span><span class=line><span class=cl>        <span class=n>ei</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_best</span> <span class=o>-</span> <span class=n>mu</span> <span class=o>-</span> <span class=n>xi</span><span class=p>)</span> <span class=o>*</span> <span class=n>norm</span><span class=o>.</span><span class=n>cdf</span><span class=p>(</span><span class=n>Z</span><span class=p>)</span> <span class=o>+</span> <span class=n>sigma</span> <span class=o>*</span> <span class=n>norm</span><span class=o>.</span><span class=n>pdf</span><span class=p>(</span><span class=n>Z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>ei</span><span class=p>[</span><span class=n>sigma</span> <span class=o>&lt;</span> <span class=mf>1e-10</span><span class=p>]</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>ei</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>upper_confidence_bound</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>var</span><span class=p>,</span> <span class=n>kappa</span><span class=o>=</span><span class=mf>2.0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;UCB for minimization (LCB) (Eq. 9)&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>sigma</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>var</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>mu</span> <span class=o>-</span> <span class=n>kappa</span> <span class=o>*</span> <span class=n>sigma</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>probability_of_improvement</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>var</span><span class=p>,</span> <span class=n>y_best</span><span class=p>,</span> <span class=n>xi</span><span class=o>=</span><span class=mf>0.01</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Probability of Improvement (Eq. 10)&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>sigma</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>var</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>np</span><span class=o>.</span><span class=n>errstate</span><span class=p>(</span><span class=n>divide</span><span class=o>=</span><span class=s2>&#34;ignore&#34;</span><span class=p>,</span> <span class=n>invalid</span><span class=o>=</span><span class=s2>&#34;ignore&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>Z</span> <span class=o>=</span> <span class=p>(</span><span class=n>y_best</span> <span class=o>-</span> <span class=n>mu</span> <span class=o>-</span> <span class=n>xi</span><span class=p>)</span> <span class=o>/</span> <span class=n>sigma</span>
</span></span><span class=line><span class=cl>        <span class=n>pi</span> <span class=o>=</span> <span class=n>norm</span><span class=o>.</span><span class=n>cdf</span><span class=p>(</span><span class=n>Z</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>pi</span><span class=p>[</span><span class=n>sigma</span> <span class=o>&lt;</span> <span class=mf>1e-10</span><span class=p>]</span> <span class=o>=</span> <span class=mf>0.0</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>pi</span>
</span></span></code></pre></div><h3 id=bayesian-optimization-loop>Bayesian Optimization Loop</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>bayesian_optimization</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>bounds</span><span class=p>,</span> <span class=n>n_init</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>n_iter</span><span class=o>=</span><span class=mi>15</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                          <span class=n>acq_func</span><span class=o>=</span><span class=s2>&#34;ei&#34;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Main Bayesian optimization loop&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># Random initial samples</span>
</span></span><span class=line><span class=cl>    <span class=n>X_init</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>(</span><span class=n>bounds</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>bounds</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                               <span class=n>size</span><span class=o>=</span><span class=p>(</span><span class=n>n_init</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>y_init</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>f</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>X_init</span><span class=p>])</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>gp</span> <span class=o>=</span> <span class=n>GaussianProcess</span><span class=p>(</span><span class=n>length_scale</span><span class=o>=</span><span class=mf>0.5</span><span class=p>,</span> <span class=n>signal_var</span><span class=o>=</span><span class=mf>1.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                         <span class=n>noise_var</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>X_sample</span> <span class=o>=</span> <span class=n>X_init</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>y_sample</span> <span class=o>=</span> <span class=n>y_init</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Optimization loop</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_iter</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>gp</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X_sample</span><span class=p>,</span> <span class=n>y_sample</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Evaluate acquisition function over candidate points</span>
</span></span><span class=line><span class=cl>        <span class=n>X_cand</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>bounds</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>bounds</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=mi>500</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>mu</span><span class=p>,</span> <span class=n>var</span> <span class=o>=</span> <span class=n>gp</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_cand</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>y_best</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>y_sample</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>acq_func</span> <span class=o>==</span> <span class=s2>&#34;ei&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>acq</span> <span class=o>=</span> <span class=n>expected_improvement</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>var</span><span class=p>,</span> <span class=n>y_best</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x_next</span> <span class=o>=</span> <span class=n>X_cand</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>acq</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>acq_func</span> <span class=o>==</span> <span class=s2>&#34;ucb&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>acq</span> <span class=o>=</span> <span class=n>upper_confidence_bound</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>var</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x_next</span> <span class=o>=</span> <span class=n>X_cand</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argmin</span><span class=p>(</span><span class=n>acq</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>acq_func</span> <span class=o>==</span> <span class=s2>&#34;pi&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>acq</span> <span class=o>=</span> <span class=n>probability_of_improvement</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>var</span><span class=p>,</span> <span class=n>y_best</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x_next</span> <span class=o>=</span> <span class=n>X_cand</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>acq</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Evaluate and add the new point</span>
</span></span><span class=line><span class=cl>        <span class=n>y_next</span> <span class=o>=</span> <span class=n>f</span><span class=p>(</span><span class=n>x_next</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>X_sample</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>vstack</span><span class=p>([</span><span class=n>X_sample</span><span class=p>,</span> <span class=n>x_next</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>        <span class=n>y_sample</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>y_sample</span><span class=p>,</span> <span class=n>y_next</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>X_sample</span><span class=p>,</span> <span class=n>y_sample</span><span class=p>,</span> <span class=n>gp</span>
</span></span></code></pre></div><h2 id=1d-optimization-demo>1D Optimization Demo</h2><p>We use a non-convex test function with multiple local minima to demonstrate Bayesian optimization.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>test_function</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Test function with multiple local minima&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=mi>3</span> <span class=o>*</span> <span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span> <span class=o>*</span> <span class=mf>0.1</span> <span class=o>-</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=mi>7</span> <span class=o>*</span> <span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>bounds</span> <span class=o>=</span> <span class=p>(</span><span class=o>-</span><span class=mf>3.0</span><span class=p>,</span> <span class=mf>3.0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>X_sample</span><span class=p>,</span> <span class=n>y_sample</span><span class=p>,</span> <span class=n>gp</span> <span class=o>=</span> <span class=n>bayesian_optimization</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>test_function</span><span class=p>,</span> <span class=n>bounds</span><span class=p>,</span> <span class=n>n_init</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>n_iter</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=n>acq_func</span><span class=o>=</span><span class=s2>&#34;ei&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plot the results</span>
</span></span><span class=line><span class=cl><span class=n>X_plot</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=n>bounds</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>bounds</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=mi>300</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>y_true</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>test_function</span><span class=p>(</span><span class=n>x</span><span class=p>)</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=n>X_plot</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>mu</span><span class=p>,</span> <span class=n>var</span> <span class=o>=</span> <span class=n>gp</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X_plot</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>sigma</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>var</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># GP surrogate</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>X_plot</span><span class=p>,</span> <span class=n>y_true</span><span class=p>,</span> <span class=s2>&#34;k--&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;True function&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>X_plot</span><span class=p>,</span> <span class=n>mu</span><span class=p>,</span> <span class=s2>&#34;b-&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;GP mean&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>fill_between</span><span class=p>(</span><span class=n>X_plot</span><span class=o>.</span><span class=n>ravel</span><span class=p>(),</span> <span class=n>mu</span> <span class=o>-</span> <span class=mi>2</span><span class=o>*</span><span class=n>sigma</span><span class=p>,</span> <span class=n>mu</span> <span class=o>+</span> <span class=mi>2</span><span class=o>*</span><span class=n>sigma</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                     <span class=n>alpha</span><span class=o>=</span><span class=mf>0.2</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=s2>&#34;blue&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;95% CI&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_sample</span><span class=p>[:</span><span class=mi>3</span><span class=p>],</span> <span class=n>y_sample</span><span class=p>[:</span><span class=mi>3</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>c</span><span class=o>=</span><span class=s2>&#34;green&#34;</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>80</span><span class=p>,</span> <span class=n>zorder</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;Initial points&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X_sample</span><span class=p>[</span><span class=mi>3</span><span class=p>:],</span> <span class=n>y_sample</span><span class=p>[</span><span class=mi>3</span><span class=p>:],</span>
</span></span><span class=line><span class=cl>                <span class=n>c</span><span class=o>=</span><span class=s2>&#34;red&#34;</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>80</span><span class=p>,</span> <span class=n>zorder</span><span class=o>=</span><span class=mi>5</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;BO samples&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s2>&#34;x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s2>&#34;f(x)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s2>&#34;Bayesian Optimization with GP Surrogate&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># EI acquisition function</span>
</span></span><span class=line><span class=cl><span class=n>acq_ei</span> <span class=o>=</span> <span class=n>expected_improvement</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>var</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>min</span><span class=p>(</span><span class=n>y_sample</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>X_plot</span><span class=p>,</span> <span class=n>acq_ei</span><span class=p>,</span> <span class=s2>&#34;r-&#34;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s2>&#34;EI&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s2>&#34;x&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s2>&#34;EI(x)&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s2>&#34;Expected Improvement&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s2>&#34;bayesian_optimization_result.png&#34;</span><span class=p>,</span> <span class=n>dpi</span><span class=o>=</span><span class=mi>150</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>best_idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>argmin</span><span class=p>(</span><span class=n>y_sample</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Best x: </span><span class=si>{</span><span class=n>X_sample</span><span class=p>[</span><span class=n>best_idx</span><span class=p>]</span><span class=o>.</span><span class=n>item</span><span class=p>()</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Best f(x): </span><span class=si>{</span><span class=n>y_sample</span><span class=p>[</span><span class=n>best_idx</span><span class=p>]</span><span class=si>:</span><span class=s2>.4f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Total evaluations: </span><span class=si>{</span><span class=nb>len</span><span class=p>(</span><span class=n>y_sample</span><span class=p>)</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p><img src=/posts/20260223_bayesian_optimization/bayesian_optimization_result.png alt="Bayesian optimization result"></p><p>The GP predictive mean (blue line) converges toward the true function (dashed black line), while the confidence interval (blue band) narrows near observed data. The EI acquisition function takes large values in regions where uncertainty is high and improvement is expected, automatically balancing exploration and exploitation.</p><h2 id=comparison-with-cem-and-mppi>Comparison with CEM and MPPI</h2><p>Bayesian optimization takes a fundamentally different approach from <a href=https://yuhi-sa.github.io/en/posts/20210329_cem/1/>Cross-Entropy Method (CEM)</a> and <a href=https://yuhi-sa.github.io/en/posts/20260215_mppi/1/>MPPI</a>. The following table highlights the key differences.</p><table><thead><tr><th></th><th>Bayesian Optimization</th><th>CEM</th><th>MPPI</th></tr></thead><tbody><tr><td>Evaluation budget</td><td>Very small (tens)</td><td>Moderate (hundreds&ndash;thousands)</td><td>Moderate (hundreds&ndash;thousands)</td></tr><tr><td>Parallelism</td><td>Sequential (batch extensions exist)</td><td>High</td><td>High</td></tr><tr><td>Gradient-free</td><td>Yes</td><td>Yes</td><td>Yes</td></tr><tr><td>Surrogate model</td><td>Yes (GP)</td><td>No</td><td>No</td></tr><tr><td>Exploration strategy</td><td>Acquisition function</td><td>Elite sample selection</td><td>Exponential weighting</td></tr><tr><td>Primary use case</td><td>Hyperparameter tuning</td><td>Combinatorial optimization, planning</td><td>Real-time control</td></tr><tr><td>Scalability</td><td>Low-dimensional (~20D)</td><td>Scales to high dimensions</td><td>Scales to high dimensions</td></tr></tbody></table><ul><li><strong>Bayesian optimization</strong> excels when each evaluation is expensive. The surrogate model enables efficient search with few evaluations, though GP computation costs limit applicability to high-dimensional problems.</li><li><strong>CEM</strong> is a sample-based method that works well when many evaluations are feasible. It updates the sampling distribution through hard selection of elite samples.</li><li><strong>MPPI</strong> is also sample-based but uses soft exponential weighting to leverage information from all samples. It is well-suited for real-time control problems.</li></ul><h2 id=references>References</h2><ul><li>Rasmussen, C. E., & Williams, C. K. I. (2006). <em>Gaussian Processes for Machine Learning</em>. MIT Press.</li><li>Shahriari, B., et al. (2016). &ldquo;Taking the Human Out of the Loop: A Review of Bayesian Optimization.&rdquo; <em>Proceedings of the IEEE</em>, 104(1), 148-175.</li><li>Snoek, J., Larochelle, H., & Adams, R. P. (2012). &ldquo;Practical Bayesian Optimization of Machine Learning Algorithms.&rdquo; <em>NeurIPS 2012</em>.</li><li>Brochu, E., Cora, V. M., & de Freitas, N. (2010). &ldquo;A Tutorial on Bayesian Optimization of Expensive Cost Functions.&rdquo; <em>arXiv:1012.2599</em>.</li></ul></div><div class="ad-slot in-content my-3"><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9558545098866170></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=article-footer></footer><meta itemprop=wordCount content="1339"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20260223_bayesian_optimization/1/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20260215_mppi/1/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>MPPI (Model Predictive Path Integral): A Unified View with the Cross-Entropy Method</span>
</a><a href=/en/posts/20260223_lowpass_filter/1/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Low-Pass Filter Design and Comparison: Moving Average, Butterworth, and Chebyshev</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260215_mppi/1/>MPPI (Model Predictive Path Integral): A Unified View with the Cross-Entropy Method</a></h3><div class=card-meta><time datetime=2026-02-15>February 15, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210108_bayes/1/>Bayesian Linear Regression: From Least Squares to Bayesian Estimation</a></h3><div class=card-meta><time datetime=2021-01-08>January 8, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260224_ekf/1/>Extended Kalman Filter (EKF): Theory and Python Implementation</a></h3><div class=card-meta><time datetime=2026-02-24>February 24, 2026</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Bayesian Optimization: Fundamentals and Python Implementation</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2026 yuhi-sa. All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Bayesian Optimization: Fundamentals and Python Implementation","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20260223_bayesian_optimization\/1\/","description":"A Python implementation of Bayesian optimization from scratch, covering Gaussian process regression, acquisition functions (EI, UCB, PI), and optimization demos.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>