<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#e74c3c"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="A step-by-step guide to the Self-Attention mechanism at the core of Transformers, with mathematical derivations, NumPy scratch implementation, and PyTorch …"><meta name=keywords content="Self-Attention,Transformer,Attention,Query Key Value,Multi-Head Attention,Python,NumPy,PyTorch,deep learning,NLP"><meta name=author content="yuhi-sa"><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20260228_self_attention/1/><meta property="og:type" content="article"><meta property="og:title" content="Understanding Self-Attention from Scratch: Math and Python Implementation"><meta property="og:description" content="A step-by-step guide to the Self-Attention mechanism at the core of Transformers, with mathematical derivations, NumPy scratch implementation, and PyTorch …"><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20260228_self_attention/1/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2026-02-28T13:00:00+09:00"><meta property="article:modified_time" content="2026-02-28T19:16:58+09:00"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Python"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Understanding Self-Attention from Scratch: Math and Python Implementation"><meta name=twitter:description content="A step-by-step guide to the Self-Attention mechanism at the core of Transformers, with mathematical derivations, NumPy scratch implementation, and PyTorch …"><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Understanding Self-Attention from Scratch: Math and Python Implementation | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://pagead2.googlesyndication.com crossorigin><link rel=dns-prefetch href=https://pagead2.googlesyndication.com><link rel=icon href=https://yuhi-sa.github.io/favicon.ico><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20260228_self_attention/1/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20260228_self_attention/1/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/en/posts/20260228_self_attention/1/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding Self-Attention from Scratch: Math and Python Implementation","description":"A step-by-step guide to the Self-Attention mechanism at the core of Transformers, with mathematical derivations, NumPy scratch implementation, and PyTorch comparison.","author":{"@type":"Person","name":"yuhi-sa","url":"https:\/\/yuhi-sa.github.io\/"},"publisher":{"@type":"Organization","name":"tomato blog","logo":{"@type":"ImageObject","url":"https:\/\/yuhi-sa.github.io\/ogp.jpeg"},"url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2026-02-28T13:00:00\u002b09:00","dateModified":"2026-02-28T19:16:58\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20260228_self_attention\/1\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20260228_self_attention\/1\/","wordCount":1738,"keywords":["Deep Learning","Machine Learning","Python"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT9M"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"tomato blog","item":"https:\/\/yuhi-sa.github.io\/en\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/yuhi-sa.github.io\/en\/posts\/"},{"@type":"ListItem","position":3,"name":"Understanding Self-Attention from Scratch: Math and Python Implementation"}]}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.72a177faa7b12de55dcc39c4ab6b6392116718d5e2735dab0214511354ecb973.css integrity="sha256-cqF3+qexLeVdzDnEq2tjkhFnGNXic12rAhRRE1TsuXM=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.403a82b29e94ee6e7a39204b5082cdf8b4966cfb44115390da7d8867a5006acd.css integrity="sha256-QDqCsp6U7m56OSBLUILN+LSWbPtEEVOQ2n2IZ6UAas0=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>window.MathJax={tex:{inlineMath:[["\\(","\\)"]],displayMath:[["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script data-ad-client=ca-pub-9558545098866170 async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Understanding Self-Attention from Scratch: Math and Python Implementation</h1><p class="lead article-description" itemprop=description>A step-by-step guide to the Self-Attention mechanism at the core of Transformers, with mathematical derivations, NumPy scratch implementation, and PyTorch comparison.</p><div class=article-meta><time datetime=2026-02-28T13:00:00+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
February 28, 2026
</time><time datetime=2026-02-28T19:16:58+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 28, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
9 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/deep-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Deep Learning
</a><a href=/en/tags/machine-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Machine Learning
</a><a href=/en/tags/python/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Python</a></div></header><div class=article-content itemprop=articleBody><h2 id=introduction>Introduction</h2><p>Transformers have become the standard architecture across a wide range of fields, starting from natural language processing (NLP) and extending to computer vision and speech processing. At the core of this architecture is the <strong>Self-Attention</strong> mechanism.</p><p>Self-Attention dynamically computes the relevance between every pair of elements in an input sequence, enabling context-aware representations. Unlike conventional filters with fixed weights (such as the <a href=https://yuhi-sa.github.io/en/posts/20220206_ema/1/>exponential moving average</a>), the weights in Self-Attention depend on the input data itself.</p><p>In this article, we derive the mathematics of Scaled Dot-Product Attention and Multi-Head Attention, implement them from scratch in NumPy, and verify our implementation against PyTorch&rsquo;s <code>nn.MultiheadAttention</code>.</p><h2 id=why-attention-is-needed>Why Attention Is Needed</h2><p>RNNs (Recurrent Neural Networks) have been widely used for sequence processing, but they suffer from two fundamental issues:</p><ol><li><strong>Sequential processing</strong>: The computation at time \(t\) depends on the result at \(t-1\), making parallelization difficult</li><li><strong>Difficulty with long-range dependencies</strong>: As sequences grow longer, vanishing/exploding gradients make it hard to learn relationships between distant positions</li></ol><p>The Attention mechanism solves both problems. Each position can directly access all other positions, eliminating the need for sequential computation proportional to the sequence length. Moreover, attention weights are computed dynamically from the input, freeing the model from fixed structural constraints.</p><h2 id=scaled-dot-product-attention>Scaled Dot-Product Attention</h2><h3 id=deriving-query-key-and-value>Deriving Query, Key, and Value</h3><p>Given an input sequence \(X \in \mathbb{R}^{n \times d_{\text{model}}}\) (\(n\) is the number of tokens, \(d_{\text{model}}\) is the model dimension), we apply three linear transformations to produce Query, Key, and Value:</p>\[Q = XW_Q, \quad K = XW_K, \quad V = XW_V \tag{1}\]<p>where \(W_Q, W_K \in \mathbb{R}^{d_{\text{model}} \times d_k}\) and \(W_V \in \mathbb{R}^{d_{\text{model}} \times d_v}\) are learnable weight matrices. Intuitively, the Query represents &ldquo;what I&rsquo;m looking for,&rdquo; the Key represents &ldquo;what I have,&rdquo; and the Value represents &ldquo;the actual information.&rdquo;</p><h3 id=attention-computation>Attention Computation</h3><p>The Attention function is defined as:</p>\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \tag{2}\]<p>Let us break this computation into steps.</p><p><strong>Step 1: Computing similarities</strong></p>\[S = QK^T \in \mathbb{R}^{n \times n} \tag{3}\]<p>\(S_{ij}\) is the dot product between the Query of token \(i\) and the Key of token \(j\), representing the similarity between the two tokens.</p><p><strong>Step 2: Scaling</strong></p>\[S_{\text{scaled}} = \frac{S}{\sqrt{d_k}} \tag{4}\]<p>When \(d_k\) is large, the dot-product values grow accordingly. If \(q\) and \(k\) have zero-mean, unit-variance independent components, the dot product \(q \cdot k = \sum_{i=1}^{d_k} q_i k_i\) has variance \(d_k\). Large values push the softmax into saturated regions where gradients become extremely small. Dividing by \(\sqrt{d_k}\) normalizes the variance to 1, avoiding this issue.</p><p><strong>Step 3: Computing attention weights</strong></p>\[A = \text{softmax}(S_{\text{scaled}}) \tag{5}\]<p>The softmax ensures each row forms a probability distribution (summing to 1). \(A_{ij}\) represents how much token \(i\) attends to token \(j\).</p><p><strong>Step 4: Weighted sum</strong></p>\[\text{Output} = AV \tag{6}\]<p>The output for each token is a weighted sum of all Value vectors, with weights given by the attention matrix. This can be viewed as a soft dictionary lookup: we query with Q, match against K, and retrieve V proportionally.</p><h2 id=numpy-scratch-implementation>NumPy Scratch Implementation</h2><h3 id=scaled-dot-product-attention-1>Scaled Dot-Product Attention</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Numerically stable softmax&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>e_x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=n>axis</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>e_x</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>e_x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=n>axis</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>scaled_dot_product_attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Scaled Dot-Product Attention (Eq. 2)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters:
</span></span></span><span class=line><span class=cl><span class=s2>        Q: Query matrix (n, d_k)
</span></span></span><span class=line><span class=cl><span class=s2>        K: Key matrix   (n, d_k)
</span></span></span><span class=line><span class=cl><span class=s2>        V: Value matrix  (n, d_v)
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        output: Attention output (n, d_v)
</span></span></span><span class=line><span class=cl><span class=s2>        weights: Attention weights (n, n)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>d_k</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># Steps 1-2: Similarity computation and scaling</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=n>Q</span> <span class=o>@</span> <span class=n>K</span><span class=o>.</span><span class=n>T</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Step 3: Attention weights</span>
</span></span><span class=line><span class=cl>    <span class=n>weights</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># Step 4: Weighted sum</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>weights</span> <span class=o>@</span> <span class=n>V</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>output</span><span class=p>,</span> <span class=n>weights</span>
</span></span></code></pre></div><h3 id=demonstration-and-visualization>Demonstration and Visualization</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Input: 4 tokens, model dimension 8</span>
</span></span><span class=line><span class=cl><span class=n>n_tokens</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_v</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Random input sequence</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Weight matrices (normally learned via training)</span>
</span></span><span class=line><span class=cl><span class=n>W_Q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=n>W_K</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=n>W_V</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_v</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Compute Q, K, V (Eq. 1)</span>
</span></span><span class=line><span class=cl><span class=n>Q</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=n>W_Q</span>
</span></span><span class=line><span class=cl><span class=n>K</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=n>W_K</span>
</span></span><span class=line><span class=cl><span class=n>V</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=n>W_V</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Attention computation</span>
</span></span><span class=line><span class=cl><span class=n>output</span><span class=p>,</span> <span class=n>weights</span> <span class=o>=</span> <span class=n>scaled_dot_product_attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Input shape:&#34;</span><span class=p>,</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Output shape:&#34;</span><span class=p>,</span> <span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Attention weights:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Heatmap of attention weights</span>
</span></span><span class=line><span class=cl><span class=n>token_labels</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;Token 0&#34;</span><span class=p>,</span> <span class=s2>&#34;Token 1&#34;</span><span class=p>,</span> <span class=s2>&#34;Token 2&#34;</span><span class=p>,</span> <span class=s2>&#34;Token 3&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>im</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;Blues&#34;</span><span class=p>,</span> <span class=n>vmin</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>vmax</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xticks</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_yticks</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xticklabels</span><span class=p>(</span><span class=n>token_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_yticklabels</span><span class=p>(</span><span class=n>token_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s2>&#34;Key position&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s2>&#34;Query position&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s2>&#34;Attention Weights&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>ax</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>j</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>weights</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>ha</span><span class=o>=</span><span class=s2>&#34;center&#34;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s2>&#34;center&#34;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>11</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>im</span><span class=p>,</span> <span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=multi-head-attention>Multi-Head Attention</h2><h3 id=motivation-and-formulation>Motivation and Formulation</h3><p>A single attention head can only compute similarities in one representation space. Multi-Head Attention projects Q, K, and V into \(h\) different subspaces and computes attention independently in each, capturing diverse relationships simultaneously.</p>\[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W_O \tag{7}\]\[\text{head}_i = \text{Attention}(QW_Q^i, KW_K^i, VW_V^i) \tag{8}\]<p>The standard choice is \(d_k = d_v = d_{\text{model}} / h\) for each head. \(W_O \in \mathbb{R}^{hd_v \times d_{\text{model}}}\) is the output projection matrix. The total parameter count remains roughly the same as single-head attention, but the model can attend to different aspects of the input in different subspaces.</p><h3 id=numpy-implementation>NumPy Implementation</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Multi-Head Attention (Eqs. 7, 8)&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>d_model</span> <span class=o>%</span> <span class=n>n_heads</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span> <span class=o>=</span> <span class=n>n_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>n_heads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>rng</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>RandomState</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>scale</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Projection matrices for each head</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_Q</span> <span class=o>=</span> <span class=n>rng</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_heads</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span> <span class=o>*</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_K</span> <span class=o>=</span> <span class=n>rng</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_heads</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span> <span class=o>*</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_V</span> <span class=o>=</span> <span class=n>rng</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_heads</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span> <span class=o>*</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>        <span class=c1># Output projection matrix</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_O</span> <span class=o>=</span> <span class=n>rng</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span> <span class=o>*</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Parameters:
</span></span></span><span class=line><span class=cl><span class=s2>            X: Input sequence (n, d_model)
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            output: Multi-Head Attention output (n, d_model)
</span></span></span><span class=line><span class=cl><span class=s2>            all_weights: Attention weights per head (n_heads, n, n)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>head_outputs</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=n>all_weights</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>Q</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_Q</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>K</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_K</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>V</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_V</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>head_out</span><span class=p>,</span> <span class=n>weights</span> <span class=o>=</span> <span class=n>scaled_dot_product_attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>head_outputs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>head_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>all_weights</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Concatenate heads and apply output projection (Eq. 7)</span>
</span></span><span class=line><span class=cl>        <span class=n>concat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>(</span><span class=n>head_outputs</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>concat</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_O</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>all_weights</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=visualizing-multi-head-attention>Visualizing Multi-Head Attention</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>n_tokens</span> <span class=o>=</span> <span class=mi>6</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>16</span>
</span></span><span class=line><span class=cl><span class=n>n_heads</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mha</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span><span class=p>,</span> <span class=n>all_weights</span> <span class=o>=</span> <span class=n>mha</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Input shape:&#34;</span><span class=p>,</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Output shape:&#34;</span><span class=p>,</span> <span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Attention weights shape:&#34;</span><span class=p>,</span> <span class=n>all_weights</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Visualize attention weights for each head</span>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>h</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_heads</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>im</span> <span class=o>=</span> <span class=n>axes</span><span class=p>[</span><span class=n>h</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>all_weights</span><span class=p>[</span><span class=n>h</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;Blues&#34;</span><span class=p>,</span> <span class=n>vmin</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>vmax</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>axes</span><span class=p>[</span><span class=n>h</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Head </span><span class=si>{</span><span class=n>h</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>axes</span><span class=p>[</span><span class=n>h</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s2>&#34;Key&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>axes</span><span class=p>[</span><span class=n>h</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s2>&#34;Query&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>suptitle</span><span class=p>(</span><span class=s2>&#34;Multi-Head Attention Weights&#34;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>14</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=positional-encoding>Positional Encoding</h2><p>Self-Attention is permutation-invariant: changing the order of tokens yields the same set of outputs. To inject positional information into the sequence, we need Positional Encoding.</p><p>The original Transformer paper proposes sinusoidal positional encoding:</p>\[PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \tag{9}\]\[PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \tag{10}\]<p>Here, \(pos\) is the position in the sequence and \(i\) is the dimension index. Each dimension uses a sinusoidal wave with a different frequency, generating a unique pattern for each position.</p><h3 id=implementation-and-visualization>Implementation and Visualization</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>positional_encoding</span><span class=p>(</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Sinusoidal Positional Encoding (Eqs. 9, 10)
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters:
</span></span></span><span class=line><span class=cl><span class=s2>        max_len: Maximum sequence length
</span></span></span><span class=line><span class=cl><span class=s2>        d_model: Model dimension
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        PE: Positional encoding matrix (max_len, d_model)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>PE</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>position</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>max_len</span><span class=p>)[:,</span> <span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>div_term</span> <span class=o>=</span> <span class=mi>10000</span> <span class=o>**</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>d_model</span> <span class=o>//</span> <span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>PE</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>/</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>PE</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>/</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>PE</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Visualize positional encoding</span>
</span></span><span class=line><span class=cl><span class=n>max_len</span> <span class=o>=</span> <span class=mi>50</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl><span class=n>PE</span> <span class=o>=</span> <span class=n>positional_encoding</span><span class=p>(</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>im</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>PE</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;RdBu&#34;</span><span class=p>,</span> <span class=n>aspect</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s2>&#34;Dimension&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s2>&#34;Position&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s2>&#34;Sinusoidal Positional Encoding&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>im</span><span class=p>,</span> <span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=comparison-with-pytorch-nnmultiheadattention>Comparison with PyTorch nn.MultiheadAttention</h2><p>PyTorch provides a built-in <code>nn.MultiheadAttention</code>. Here we set identical weights in both our scratch implementation and PyTorch&rsquo;s module, and verify that the outputs match.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>n_tokens</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=n>n_heads</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>n_heads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Input data</span>
</span></span><span class=line><span class=cl><span class=n>X_np</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- Scratch implementation ---</span>
</span></span><span class=line><span class=cl><span class=n>mha_np</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_np</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>mha_np</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>X_np</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- PyTorch implementation ---</span>
</span></span><span class=line><span class=cl><span class=n>mha_pt</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Copy weights from scratch implementation into PyTorch&#39;s in_proj_weight</span>
</span></span><span class=line><span class=cl><span class=c1># PyTorch stores [W_Q; W_K; W_V] concatenated as (3*d_model, d_model)</span>
</span></span><span class=line><span class=cl><span class=n>W_Q_cat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>mha_np</span><span class=o>.</span><span class=n>W_Q</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_heads</span><span class=p>)],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl><span class=n>W_K_cat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>mha_np</span><span class=o>.</span><span class=n>W_K</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_heads</span><span class=p>)],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl><span class=n>W_V_cat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>mha_np</span><span class=o>.</span><span class=n>W_V</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_heads</span><span class=p>)],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl><span class=n>in_proj_weight</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>W_Q_cat</span><span class=p>,</span> <span class=n>W_K_cat</span><span class=p>,</span> <span class=n>W_V_cat</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>mha_pt</span><span class=o>.</span><span class=n>in_proj_weight</span><span class=o>.</span><span class=n>copy_</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>in_proj_weight</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>mha_pt</span><span class=o>.</span><span class=n>out_proj</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>copy_</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>mha_np</span><span class=o>.</span><span class=n>W_O</span><span class=o>.</span><span class=n>T</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># PyTorch expects (seq_len, batch, d_model) by default</span>
</span></span><span class=line><span class=cl><span class=n>X_pt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>X_np</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># (n_tokens, 1, d_model)</span>
</span></span><span class=line><span class=cl><span class=n>out_pt</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>mha_pt</span><span class=p>(</span><span class=n>X_pt</span><span class=p>,</span> <span class=n>X_pt</span><span class=p>,</span> <span class=n>X_pt</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_pt</span> <span class=o>=</span> <span class=n>out_pt</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Compare outputs</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Scratch output:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>out_np</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;PyTorch output:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>out_pt</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Max difference:&#34;</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>out_np</span> <span class=o>-</span> <span class=n>out_pt</span><span class=p>)))</span>
</span></span></code></pre></div><p>By setting the same weights, we can confirm that both outputs agree within numerical precision.</p><h2 id=intuitive-understanding-of-attention>Intuitive Understanding of Attention</h2><p>The essence of Self-Attention is a <strong>data-dependent weighted average</strong>.</p><p>In conventional filters (such as <a href=https://yuhi-sa.github.io/en/posts/20260225_moving_average/1/>moving averages</a> or <a href=https://yuhi-sa.github.io/en/posts/20220206_ema/1/>EMA</a>), the weights are predetermined and independent of the input. In Self-Attention, however, the weights (the attention matrix \(A\)) are computed from the input \(X\) itself.</p><table><thead><tr><th>Property</th><th>Fixed-weight filters</th><th>Self-Attention</th></tr></thead><tbody><tr><td>Weight assignment</td><td>Fixed in advance</td><td>Dynamically computed from input</td></tr><tr><td>Reference range</td><td>Local (window-dependent)</td><td>Global (entire sequence)</td></tr><tr><td>Adaptivity</td><td>None</td><td>Weights change per input</td></tr><tr><td>Complexity</td><td>\(O(n)\)</td><td>\(O(n^2)\) (quadratic in sequence length)</td></tr><tr><td>Use cases</td><td>Signal smoothing, denoising</td><td>Sequence relationship modeling</td></tr></tbody></table><p>This &ldquo;input-dependent dynamic weighting&rdquo; is the source of Attention&rsquo;s power. Each token can selectively decide which other tokens to attend to based on context, enabling the model to capture long-range dependencies and complex structures flexibly.</p><h2 id=summary>Summary</h2><ul><li><strong>Self-Attention</strong> dynamically computes the relevance between all pairs of elements in a sequence to produce context-aware representations</li><li><strong>Scaled Dot-Product Attention</strong> computes Query-Key dot products, scales by \(\sqrt{d_k}\), applies softmax to obtain attention weights, and computes a weighted sum of Values</li><li><strong>Multi-Head Attention</strong> computes attention independently in multiple subspaces, capturing diverse patterns simultaneously</li><li><strong>Positional Encoding</strong> injects position information into the permutation-invariant Self-Attention mechanism</li><li>The essence of Attention is a <strong>data-dependent weighted average</strong>, fundamentally different from fixed-weight filters</li></ul><h2 id=related-articles>Related Articles</h2><ul><li><a href=https://yuhi-sa.github.io/en/posts/20260226_sgd_adam/1/>From SGD to Adam: Evolution of Gradient-Based Optimization</a> - Covers the optimization methods used to train Transformers.</li><li><a href=https://yuhi-sa.github.io/en/posts/20220206_ema/1/>Frequency Characteristics of the EMA Filter</a> - A reference for understanding the contrast between fixed-weight filters and dynamic attention weights.</li><li><a href=https://yuhi-sa.github.io/en/posts/20260226_ensemble_learning/1/>Ensemble Learning Methods and Comparison</a> - Covers ensemble methods as an alternative machine learning approach.</li><li><a href=https://yuhi-sa.github.io/en/posts/20260226_svm/1/>SVM and Kernel Methods</a> - Deepens understanding of the similarity between kernel tricks for nonlinear transformations and Attention.</li><li><a href=https://yuhi-sa.github.io/en/posts/20260225_moving_average/1/>Types and Comparison of Moving Average Filters</a> - A reference for contrasting Attention with fixed-weight filters.</li></ul><h2 id=references>References</h2><ul><li>Vaswani, A., et al. (2017). &ldquo;Attention Is All You Need.&rdquo; <em>NeurIPS 2017</em>.</li><li>PyTorch Documentation: <code>nn.MultiheadAttention</code>. <a href=https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html>https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a></li><li>Alammar, J. (2018). &ldquo;The Illustrated Transformer.&rdquo; <a href=https://jalammar.github.io/illustrated-transformer/>https://jalammar.github.io/illustrated-transformer/</a></li></ul></div><div class="ad-slot in-content my-3"><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9558545098866170></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=article-footer></footer><meta itemprop=wordCount content="1738"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20260228_self_attention/1/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20260228_notch_filter/1/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Notch Filter Design and Python Implementation: Removing Power Line Noise</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260228_timeseries_anomaly/1/>Time-Series Anomaly Detection: From Statistical Methods to Kalman Filter in Python</a></h3><div class=card-meta><time datetime=2026-02-28>February 28, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260226_svm/1/>Support Vector Machines (SVM): Kernel Methods and Nonlinear Classification with Python</a></h3><div class=card-meta><time datetime=2026-02-26>February 26, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260226_sgd_adam/1/>From SGD to Adam: Evolution of Gradient-Based Optimization</a></h3><div class=card-meta><time datetime=2026-02-26>February 26, 2026</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Understanding Self-Attention from Scratch: Math and Python Implementation</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2026 yuhi-sa. All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Understanding Self-Attention from Scratch: Math and Python Implementation","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20260228_self_attention\/1\/","description":"A step-by-step guide to the Self-Attention mechanism at the core of Transformers, with mathematical derivations, NumPy scratch implementation, and PyTorch comparison.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>