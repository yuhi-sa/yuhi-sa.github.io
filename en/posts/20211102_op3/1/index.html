<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><meta name=robots content="index,follow"><meta property="og:title" content="Code description of ROS package to make op3 acquire walking using reinforcement learning | tomato blog"><meta property="og:description" content="勉強したことなどをメモしています"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20211102_op3/1/"><meta name=twitter:card content="summary_large_image"><title>Code description of ROS package to make op3 acquire walking using reinforcement learning | tomato blog</title>
<link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet><link rel=stylesheet href=/css/main.min.cbcf0adf3096d54322591302ca01248346902aa2474afcbd71c3a1b999e09ad9.css integrity="sha256-y88K3zCW1UMiWRMCygEkg0aQKqJHSvy9ccOhuZngmtk=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.aa0332253f313dc48905008b4ab314155e5a13302588d25bcf4949f7c1abdde0.css integrity="sha256-qgMyJT8xPcSJBQCLSrMUFV5aEzAliNJbz0lJ98Gr3eA=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script></head><body><header><nav class="navbar navbar-expand-lg navbar-light bg-light"><div class=navbar-brand style=padding-left:10px>tomato blog</div><button class="navbar-toggler ml-auto" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ml-auto"><li class=nav-item><a zgotmplz href=https://yuhi-sa.github.io/ class=nav-link>Blog</a></li><li class=nav-item><a zgotmplz href=https://yuhi-sa.github.io/tags class=nav-link>Tags</a></li><li class=nav-item><a zgotmplz href=https://yuhi-sa.github.io/posts/about class=nav-link>About</a></li><li class=nav-item><a zgotmplz href=https://yuhi-sa.github.io/posts/privacy_policy class=nav-link>Privacy policy</a></li></ul></div></nav><script async src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LN6QP6VVM3")</script><script data-ad-client=ca-pub-9558545098866170 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main><div style="max-width:80%;margin:0 auto"><h1 id=code-description-of-ros-package-to-make-op3-acquire-walking-using-reinforcement-learning>Code description of ROS package to make op3 acquire walking using reinforcement learning</h1><h2 id=introduction>Introduction</h2><p>ROS package for ROBOTIS OP3 to acquire walking using reinforcement learning</p><ul><li><a href=https://github.com/yuhi-sa/op3_walk>op3_walk</a></li></ul><h2 id=result-video>Result Video</h2><ul><li><a href=https://github.com/yuhi-sa/op3_walk/blob/main/docs/op3_controller_demo.mp4>op3_controller_demo</a></li></ul><h2 id=methods>Methods</h2><p>This package uses deep reinforcement learning (DQN).<br>The action value function is defined as a three-layer neural network (NN), and the Q-value is updated as follows</p><p>$Q(s_t,a_t) = Q(s_t,a_t) + \eta(R_{t+1)} + \gamma \max_a Q(s_{t+1},a) - Q(s_t,a_t)$</p><p>The neural network is updated by back propagation using the loss function L.</p><p>$ L = \mathbb{E}(R_{t+1} + \gamma \max Q(s_{t+1},a_t)- Q(s_t,a_t))$</p><h2 id=program>Program</h2><h3 id=functionpyhttpsgithubcomyuhi-saop3_walkblobmainscriptsfunctionpy-and-motionpyhttpsgithubcomyuhi-saop3_walkblobmainscriptsmotionpy><a href=https://github.com/yuhi-sa/op3_walk/blob/main/scripts/function.py>function.py</a> and <a href=https://github.com/yuhi-sa/op3_walk/blob/main/scripts/motion.py>motion.py</a></h3><p>[function] contains the definition of the agent.<br>The Agent class has a Brain class that defines the neural network.<br>With the actions and states stored in the ReplayMemory class, Brain calculates and updates the loss.<br>The actions are discretized, and epsilon-greedy selection is made among the actions defined in [motion].</p><p>I used the code from this book as a reference.</p><ul><li><a href=https://github.com/YutaroOgawa/Deep-Reinforcement-Learning-Book>Deep-Reinforcement-Learning-Book</a></li></ul><h3 id=learningpyhttpsgithubcomyuhi-saop3_walkblobmainscriptslearningpy><a href=https://github.com/yuhi-sa/op3_walk/blob/main/scripts/learning.py>learning.py</a></h3><p>Input the state subscribed from [controller] to the Agent, calculate the action, and publish it.<br>This one uses pytorch to define the neural network, so it needs to be run in python3.</p><h3 id=controllerpyhttpsgithubcomyuhi-saop3_walkblobmainscriptscontrollerpy><a href=https://github.com/yuhi-sa/op3_walk/blob/main/scripts/controller.py>controller.py</a></h3><p>[controller]. It subscribes to actions published by [learning] and actually runs op3.
Then publish the state.<br>Due to the op3 package, you will need to run this in python2.</p><h2 id=learning-curve>Learning curve</h2><p>Learning curve for walking distance</p><p><img style="max-width:50%;height:auto;display:block;margin:0 auto" src="https://github.com/yuhi-sa/op3_walk/blob/main/docs/learning.png?raw=true" alt=歩行距離></p></div><div><div>Tags:</div><ul><li><a href=/en/tags/others/>Others</a></li></ul></div><nav aria-label=breadcrumb><ol class=breadcrumb><li class=breadcrumb-item><a href=/en/>Home</a></li><li class="breadcrumb-item active" aria-current=page>Code description of ROS package to make op3 acquire walking using reinforcement learning</li></ol></nav></main><footer><p style=text-align:center>Copyright 2025. All rights reserved.</p></footer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet><link rel=stylesheet href=/css/main.min.cbcf0adf3096d54322591302ca01248346902aa2474afcbd71c3a1b999e09ad9.css integrity="sha256-y88K3zCW1UMiWRMCygEkg0aQKqJHSvy9ccOhuZngmtk=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.aa0332253f313dc48905008b4ab314155e5a13302588d25bcf4949f7c1abdde0.css integrity="sha256-qgMyJT8xPcSJBQCLSrMUFV5aEzAliNJbz0lJ98Gr3eA=" crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js></script><script src=https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script></body></html>