<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no"><meta name=robots content="
    index
  
  ,follow"><meta name=description content="A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples."><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20210108_bayes/1/><meta property="og:type" content="
    article
  "><meta property="og:title" content="
    Bayesian Linear Regression: From Least Squares to Bayesian Estimation
  "><meta property="og:description" content="A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples."><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20210108_bayes/1/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-01-08T15:17:23+09:00"><meta property="article:modified_time" content="2026-02-14T00:31:41+09:00"><meta property="article:tag" content="Bayesian Statistics"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Python"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="
    Bayesian Linear Regression: From Least Squares to Bayesian Estimation
  "><meta name=twitter:description content="A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples."><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Bayesian Linear Regression: From Least Squares to Bayesian Estimation |
tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20210108_bayes/1/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20210108_bayes/1/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/en/posts/20210108_bayes/1/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Bayesian Linear Regression: From Least Squares to Bayesian Estimation","description":"A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples.","author":{"@type":"Person","name":""},"publisher":{"@type":"Organization","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-01-08T15:17:23\u002b09:00","dateModified":"2026-02-14T00:31:41\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210108_bayes\/1\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210108_bayes\/1\/","wordCount":857,"keywords":["Bayesian Statistics","Machine Learning","Python"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT5M"}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.de039aff98fb649abaf5fe23d3d4981f460573d7728709826e219c496c8e5be2.css integrity="sha256-3gOa/5j7ZJq69f4j09SYH0YFc9dyhwmCbiGcSWyOW+I=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c50b1500452fdb66f687cdcbf45a7774123c0111e1054e26db51fee604557b28.css integrity="sha256-xQsVAEUv22b2h83L9Fp3dBI8ARHhBU4m21H+5gRVeyg=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script async src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LN6QP6VVM3")</script><script data-ad-client=ca-pub-9558545098866170 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Bayesian Linear Regression: From Least Squares to Bayesian Estimation</h1><p class="lead article-description" itemprop=description>A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples.</p><div class=article-meta><time datetime=2021-01-08T15:17:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
January 8, 2021
</time><time datetime=2026-02-14T00:31:41+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 14, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
5 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/bayesian-statistics/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Bayesian Statistics
</a><a href=/en/tags/machine-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Machine Learning
</a><a href=/en/tags/python/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Python</a></div></header><div class=article-content itemprop=articleBody><h2 id=introduction>Introduction</h2><p>Regression is the task of learning the relationship between input data and corresponding output data, and predicting the output for unseen inputs. This article uses <strong>linear regression</strong> as an example to compare four representative parameter estimation methods: <strong>least squares</strong>, <strong>maximum likelihood estimation</strong>, <strong>MAP estimation</strong>, and <strong>Bayesian estimation</strong>, and explains their differences and characteristics.</p><p>As the model, we consider a linear combination of <strong>basis functions</strong> $\phi(x)$, which are nonlinear functions of the input $x$. With $w$ as the model parameters (weights) and $\epsilon$ as the error, the model is:</p><p>$$ y = \Phi w + \epsilon $$</p><p>Here, $\Phi$ is the design matrix, where each row contains the basis function vector for a data point.</p><hr><h2 id=1-least-squares-estimation>1. Least Squares Estimation</h2><p>Least squares finds the parameters $\hat{w}$ that minimize the <strong>sum of squared errors</strong> $S(w)$ between the model predictions and the actual target values.</p><p>$$ S(w) = (y - \Phi w)^T (y - \Phi w) $$</p><p>Setting the derivative of $S(w)$ with respect to $w$ to zero yields:</p><p>$$ \hat{w}_{LS} = (\Phi^T \Phi)^{-1} \Phi^T y $$</p><p>This has an analytical solution, making computation very fast. However, it is prone to <strong>overfitting</strong> when the training data is limited or the model has high degrees of freedom.</p><h2 id=2-maximum-likelihood-estimation-mle>2. Maximum Likelihood Estimation (MLE)</h2><p>MLE finds the parameters $\hat{w}$ that maximize the probability of observing the data (<strong>likelihood</strong>).</p><p>Assuming the error term $\epsilon$ follows a Gaussian distribution with mean $0$ and variance $\sigma^2$, the conditional probability of the target $y$ becomes a Gaussian with mean $\Phi w$ and variance $\sigma^2$.</p><p>$$ p(y | w, \sigma^2) = \mathcal{N}(y | \Phi w, \sigma^2 I) = \frac{1}{(2\pi\sigma^2)^{N/2}} \exp\lbrace-\frac{1}{2\sigma^2}(y-\Phi w)^T(y-\Phi w)\rbrace $$</p><p>We maximize the <strong>log-likelihood</strong>:</p><p>$$ \ln p(y|w) = -\frac{N}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}(y-\Phi w)^T(y-\Phi w) $$</p><p>Maximizing the log-likelihood is equivalent to minimizing the squared error term on the right. Therefore, <strong>under a Gaussian error assumption, MLE yields the same solution as least squares.</strong></p><p>$$ \hat{w}_{ML} = (\Phi^T\Phi)^{-1}\Phi^{T}y $$</p><h2 id=3-map-estimation-maximum-a-posteriori>3. MAP Estimation (Maximum A Posteriori)</h2><p>MAP estimation is a general framework for suppressing overfitting. It treats the parameters $w$ as random variables and introduces a <strong>prior distribution</strong> $p(w)$. Using Bayes&rsquo; theorem, it finds the $\hat{w}$ that maximizes the <strong>posterior distribution</strong> $p(w|y)$.</p><p>$$ p(w|y) = \frac{p(y|w)p(w)}{p(y)} \propto p(y|w)p(w) $$</p><p>Assuming a Gaussian prior with mean $0$ and covariance $\alpha^{-1}I$ for $w$, this encodes the prior belief that the weight values should be close to zero, acting as <strong>regularization</strong> that penalizes large weights.</p><p>$$ p(w|\alpha) = \mathcal{N}(w|0, \alpha^{-1}I) $$</p><p>The MAP estimate $\hat{w}_{MAP}$ is:</p><p>$$ \hat{w}_{MAP} = (\Phi^T\Phi + \frac{\beta}{\alpha}I)^{-1}\Phi^{T}y $$</p><p>where $\beta = 1/\sigma^2$. This has the same form as <strong>ridge regression (L2-regularized least squares)</strong>, and the regularization term $\frac{\beta}{\alpha}I$ ensures stable solutions even when $\Phi^T\Phi$ is not invertible, while suppressing overfitting.</p><h2 id=4-bayesian-estimation>4. Bayesian Estimation</h2><p>Least squares, MLE, and MAP estimation all find a single optimal value for $w$ (point estimation). However, this approach cannot express <strong>uncertainty</strong> about the parameters.</p><p>Bayesian estimation does not seek a single point estimate but instead computes the <strong>full posterior distribution $p(w|y)$</strong>, representing the probability distribution over all possible values of $w$ given the observed data.</p><p>With the Gaussian prior and likelihood used in MAP, the posterior is also Gaussian (conjugacy):</p><p>$$ p(w|y) = \mathcal{N}(w | \mu_N, \Sigma_N) $$</p><p>where the posterior mean $\mu_N$ and covariance $\Sigma_N$ are:</p><p>$$ \mu_N = (\Phi^T\Phi + \frac{\beta}{\alpha}I)^{-1}\Phi^{T}y $$
$$ \Sigma_N = (\beta\Phi^T\Phi + \alpha I)^{-1} $$</p><p>For prediction on a new input $x_*$, we integrate over all possible $w$, weighted by the posterior probability. This is called the <strong>predictive distribution</strong>.</p><p>$$ p(y** | x*_, y) = \int p(y__ | x_*, w) p(w|y) dw $$</p><p>The predictive distribution provides not only the mean prediction but also the <strong>variance (confidence interval)</strong> indicating prediction uncertainty.</p><h2 id=experimental-results>Experimental Results</h2><ul><li><strong>Data</strong>: 15 training points generated from $y = \sin(2\pi x)$ with added noise</li><li><strong>Basis functions</strong>: 9th-degree polynomial ($f_j(x) = x^j, j=0, &mldr;, 9$)</li><li><strong>Hyperparameters</strong>: $\alpha=1.0, \beta=10.0$</li></ul><h4 id=least-squares--maximum-likelihood-estimation>Least Squares / Maximum Likelihood Estimation</h4><p>The model tries to fit the training data closely, causing large oscillations in regions without data &ndash; a clear case of overfitting.
<img src=.././fig1.png alt=1.png></p><h4 id=map-estimation>MAP Estimation</h4><p>The prior distribution (regularization) suppresses overfitting, yielding a smoother prediction curve.
<img src=.././fig2.png alt=3.png></p><h4 id=bayesian-estimation>Bayesian Estimation</h4><p>In addition to a smooth prediction (mean, solid line) similar to MAP, the prediction uncertainty increases in regions with less training data, as shown by the widening confidence interval (blue shaded area).
<img src=.././fig3.png alt=4.png></p><h4 id=model-evaluation-coefficient-of-determination-r2>Model Evaluation (Coefficient of Determination $R^2$)</h4><p>The coefficient of determination indicates how well the model fits the data (closer to 1 is better). MAP and Bayesian estimation (mean) achieve higher scores than least squares.
<img src=.././fig4.png alt=fig8.png></p><h4 id=effect-of-hyperparameter-beta>Effect of Hyperparameter $\beta$</h4><p>In Bayesian estimation, increasing the likelihood precision parameter $\beta$ (inverse noise variance) causes the model to fit training data more tightly. When $\beta$ is too large, the confidence interval narrows and the result approaches overfitting.</p><ul><li>$\beta=50$
<img src=.././fig5.png alt=50.png></li><li>$\beta=100$
<img src=.././fig6.png alt=100.png></li><li>$\beta=1000$
<img src=.././fig7.png alt=1000.png></li></ul><h2 id=summary>Summary</h2><ul><li><strong>Least Squares / MLE</strong>: Simple and fast, but prone to overfitting.</li><li><strong>MAP Estimation</strong>: Introduces a prior (regularization) to suppress overfitting.</li><li><strong>Bayesian Estimation</strong>: Accounts for parameter uncertainty and provides confidence intervals for predictions. It gives richer information, such as increasing uncertainty in data-sparse regions.</li></ul><h2 id=references>References</h2><ul><li>C.M. Bishop, &ldquo;Pattern Recognition and Machine Learning,&rdquo; Springer (2006)</li><li>Atsushi Suyama, &ldquo;Introduction to Machine Learning with Bayesian Inference,&rdquo; Kodansha (2017)</li></ul></div><footer class=article-footer></footer><meta itemprop=wordCount content="857"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20210108_bayes/1/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20210107_prml_ent/1/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Fundamentals of Information Theory: From Entropy to Mutual Information (PRML 1.6)</span>
</a><a href=/en/posts/20210109_nnga/1/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Training Neural Networks with Genetic Algorithms in Python</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210107_prml_ent/1/>Fundamentals of Information Theory: From Entropy to Mutual Information (PRML 1.6)</a></h3><div class=card-meta><time datetime=2021-01-07>January 7, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20201223_elgamal/1/>Elliptic Curve ElGamal Encryption: Principles and Simplified Python Implementation</a></h3><div class=card-meta><time datetime=2020-12-23>December 23, 2020</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20201223_slack_api/1/>How to Send Experiment Results (Text and Images) to Slack with Python</a></h3><div class=card-meta><time datetime=2020-12-23>December 23, 2020</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Bayesian Linear Regression: From Least Squares to Bayesian Estimation</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy;
2026
tomato blog.
All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Bayesian Linear Regression: From Least Squares to Bayesian Estimation","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210108_bayes\/1\/","description":"A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>