<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#e74c3c"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples."><meta name=author content="yuhi-sa"><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20210108_bayes/1/><meta property="og:type" content="article"><meta property="og:title" content="Bayesian Linear Regression: From Least Squares to Bayesian Estimation"><meta property="og:description" content="A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples."><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20210108_bayes/1/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-01-08T15:17:23+09:00"><meta property="article:modified_time" content="2026-02-14T10:00:21+09:00"><meta property="article:tag" content="Bayesian Statistics"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Python"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Bayesian Linear Regression: From Least Squares to Bayesian Estimation"><meta name=twitter:description content="A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples."><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Bayesian Linear Regression: From Least Squares to Bayesian Estimation | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://pagead2.googlesyndication.com crossorigin><link rel=dns-prefetch href=https://pagead2.googlesyndication.com><link rel=icon href=https://yuhi-sa.github.io/favicon.ico><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20210108_bayes/1/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20210108_bayes/1/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/en/posts/20210108_bayes/1/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Bayesian Linear Regression: From Least Squares to Bayesian Estimation","description":"A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples.","author":{"@type":"Person","name":"yuhi-sa","url":"https:\/\/yuhi-sa.github.io\/"},"publisher":{"@type":"Organization","name":"tomato blog","logo":{"@type":"ImageObject","url":"https:\/\/yuhi-sa.github.io\/ogp.jpeg"},"url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-01-08T15:17:23\u002b09:00","dateModified":"2026-02-14T10:00:21\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210108_bayes\/1\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210108_bayes\/1\/","wordCount":845,"keywords":["Bayesian Statistics","Machine Learning","Python"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT4M"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"tomato blog","item":"https:\/\/yuhi-sa.github.io\/en\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/yuhi-sa.github.io\/en\/posts\/"},{"@type":"ListItem","position":3,"name":"Bayesian Linear Regression: From Least Squares to Bayesian Estimation"}]}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.72a177faa7b12de55dcc39c4ab6b6392116718d5e2735dab0214511354ecb973.css integrity="sha256-cqF3+qexLeVdzDnEq2tjkhFnGNXic12rAhRRE1TsuXM=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.403a82b29e94ee6e7a39204b5082cdf8b4966cfb44115390da7d8867a5006acd.css integrity="sha256-QDqCsp6U7m56OSBLUILN+LSWbPtEEVOQ2n2IZ6UAas0=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>window.MathJax={tex:{inlineMath:[["\\(","\\)"]],displayMath:[["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script data-ad-client=ca-pub-9558545098866170 async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Bayesian Linear Regression: From Least Squares to Bayesian Estimation</h1><p class="lead article-description" itemprop=description>A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples.</p><div class=article-meta><time datetime=2021-01-08T15:17:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
January 8, 2021
</time><time datetime=2026-02-14T10:00:21+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 14, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
4 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/bayesian-statistics/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Bayesian Statistics
</a><a href=/en/tags/machine-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Machine Learning
</a><a href=/en/tags/python/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Python</a></div></header><div class=article-content itemprop=articleBody><h2 id=introduction>Introduction</h2><p>Regression is the task of learning the relationship between input data and corresponding output data, and predicting the output for unseen inputs. This article uses <strong>linear regression</strong> as an example to compare four representative parameter estimation methods: <strong>least squares</strong>, <strong>maximum likelihood estimation</strong>, <strong>MAP estimation</strong>, and <strong>Bayesian estimation</strong>, and explains their differences and characteristics.</p><p>As the model, we consider a linear combination of <strong>basis functions</strong> \(\phi(x)\), which are nonlinear functions of the input \(x\). With \(w\) as the model parameters (weights) and \(\epsilon\) as the error, the model is:</p>\[ y = \Phi w + \epsilon \]<p>Here, \(\Phi\) is the design matrix, where each row contains the basis function vector for a data point.</p><hr><h2 id=1-least-squares-estimation>1. Least Squares Estimation</h2><p>Least squares finds the parameters \(\hat{w}\) that minimize the <strong>sum of squared errors</strong> \(S(w)\) between the model predictions and the actual target values.</p>\[ S(w) = (y - \Phi w)^T (y - \Phi w) \]<p>Setting the derivative of \(S(w)\) with respect to \(w\) to zero yields:</p>\[ \hat{w}\_{LS} = (\Phi^T \Phi)^{-1} \Phi^T y \]<p>This has an analytical solution, making computation very fast. However, it is prone to <strong>overfitting</strong> when the training data is limited or the model has high degrees of freedom.</p><h2 id=2-maximum-likelihood-estimation-mle>2. Maximum Likelihood Estimation (MLE)</h2><p>MLE finds the parameters \(\hat{w}\) that maximize the probability of observing the data (<strong>likelihood</strong>).</p><p>Assuming the error term \(\epsilon\) follows a Gaussian distribution with mean \(0\) and variance \(\sigma^2\), the conditional probability of the target \(y\) becomes a Gaussian with mean \(\Phi w\) and variance \(\sigma^2\).</p>\[ p(y | w, \sigma^2) = \mathcal{N}(y | \Phi w, \sigma^2 I) = \frac{1}{(2\pi\sigma^2)^{N/2}} \exp\lbrace-\frac{1}{2\sigma^2}(y-\Phi w)^T(y-\Phi w)\rbrace \]<p>We maximize the <strong>log-likelihood</strong>:</p>\[ \ln p(y|w) = -\frac{N}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}(y-\Phi w)^T(y-\Phi w) \]<p>Maximizing the log-likelihood is equivalent to minimizing the squared error term on the right. Therefore, <strong>under a Gaussian error assumption, MLE yields the same solution as least squares.</strong></p>\[ \hat{w}\_{ML} = (\Phi^T\Phi)^{-1}\Phi^{T}y \]<h2 id=3-map-estimation-maximum-a-posteriori>3. MAP Estimation (Maximum A Posteriori)</h2><p>MAP estimation is a general framework for suppressing overfitting. It treats the parameters \(w\) as random variables and introduces a <strong>prior distribution</strong> \(p(w)\). Using Bayes&rsquo; theorem, it finds the \(\hat{w}\) that maximizes the <strong>posterior distribution</strong> \(p(w|y)\).</p>\[ p(w|y) = \frac{p(y|w)p(w)}{p(y)} \propto p(y|w)p(w) \]<p>Assuming a Gaussian prior with mean \(0\) and covariance \(\alpha^{-1}I\) for \(w\), this encodes the prior belief that the weight values should be close to zero, acting as <strong>regularization</strong> that penalizes large weights.</p>\[ p(w|\alpha) = \mathcal{N}(w|0, \alpha^{-1}I) \]<p>The MAP estimate \(\hat{w}_{MAP}\) is:</p>\[ \hat{w}\_{MAP} = (\Phi^T\Phi + \frac{\beta}{\alpha}I)^{-1}\Phi^{T}y \]<p>where \(\beta = 1/\sigma^2\). This has the same form as <strong>ridge regression (L2-regularized least squares)</strong>, and the regularization term \(\frac{\beta}{\alpha}I\) ensures stable solutions even when \(\Phi^T\Phi\) is not invertible, while suppressing overfitting.</p><h2 id=4-bayesian-estimation>4. Bayesian Estimation</h2><p>Least squares, MLE, and MAP estimation all find a single optimal value for \(w\) (point estimation). However, this approach cannot express <strong>uncertainty</strong> about the parameters.</p><p>Bayesian estimation does not seek a single point estimate but instead computes the <strong>full posterior distribution \(p(w|y)\)</strong>, representing the probability distribution over all possible values of \(w\) given the observed data.</p><p>With the Gaussian prior and likelihood used in MAP, the posterior is also Gaussian (conjugacy):</p>\[ p(w|y) = \mathcal{N}(w | \mu_N, \Sigma_N) \]<p>where the posterior mean \(\mu_N\) and covariance \(\Sigma_N\) are:</p>\[ \mu_N = (\Phi^T\Phi + \frac{\beta}{\alpha}I)^{-1}\Phi^{T}y \]<p></p>\[ \Sigma_N = (\beta\Phi^T\Phi + \alpha I)^{-1} \]<p>For prediction on a new input \(x_*\), we integrate over all possible \(w\), weighted by the posterior probability. This is called the <strong>predictive distribution</strong>.</p>\[ p(y*\* | x*\_, y) = \int p(y\_\_ | x\_\*, w) p(w|y) dw \]<p>The predictive distribution provides not only the mean prediction but also the <strong>variance (confidence interval)</strong> indicating prediction uncertainty.</p><h2 id=experimental-results>Experimental Results</h2><ul><li><strong>Data</strong>: 15 training points generated from \(y = \sin(2\pi x)\) with added noise</li><li><strong>Basis functions</strong>: 9th-degree polynomial (\(f_j(x) = x^j, j=0, ..., 9\))</li><li><strong>Hyperparameters</strong>: \(\alpha=1.0, \beta=10.0\)</li></ul><h4 id=least-squares--maximum-likelihood-estimation>Least Squares / Maximum Likelihood Estimation</h4><p>The model tries to fit the training data closely, causing large oscillations in regions without data &ndash; a clear case of overfitting.
<img src=/posts/20210108_bayes/fig1.png alt="Least squares regression result showing overfitting"></p><h4 id=map-estimation>MAP Estimation</h4><p>The prior distribution (regularization) suppresses overfitting, yielding a smoother prediction curve.
<img src=/posts/20210108_bayes/fig2.png alt="MAP estimation regression result with regularization"></p><h4 id=bayesian-estimation>Bayesian Estimation</h4><p>In addition to a smooth prediction (mean, solid line) similar to MAP, the prediction uncertainty increases in regions with less training data, as shown by the widening confidence interval (blue shaded area).
<img src=/posts/20210108_bayes/fig3.png alt="Bayesian estimation regression result with confidence interval"></p><h4 id=model-evaluation-coefficient-of-determination->Model Evaluation (Coefficient of Determination \(R^2\))</h4><p>The coefficient of determination indicates how well the model fits the data (closer to 1 is better). MAP and Bayesian estimation (mean) achieve higher scores than least squares.
<img src=/posts/20210108_bayes/fig4.png alt="Comparison of R-squared values across estimation methods"></p><h4 id=effect-of-hyperparameter>Effect of Hyperparameter \(\beta\)</h4><p>In Bayesian estimation, increasing the likelihood precision parameter \(\beta\) (inverse noise variance) causes the model to fit training data more tightly. When \(\beta\) is too large, the confidence interval narrows and the result approaches overfitting.</p><ul><li>\(\beta=50\)
<img src=/posts/20210108_bayes/fig5.png alt="Bayesian estimation result with beta=50"></li><li>\(\beta=100\)
<img src=/posts/20210108_bayes/fig6.png alt="Bayesian estimation result with beta=100"></li><li>\(\beta=1000\)
<img src=/posts/20210108_bayes/fig7.png alt="Bayesian estimation result with beta=1000"></li></ul><h2 id=summary>Summary</h2><ul><li><strong>Least Squares / MLE</strong>: Simple and fast, but prone to overfitting.</li><li><strong>MAP Estimation</strong>: Introduces a prior (regularization) to suppress overfitting.</li><li><strong>Bayesian Estimation</strong>: Accounts for parameter uncertainty and provides confidence intervals for predictions. It gives richer information, such as increasing uncertainty in data-sparse regions.</li></ul><h2 id=references>References</h2><ul><li>C.M. Bishop, &ldquo;Pattern Recognition and Machine Learning,&rdquo; Springer (2006)</li><li>Atsushi Suyama, &ldquo;Introduction to Machine Learning with Bayesian Inference,&rdquo; Kodansha (2017)</li></ul></div><div class="ad-slot in-content my-3"><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9558545098866170></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=article-footer></footer><meta itemprop=wordCount content="845"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20210108_bayes/1/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20210107_prml_ent/1/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Fundamentals of Information Theory: From Entropy to Mutual Information (PRML 1.6)</span>
</a><a href=/en/posts/20210109_nnga/1/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Training Neural Networks with Genetic Algorithms in Python</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260223_bayesian_optimization/1/>Bayesian Optimization: Fundamentals and Python Implementation</a></h3><div class=card-meta><time datetime=2026-02-23>February 23, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260215_mppi/1/>MPPI (Model Predictive Path Integral): A Unified View with the Cross-Entropy Method</a></h3><div class=card-meta><time datetime=2026-02-15>February 15, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260215_ckf/1/>Cubature Kalman Filter (CKF): Theory and Python Implementation</a></h3><div class=card-meta><time datetime=2026-02-15>February 15, 2026</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Bayesian Linear Regression: From Least Squares to Bayesian Estimation</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2026 yuhi-sa. All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Bayesian Linear Regression: From Least Squares to Bayesian Estimation","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210108_bayes\/1\/","description":"A comparison of least squares, maximum likelihood, MAP, and Bayesian estimation for linear regression, with experimental results and Python examples.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>