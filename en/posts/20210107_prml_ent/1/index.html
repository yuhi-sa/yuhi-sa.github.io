<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no"><meta name=robots content="
    index
  
  ,follow"><meta name=description content="An introduction to information theory covering information content, entropy, KL divergence, and mutual information, based on PRML Chapter 1.6."><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20210107_prml_ent/1/><meta property="og:type" content="
    article
  "><meta property="og:title" content="
    Fundamentals of Information Theory: From Entropy to Mutual Information (PRML 1.6)
  "><meta property="og:description" content="An introduction to information theory covering information content, entropy, KL divergence, and mutual information, based on PRML Chapter 1.6."><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20210107_prml_ent/1/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-01-07T15:17:23+09:00"><meta property="article:modified_time" content="2026-02-14T00:31:41+09:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Information Theory"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="
    Fundamentals of Information Theory: From Entropy to Mutual Information (PRML 1.6)
  "><meta name=twitter:description content="An introduction to information theory covering information content, entropy, KL divergence, and mutual information, based on PRML Chapter 1.6."><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Fundamentals of Information Theory: From Entropy to Mutual Information (PRML 1.6) |
tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20210107_prml_ent/1/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20210107_prml_ent/1/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/en/posts/20210107_prml_ent/1/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Fundamentals of Information Theory: From Entropy to Mutual Information (PRML 1.6)","description":"An introduction to information theory covering information content, entropy, KL divergence, and mutual information, based on PRML Chapter 1.6.","author":{"@type":"Person","name":""},"publisher":{"@type":"Organization","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-01-07T15:17:23\u002b09:00","dateModified":"2026-02-14T00:31:41\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210107_prml_ent\/1\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210107_prml_ent\/1\/","wordCount":830,"keywords":["Machine Learning","Information Theory"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT4M"}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.de039aff98fb649abaf5fe23d3d4981f460573d7728709826e219c496c8e5be2.css integrity="sha256-3gOa/5j7ZJq69f4j09SYH0YFc9dyhwmCbiGcSWyOW+I=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c50b1500452fdb66f687cdcbf45a7774123c0111e1054e26db51fee604557b28.css integrity="sha256-xQsVAEUv22b2h83L9Fp3dBI8ARHhBU4m21H+5gRVeyg=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script async src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LN6QP6VVM3")</script><script data-ad-client=ca-pub-9558545098866170 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Fundamentals of Information Theory: From Entropy to Mutual Information (PRML 1.6)</h1><p class="lead article-description" itemprop=description>An introduction to information theory covering information content, entropy, KL divergence, and mutual information, based on PRML Chapter 1.6.</p><div class=article-meta><time datetime=2021-01-07T15:17:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
January 7, 2021
</time><time datetime=2026-02-14T00:31:41+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 14, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
4 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/machine-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Machine Learning
</a><a href=/en/tags/information-theory/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Information Theory</a></div></header><div class=article-content itemprop=articleBody><p>Information theory is a mathematical framework for quantifying, compressing, and communicating information. In machine learning, its concepts are applied in various contexts, including model evaluation and regularization.</p><h2 id=information-content>Information Content</h2><p>The &ldquo;information content&rdquo; $h(x)$ gained from observing an event $x$ is defined by how &ldquo;surprising&rdquo; the event is &ndash; that is, how low its probability $p(x)$ is.</p><p>Information content is naturally defined to satisfy the following properties:</p><ol><li><strong>Additivity</strong>: The information gained from observing two independent events $x, y$ equals the sum of the information from each.
$h(x, y) = h(x) + h(y)$</li><li><strong>Independence</strong>: The joint probability of independent events is the product of their individual probabilities.
$p(x, y) = p(x)p(y)$</li></ol><p>From these two properties, it follows that information content should be defined using the logarithm of the probability. A negative sign is added so that information content is non-negative (since $p(x) \le 1$).</p><p>$$ h(x) = -\log_2 p(x) $$</p><p>When using base 2 for the logarithm, the unit of information is the <strong>bit</strong>.</p><h2 id=entropy>Entropy</h2><p>Entropy represents the <strong>average information content</strong> generated by a random variable $X$. It is computed as the expected value of $h(x)$ with respect to the distribution $p(x)$.</p><p>$$ H[X] = \mathbb{E}_{p(x)}[h(x)] = -\sum_x p(x) \log_2 p(x) $$</p><p>Entropy can be interpreted as the degree of &ldquo;uncertainty&rdquo; or &ldquo;unpredictability&rdquo; of a random variable.</p><ul><li><strong>Low entropy</strong>: The distribution is concentrated on a few specific values (sharp peak). Outcomes are easy to predict, so uncertainty is low.</li><li><strong>High entropy</strong>: The distribution is spread across many values (close to uniform). Outcomes are hard to predict, so uncertainty is high.</li></ul><p><img src=.././fig1.png alt=fig1.png></p><p>According to the <strong>noiseless coding theorem</strong>, entropy provides a lower bound on the number of bits needed to transmit the values of a random variable without error. For example, when encoding characters with unequal frequencies, assigning shorter codes to frequent characters and longer codes to rare ones brings the average code length close to the entropy.</p><h3 id=differential-entropy>Differential Entropy</h3><p>The extension of entropy to continuous random variables is called <strong>differential entropy</strong>.</p><p>$$ H[X] = -\int p(x) \ln p(x) dx $$</p><p><strong>Note</strong>: Unlike its discrete counterpart, differential entropy can take negative values and does not represent an absolute quantity of information.</p><h4 id=maximizing-differential-entropy>Maximizing Differential Entropy</h4><p>Considering which distribution maximizes differential entropy under certain constraints reveals the nature of that random variable. For example, the distribution that maximizes differential entropy given fixed mean $\mu$ and variance $\sigma^2$ is the <strong>Gaussian distribution</strong>.</p><p>$$ p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\lbrace-\frac{(x-\mu)^2}{2\sigma^2}\rbrace $$</p><p>The differential entropy of a Gaussian distribution is:</p><p>$$ H[X] = \frac{1}{2} {1 + \ln(2\pi\sigma^2)} $$</p><p>This shows that entropy increases with larger variance $\sigma^2$ (greater spread of the distribution).</p><h2 id=conditional-entropy-and-mutual-information>Conditional Entropy and Mutual Information</h2><h3 id=conditional-entropy>Conditional Entropy</h3><p>Consider the joint distribution $p(x, y)$ of two random variables $X, Y$. <strong>Conditional entropy</strong> measures how much uncertainty remains about $Y$ given that $X = x$ is known.</p><p>$$ H[Y|X] = -\iint p(x, y) \ln p(y|x) dy dx $$</p><p>These satisfy the chain rule of entropy:</p><p>$$ H[X, Y] = H[Y|X] + H[X] $$</p><p>This can be interpreted as: &ldquo;the information needed to determine both $X$ and $Y$ equals the information needed to determine $X$ plus the additional information needed to determine $Y$ given $X$.&rdquo;</p><h3 id=relative-entropy-kullback-leibler-divergence>Relative Entropy (Kullback-Leibler Divergence)</h3><p>When approximating an unknown true distribution $p(x)$ with a model $q(x)$, the <strong>relative entropy</strong> or <strong>KL divergence</strong> is used to measure the &ldquo;gap&rdquo; between the two distributions.</p><p>$$ KL(p||q) = -\int p(x) \ln \lbrace \frac{q(x)}{p(x)} \rbrace dx $$</p><p>KL divergence has the following important properties:</p><ul><li>$KL(p||q) \ge 0$</li><li>$KL(p||q) = 0$ if and only if $p(x) = q(x)$</li></ul><p>This makes KL divergence interpretable as a &ldquo;distance-like&rdquo; measure between two distributions (though it is not a true mathematical distance since it lacks symmetry: $KL(p||q) \neq KL(q||p)$).</p><h4 id=kl-divergence-minimization-and-maximum-likelihood>KL Divergence Minimization and Maximum Likelihood</h4><p>When data is generated from an unknown distribution $p(x)$ and we approximate it with a parameterized model $q(x|\theta)$, the goal is to find $\theta$ that minimizes the KL divergence between them.</p><p>Expanding $KL(p||q)$:
$$ KL(p||q) = -\int p(x) \ln q(x|\theta) dx + \int p(x) \ln p(x) dx $$</p><p>The second term is the entropy of the true distribution and does not depend on $\theta$. Therefore, minimizing $KL(p||q)$ is equivalent to maximizing the first term &ndash; the <strong>expected log-likelihood</strong>.</p><p>Given a dataset ${x_n}$, this expectation can be approximated by the sample average, so <strong>minimizing KL divergence is equivalent to maximum likelihood estimation</strong>.</p><h3 id=mutual-information>Mutual Information</h3><p><strong>Mutual information</strong> measures how strongly two random variables $X, Y$ depend on each other &ndash; how much knowing one reduces the uncertainty about the other.</p><p>It is defined as the KL divergence between the joint distribution $p(x, y)$ and the product of marginals $p(x)p(y)$:</p><p>$$ I[X, Y] \equiv KL(p(x, y) || p(x)p(y)) = \iint p(x, y) \ln ( \frac{p(x, y)}{p(x)p(y)} ) dx dy $$</p><p>Mutual information can also be expressed using entropy:</p><p>$$ I[X, Y] = H[X] - H[X|Y] = H[Y] - H[Y|X] $$</p><p>This is interpreted as &ldquo;the amount by which knowing $Y$ reduces the uncertainty of $X$.&rdquo;</p><h2 id=references>References</h2><ul><li>C.M. Bishop, &ldquo;Pattern Recognition and Machine Learning,&rdquo; Springer (2006)</li></ul></div><footer class=article-footer></footer><meta itemprop=wordCount content="830"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20210107_prml_ent/1/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20210107_pso_tcpso/1/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Particle Swarm Optimization (PSO) and Two-Swarm Cooperative PSO (TCPSO) with Matlab</span>
</a><a href=/en/posts/20210108_bayes/1/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Bayesian Linear Regression: From Least Squares to Bayesian Estimation</span></a></nav><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Fundamentals of Information Theory: From Entropy to Mutual Information (PRML 1.6)</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy;
2026
tomato blog.
All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Fundamentals of Information Theory: From Entropy to Mutual Information (PRML 1.6)","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210107_prml_ent\/1\/","description":"An introduction to information theory covering information content, entropy, KL divergence, and mutual information, based on PRML Chapter 1.6.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>