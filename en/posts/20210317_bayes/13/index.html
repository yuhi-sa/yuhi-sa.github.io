<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="A detailed explanation of Variational Autoencoders (VAE), covering the encoder-decoder architecture, ELBO derivation, and the reparameterization trick."><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20210317_bayes/13/><meta property="og:type" content="article"><meta property="og:title" content="Variational Autoencoder (VAE)"><meta property="og:description" content="A detailed explanation of Variational Autoencoders (VAE), covering the encoder-decoder architecture, ELBO derivation, and the reparameterization trick."><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20210317_bayes/13/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-03-18T12:00:23+09:00"><meta property="article:modified_time" content="2026-02-10T00:07:14+09:00"><meta property="article:tag" content="Bayesian Statistics"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Deep Learning"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Variational Autoencoder (VAE)"><meta name=twitter:description content="A detailed explanation of Variational Autoencoders (VAE), covering the encoder-decoder architecture, ELBO derivation, and the reparameterization trick."><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Variational Autoencoder (VAE) | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20210317_bayes/13/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20210317_bayes/13/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Variational Autoencoder (VAE)","description":"A detailed explanation of Variational Autoencoders (VAE), covering the encoder-decoder architecture, ELBO derivation, and the reparameterization trick.","author":{"@type":"Person","name":""},"publisher":{"@type":"Organization","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-03-18T12:00:23\u002b09:00","dateModified":"2026-02-10T00:07:14\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210317_bayes\/13\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210317_bayes\/13\/","wordCount":672,"keywords":["Bayesian Statistics","Machine Learning","Deep Learning"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT4M"}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.72a177faa7b12de55dcc39c4ab6b6392116718d5e2735dab0214511354ecb973.css integrity="sha256-cqF3+qexLeVdzDnEq2tjkhFnGNXic12rAhRRE1TsuXM=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c50b1500452fdb66f687cdcbf45a7774123c0111e1054e26db51fee604557b28.css integrity="sha256-xQsVAEUv22b2h83L9Fp3dBI8ARHhBU4m21H+5gRVeyg=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script async src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LN6QP6VVM3")</script><script data-ad-client=ca-pub-9558545098866170 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Variational Autoencoder (VAE)</h1><p class="lead article-description" itemprop=description>A detailed explanation of Variational Autoencoders (VAE), covering the encoder-decoder architecture, ELBO derivation, and the reparameterization trick.</p><div class=article-meta><time datetime=2021-03-18T12:00:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
March 18, 2021
</time><time datetime=2026-02-10T00:07:14+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated February 10, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
4 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/bayesian-statistics/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Bayesian Statistics
</a><a href=/en/tags/machine-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Machine Learning
</a><a href=/en/tags/deep-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Deep Learning</a></div></header><div class=article-content itemprop=articleBody><p>The Variational Autoencoder (VAE) is a type of generative model that aims to learn the latent structure of data and generate new data. Similar to how the EM algorithm maximizes the log-likelihood lower bound, VAE also learns by maximizing the Evidence Lower Bound (ELBO).</p><p>A key feature of VAE is that it has a <strong>probabilistic encoder (recognition model)</strong> and a <strong>probabilistic decoder (generative model)</strong>. This makes the latent space smooth and enables meaningful data generation.</p><ul><li><strong>Recognition model (encoder)</strong>: Estimates the probability distribution $q_\phi(z|x)$ of latent variables $z$ from input data $x$. It has parameters $\phi$.</li><li><strong>Generative model (decoder)</strong>: Generates the probability distribution $p_\theta(x|z)$ of data $x$ from latent variables $z$. It has parameters $\theta$.</li></ul><p>The latent variable $z$ can be interpreted as a lower-dimensional, abstract &ldquo;latent representation&rdquo; or &ldquo;latent code&rdquo; of the information contained in input data $x$.</p><h2 id=autoencoder>Autoencoder</h2><p>An autoencoder is a neural network that learns to compress and encode input data, then reconstruct (decode) it to output the same content as the input. The intermediate layer (latent space) becomes a compressed representation (code) that captures the important features of the input data.</p><p>VAE introduces the concept of variational inference into this autoencoder framework.</p><h2 id=the-elbo-in-vae>The ELBO in VAE</h2><p>VAE learning is performed by maximizing the following Evidence Lower Bound (ELBO):</p><p>$$ \mathcal{L}(\theta, \phi) = \mathbb{E}<em>{q</em>\phi(z|x)}[\log p_\theta(x|z)] - KL(q_\phi(z|x) || p(z)) $$</p><p>This formula consists of two terms:</p><ol><li><p><strong>Reconstruction error (first term)</strong>: $\mathbb{E}<em>{q</em>\phi(z|x)}[\log p_\theta(x|z)]$
This represents how accurately the decoder can reconstruct the original input $x$ using the latent variable $z$ generated by the encoder. Maximizing this term corresponds to minimizing the reconstruction error.</p></li><li><p><strong>Regularization term (second term)</strong>: $KL(q_\phi(z|x) || p(z))$
This measures how close the distribution $q_\phi(z|x)$ of latent variables $z$ estimated by the encoder is to the predefined <strong>prior distribution</strong> $p(z)$ of the latent variables. Minimizing this term serves to make the latent space smooth and enable meaningful data generation.</p></li></ol><h3 id=recognition-model-q_phizx>Recognition Model $q_\phi(z|x)$</h3><p>The recognition model estimates the distribution of latent variables $z$ from input $x$. Typically, a neural network is used to output the mean $\mu(x)$ and variance $\sigma^2(x)$ (or log-variance $\log \sigma^2(x)$) of the multivariate normal distribution that $z$ follows.</p><p>$$ q_\phi(z|x) = \mathcal{N}(z | \mu_\phi(x), \text{diag}(\sigma^2_\phi(x))) $$</p><p>Here, $\mu_\phi(x)$ and $\sigma^2_\phi(x)$ are outputs of the neural network that takes input $x$.</p><h3 id=generative-model-p_thetaxz>Generative Model $p_\theta(x|z)$</h3><p>The generative model produces the distribution of data $x$ from latent variables $z$. The choice of probability distribution depends on the type of data $x$.</p><ul><li><strong>Binary data (e.g., black-and-white images)</strong>: Bernoulli distribution (or categorical distribution)</li><li><strong>Continuous data (e.g., grayscale images)</strong>: Gaussian distribution</li></ul><p>For example, for continuous data, a multivariate normal distribution with variance fixed at 1 may be used:</p><p>$$ p_\theta(x|z) = \mathcal{N}(x | \nu_\theta(z), I) $$</p><p>Here, $\nu_\theta(z)$ is the output of the neural network that takes latent variable $z$ as input.</p><h3 id=prior-distribution-of-latent-variables-pz>Prior Distribution of Latent Variables $p(z)$</h3><p>The prior distribution of latent variables is typically defined as a product of standard normal distributions (mean 0, variance 1).</p><p>$$ p(z) = \prod_{j=1}^k \mathcal{N}(z_j | 0, 1) $$</p><p>This prior is fixed during training and does not depend on parameters $\theta$ or $\phi$.</p><h2 id=gradient-descent-and-the-reparameterization-trick>Gradient Descent and the Reparameterization Trick</h2><p>VAE training uses gradient descent (optimization algorithms such as Adam) to update parameters $\theta$ and $\phi$ to maximize the ELBO $\mathcal{L}(\theta, \phi)$.</p><p>The gradient of the reconstruction error term is relatively straightforward to compute, but directly computing the gradient of the regularization term $KL(q_\phi(z|x) || p(z))$ is difficult. This is because the expectation is taken over $q_\phi(z|x)$, which depends on $\phi$.</p><p>To solve this problem, the <strong>reparameterization trick</strong> is used. This technique represents the latent variable $z$ using a random variable $\epsilon$ that does not depend on parameter $\phi$ and a deterministic function $g(\epsilon, x, \phi)$ that depends on $\phi$.</p><p>For example, in the case of a Gaussian distribution, $z = \mu_\phi(x) + \sigma_\phi(x) \cdot \epsilon$ (where $\epsilon \sim \mathcal{N}(0, I)$). This allows the expectation to be computed over $\epsilon$, which does not depend on $\phi$, enabling standard gradient descent to be applied.</p><h2 id=references>References</h2><ul><li>Taro Tezuka, &ldquo;Understanding Bayesian Statistics and Machine Learning,&rdquo; Kodansha (2017)</li></ul></div><footer class=article-footer></footer><meta itemprop=wordCount content="672"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20210317_bayes/13/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20210317_bayes/12/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Markov Chain Monte Carlo (MCMC)</span>
</a><a href=/en/posts/20210318_pid/1/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Fundamentals of PID Control and the Role of Each Component</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210317_bayes/12/>Markov Chain Monte Carlo (MCMC)</a></h3><div class=card-meta><time datetime=2021-03-18>March 18, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210317_bayes/11/>Variational Bayes</a></h3><div class=card-meta><time datetime=2021-03-18>March 18, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210317_bayes/10/>The EM Algorithm (2): Evidence Lower Bound and KL Divergence</a></h3><div class=card-meta><time datetime=2021-03-17>March 17, 2021</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Variational Autoencoder (VAE)</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2026
tomato blog.
All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Variational Autoencoder (VAE)","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210317_bayes\/13\/","description":"A detailed explanation of Variational Autoencoders (VAE), covering the encoder-decoder architecture, ELBO derivation, and the reparameterization trick.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>