<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#e74c3c"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="An introduction to model-based reinforcement learning covering the Bellman equation, dynamic programming, value iteration, and policy iteration methods."><meta name=author content="yuhi-sa"><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20210319_rl/2/><meta property="og:type" content="article"><meta property="og:title" content="Model-Based Reinforcement Learning"><meta property="og:description" content="An introduction to model-based reinforcement learning covering the Bellman equation, dynamic programming, value iteration, and policy iteration methods."><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20210319_rl/2/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-03-18T19:00:23+09:00"><meta property="article:modified_time" content="2026-02-14T01:34:22+09:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Model-Based Reinforcement Learning"><meta name=twitter:description content="An introduction to model-based reinforcement learning covering the Bellman equation, dynamic programming, value iteration, and policy iteration methods."><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Model-Based Reinforcement Learning | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://pagead2.googlesyndication.com crossorigin><link rel=dns-prefetch href=https://pagead2.googlesyndication.com><link rel=icon href=https://yuhi-sa.github.io/favicon.ico><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20210319_rl/2/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20210319_rl/2/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/en/posts/20210319_rl/2/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Model-Based Reinforcement Learning","description":"An introduction to model-based reinforcement learning covering the Bellman equation, dynamic programming, value iteration, and policy iteration methods.","author":{"@type":"Person","name":"yuhi-sa","url":"https:\/\/yuhi-sa.github.io\/"},"publisher":{"@type":"Organization","name":"tomato blog","logo":{"@type":"ImageObject","url":"https:\/\/yuhi-sa.github.io\/ogp.jpeg"},"url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-03-18T19:00:23\u002b09:00","dateModified":"2026-02-14T01:34:22\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/2\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/2\/","wordCount":629,"keywords":["Reinforcement Learning"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT3M"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"tomato blog","item":"https:\/\/yuhi-sa.github.io\/en\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/yuhi-sa.github.io\/en\/posts\/"},{"@type":"ListItem","position":3,"name":"Model-Based Reinforcement Learning"}]}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.de039aff98fb649abaf5fe23d3d4981f460573d7728709826e219c496c8e5be2.css integrity="sha256-3gOa/5j7ZJq69f4j09SYH0YFc9dyhwmCbiGcSWyOW+I=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c50b1500452fdb66f687cdcbf45a7774123c0111e1054e26db51fee604557b28.css integrity="sha256-xQsVAEUv22b2h83L9Fp3dBI8ARHhBU4m21H+5gRVeyg=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script data-ad-client=ca-pub-9558545098866170 async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Model-Based Reinforcement Learning</h1><p class="lead article-description" itemprop=description>An introduction to model-based reinforcement learning covering the Bellman equation, dynamic programming, value iteration, and policy iteration methods.</p><div class=article-meta><time datetime=2021-03-18T19:00:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
March 18, 2021
</time><time datetime=2026-02-14T01:34:22+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 14, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
3 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/reinforcement-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Reinforcement Learning</a></div></header><div class=article-content itemprop=articleBody><p>Reinforcement learning methods are broadly divided into two types depending on whether they use a model of the environment (transition function and reward function).</p><ul><li><strong>Model-Based Reinforcement Learning</strong>: Methods that explicitly use a model of the environment (transition function $T(s&rsquo;|s,a)$ and reward function $R(s,s&rsquo;)$) to learn actions. This includes cases where the model is not known in advance but is estimated through interaction with the environment.</li><li><strong>Model-Free Reinforcement Learning</strong>: Methods where the agent directly learns optimal actions from experience without explicitly constructing or using a model of the environment.</li></ul><p>This article explains model-based reinforcement learning.</p><h2 id=definition-of-value>Definition of Value</h2><p>In reinforcement learning, the goal is to maximize the sum of future rewards. This sum of future rewards is called &ldquo;value.&rdquo;</p><ol><li><p><strong>Sum of Future Rewards</strong>: The sum of immediate rewards $G_t$ obtained from time $t$ onward can be defined recursively as follows:
$$ G<em>t = r</em>{t+1} + \gamma G*{t+1} $$
Here, $r*{t+1}$ is the immediate reward obtained at time $t+1$, and $\gamma$ is the <strong>discount factor</strong> ($0 \le \gamma \le 1$), a coefficient for converting future rewards to present value.</p></li><li><p><strong>Value as Expected Value</strong>: Since environmental transitions are stochastic, the same action does not always produce the same result. Therefore, value is defined as an expected value.
The value function $V_\pi(s)$ of state $s$ under policy $\pi$ represents the expected cumulative reward when starting from state $s$ and following policy $\pi$.</p></li></ol><h3 id=bellman-equation>Bellman Equation</h3><p>The <strong>Bellman equation</strong> expresses the value function recursively in terms of expected values. It shows that the value of a state is determined by the actions available from that state and the values of the next states resulting from those actions.</p><p>$$ V*\pi(s) = \sum_a \pi(a|s) \sum*{s&rsquo;} T(s&rsquo;|s,a) (R(s,a,s&rsquo;) + \gamma V_\pi(s&rsquo;)) $$
Here, $R(s,a,s&rsquo;)$ is the reward obtained when taking action $a$ in state $s$ and transitioning to state $s&rsquo;$.</p><h2 id=learning-dynamic-programming>Learning: Dynamic Programming</h2><p>In model-based reinforcement learning, since the environment model is known, <strong>dynamic programming</strong> can be used to compute optimal policies and value functions. Dynamic programming sets appropriate initial values for the value function and improves its accuracy by repeatedly applying the Bellman equation.</p><p>There are two main approaches for obtaining optimal actions through dynamic programming.</p><h3 id=value-iteration>Value Iteration</h3><p>The agent calculates the value of each state and decides actions to transition to the state with the highest value. Value iteration aims to directly find the optimal value function $V^*(s)$.</p><p>$$ V*{k+1}(s) = \max_a \left{ \sum*{s&rsquo;} T(s&rsquo;|s,a) (R(s,a,s&rsquo;) + \gamma V_k(s&rsquo;)) \right} $$
Through this iterative computation, $V_k(s)$ converges to the optimal value function $V^*(s)$. Once the optimal value function is obtained, the optimal action at each state can be determined as the action that maximizes the value.</p><h3 id=policy-iteration>Policy Iteration</h3><p>Policy iteration finds the optimal policy by alternating between two steps: &ldquo;policy evaluation&rdquo; and &ldquo;policy improvement.&rdquo;</p><ol><li><strong>Policy Evaluation</strong>: Computes the value function $V_\pi(s)$ under the current policy $\pi$. This is done by solving the Bellman equation.
$$ V*\pi(s) = \sum_a \pi(a|s) \sum*{s&rsquo;} T(s&rsquo;|s,a) (R(s,a,s&rsquo;) + \gamma V_\pi(s&rsquo;)) $$</li><li><strong>Policy Improvement</strong>: Using the evaluated value function $V_\pi(s)$, greedily determines a better policy $\pi&rsquo;$ than the current policy $\pi$.
$$ \pi&rsquo;(s) = \arg\max<em>a \left{ \sum</em>{s&rsquo;} T(s&rsquo;|s,a) (R(s,a,s&rsquo;) + \gamma V_\pi(s&rsquo;)) \right} $$
By repeating this process, the policy and value function converge to optimal ones.</li></ol><h2 id=difference-between-model-based-and-model-free>Difference Between Model-Based and Model-Free</h2><p>In model-based reinforcement learning, since the environment&rsquo;s transition function and reward function are known (or can be estimated), the optimal policy can be computed from model information alone without actually running the agent in the environment.</p><p>In contrast, in model-free reinforcement learning, since the environment model is unknown, the agent must actually interact with the environment and learn optimal policies and value functions directly from its experience (sequences of states, actions, rewards, and next states).</p><h2 id=references>References</h2><ul><li>Takahiro Kubo, &ldquo;Introduction to Reinforcement Learning with Python: From Basics to Practice&rdquo;, Shoeisha (2019)</li></ul></div><div class="ad-slot in-content my-3"><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9558545098866170></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=article-footer></footer><meta itemprop=wordCount content="629"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20210319_rl/2/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20210319_rl/1/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Reinforcement Learning Basics: Overview and Markov Decision Process</span>
</a><a href=/en/posts/20210319_rl/3/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Model-Free Reinforcement Learning</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/1/>Reinforcement Learning Basics: Overview and Markov Decision Process</a></h3><div class=card-meta><time datetime=2021-03-18>March 18, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20200831_rl_map/1/>A Comprehensive Overview of Reinforcement Learning</a></h3><div class=card-meta><time datetime=2020-08-31>August 31, 2020</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Model-Based Reinforcement Learning</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy;
2026
yuhi-sa.
All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Model-Based Reinforcement Learning","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/2\/","description":"An introduction to model-based reinforcement learning covering the Bellman equation, dynamic programming, value iteration, and policy iteration methods.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>