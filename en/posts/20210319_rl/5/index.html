<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#e74c3c"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="An explanation of policy gradient methods in reinforcement learning, covering the policy gradient theorem, log-derivative trick, and variance reduction with …"><meta name=author content="yuhi-sa"><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20210319_rl/5/><meta property="og:type" content="article"><meta property="og:title" content="Policy Gradient Methods"><meta property="og:description" content="An explanation of policy gradient methods in reinforcement learning, covering the policy gradient theorem, log-derivative trick, and variance reduction with …"><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20210319_rl/5/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-03-22T11:00:23+09:00"><meta property="article:modified_time" content="2026-02-14T01:34:22+09:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Policy Gradient Methods"><meta name=twitter:description content="An explanation of policy gradient methods in reinforcement learning, covering the policy gradient theorem, log-derivative trick, and variance reduction with …"><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Policy Gradient Methods | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://pagead2.googlesyndication.com crossorigin><link rel=dns-prefetch href=https://pagead2.googlesyndication.com><link rel=icon href=https://yuhi-sa.github.io/favicon.ico><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20210319_rl/5/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20210319_rl/5/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/en/posts/20210319_rl/5/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Policy Gradient Methods","description":"An explanation of policy gradient methods in reinforcement learning, covering the policy gradient theorem, log-derivative trick, and variance reduction with baselines.","author":{"@type":"Person","name":"yuhi-sa","url":"https:\/\/yuhi-sa.github.io\/"},"publisher":{"@type":"Organization","name":"tomato blog","logo":{"@type":"ImageObject","url":"https:\/\/yuhi-sa.github.io\/ogp.jpeg"},"url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-03-22T11:00:23\u002b09:00","dateModified":"2026-02-14T01:34:22\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/5\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/5\/","wordCount":453,"keywords":["Reinforcement Learning"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT3M"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"tomato blog","item":"https:\/\/yuhi-sa.github.io\/en\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/yuhi-sa.github.io\/en\/posts\/"},{"@type":"ListItem","position":3,"name":"Policy Gradient Methods"}]}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.72a177faa7b12de55dcc39c4ab6b6392116718d5e2735dab0214511354ecb973.css integrity="sha256-cqF3+qexLeVdzDnEq2tjkhFnGNXic12rAhRRE1TsuXM=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.f96a20189e31f38ad6d3afd6435427cd8819cd0ba84dc5c0e46b70e6f5a52583.css integrity="sha256-+WogGJ4x84rW06/WQ1QnzYgZzQuoTcXA5Gtw5vWlJYM=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>window.MathJax={tex:{inlineMath:[["\\(","\\)"]],displayMath:[["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script data-ad-client=ca-pub-9558545098866170 async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Policy Gradient Methods</h1><p class="lead article-description" itemprop=description>An explanation of policy gradient methods in reinforcement learning, covering the policy gradient theorem, log-derivative trick, and variance reduction with baselines.</p><div class=article-meta><time datetime=2021-03-22T11:00:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
March 22, 2021
</time><time datetime=2026-02-14T01:34:22+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 14, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
3 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/reinforcement-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Reinforcement Learning</a></div></header><div class=article-content itemprop=articleBody><p>The Policy Gradient Method is a policy-based approach in reinforcement learning. Instead of learning a value function, it represents the <strong>policy itself</strong> as a parameterized function and directly optimizes its parameters to find the optimal policy.</p><p>The policy is represented as a function that takes state \(s\) as input and outputs the probability \(\pi_\theta(a|s)\) of selecting each action \(a\). Here, \(\theta\) represents the policy parameters.</p><h2 id=policy-evaluation-and-optimization>Policy Evaluation and Optimization</h2><p>The quality of a policy is evaluated by the <strong>expected reward</strong> (or expected cumulative reward) obtained when following that policy. The goal of the policy gradient method is to maximize this expected reward.</p><p>The expected reward \(J(\theta)\) can be expressed as:</p>\[ J(\theta) = \sum*s d^{\pi*\theta}(s) \sum*a \pi*\theta(a|s) Q^{\pi\_\theta}(s,a) \]<p>Where:</p><ul><li>\(d^{\pi_\theta}(s)\): The stationary distribution (or discounted state visitation frequency) of visiting state \(s\) under policy \(\pi_\theta\).</li><li>\(Q^{\pi_\theta}(s,a)\): The action-value function for taking action \(a\) in state \(s\) under policy \(\pi_\theta\).</li></ul><p>To maximize this expected reward \(J(\theta)\), the parameter \(\theta\) is updated using gradient ascent. That is, the gradient \(\nabla J(\theta)\) of the expected reward is computed, and the parameters are incrementally updated in that direction.</p><h2 id=policy-gradient-theorem>Policy Gradient Theorem</h2><p>The policy gradient theorem shows that the gradient \(\nabla J(\theta)\) of the expected reward can be expressed in the following simple form:</p>\[ \nabla J(\theta) \propto \sum*s d^{\pi*\theta}(s) \sum*a \nabla \pi*\theta(a|s) Q^{\pi\_\theta}(s,a) \]<p>Furthermore, using the log-derivative trick, this gradient can be expressed in the form of an expectation:</p>\[ \nabla J(\theta) = \mathbb{E}_{s \sim d^{\pi_\theta}, a \sim \pi*\theta}[\nabla \log \pi*\theta(a|s) Q^{\pi\_\theta}(s,a)] \]<p>This formula helps to intuitively interpret the policy gradient:</p><ul><li>\(\nabla \log \pi_\theta(a|s)\): Indicates the direction (gradient) that increases the probability of selecting action \(a\).</li><li>\(Q^{\pi_\theta}(s,a)\): A weight representing the &ldquo;goodness&rdquo; of that action.</li></ul><p>In other words, the policy gradient method updates the policy to &ldquo;increase the probability of selecting good actions (actions with high \(Q\) values) and decrease the probability of selecting bad actions (actions with low \(Q\) values).&rdquo;</p><h2 id=implementation-challenges-and-solutions>Implementation Challenges and Solutions</h2><p>There are several challenges in implementing the policy gradient method:</p><ul><li><strong>Estimating \(Q^{\pi_\theta}(s,a)\)</strong>: Since the action-value function \(Q^{\pi_\theta}(s,a)\) is unknown, it needs to be estimated from experience. Monte Carlo methods (cumulative rewards after episode completion) or TD learning (TD errors) are used for approximation.</li><li><strong>Variance Reduction</strong>: When using Monte Carlo methods for gradient estimation, variance can be large, leading to unstable learning. To address this, it is common to introduce a <strong>baseline</strong> (e.g., state-value function \(V(s)\)) and use the advantage function \(A(s,a) = Q(s,a) - V(s)\).</li></ul><p>The policy gradient method is a highly flexible approach that can also be applied to continuous action space problems and serves as the foundation for more advanced algorithms such as Actor-Critic methods.</p><h2 id=references>References</h2><ul><li>Takahiro Kubo, &ldquo;Introduction to Reinforcement Learning with Python: From Basics to Practice&rdquo;, Shoeisha (2019)</li></ul></div><div class="ad-slot in-content my-3"><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9558545098866170></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=article-footer></footer><meta itemprop=wordCount content="453"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20210319_rl/5/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20210319_rl/4/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Reinforcement Learning with Neural Networks</span>
</a><a href=/en/posts/20210319_rl/6/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Advantage Actor-Critic (A2C)</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/4/>Reinforcement Learning with Neural Networks</a></h3><div class=card-meta><time datetime=2021-03-22>March 22, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/3/>Model-Free Reinforcement Learning</a></h3><div class=card-meta><time datetime=2021-03-19>March 19, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/2/>Model-Based Reinforcement Learning</a></h3><div class=card-meta><time datetime=2021-03-18>March 18, 2021</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Policy Gradient Methods</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2026 yuhi-sa. All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Policy Gradient Methods","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/5\/","description":"An explanation of policy gradient methods in reinforcement learning, covering the policy gradient theorem, log-derivative trick, and variance reduction with baselines.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>