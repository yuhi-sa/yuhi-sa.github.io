<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no"><meta name=robots content="
    index
  
  ,follow"><meta name=description content="A comprehensive overview of deep reinforcement learning algorithms including A3C, DDPG, DPG, TRPO, and PPO with their mechanisms and characteristics."><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20210319_rl/7/><meta property="og:type" content="
    article
  "><meta property="og:title" content="
    Key Algorithms in Deep Reinforcement Learning
  "><meta property="og:description" content="A comprehensive overview of deep reinforcement learning algorithms including A3C, DDPG, DPG, TRPO, and PPO with their mechanisms and characteristics."><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20210319_rl/7/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-03-22T11:35:23+09:00"><meta property="article:modified_time" content="2026-02-14T00:31:41+09:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Deep Learning"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="
    Key Algorithms in Deep Reinforcement Learning
  "><meta name=twitter:description content="A comprehensive overview of deep reinforcement learning algorithms including A3C, DDPG, DPG, TRPO, and PPO with their mechanisms and characteristics."><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Key Algorithms in Deep Reinforcement Learning |
tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20210319_rl/7/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20210319_rl/7/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Key Algorithms in Deep Reinforcement Learning","description":"A comprehensive overview of deep reinforcement learning algorithms including A3C, DDPG, DPG, TRPO, and PPO with their mechanisms and characteristics.","author":{"@type":"Person","name":""},"publisher":{"@type":"Organization","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-03-22T11:35:23\u002b09:00","dateModified":"2026-02-14T00:31:41\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/7\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/7\/","wordCount":579,"keywords":["Reinforcement Learning","Deep Learning"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT3M"}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.de039aff98fb649abaf5fe23d3d4981f460573d7728709826e219c496c8e5be2.css integrity="sha256-3gOa/5j7ZJq69f4j09SYH0YFc9dyhwmCbiGcSWyOW+I=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c50b1500452fdb66f687cdcbf45a7774123c0111e1054e26db51fee604557b28.css integrity="sha256-xQsVAEUv22b2h83L9Fp3dBI8ARHhBU4m21H+5gRVeyg=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script async src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LN6QP6VVM3")</script><script data-ad-client=ca-pub-9558545098866170 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Key Algorithms in Deep Reinforcement Learning</h1><p class="lead article-description" itemprop=description>A comprehensive overview of deep reinforcement learning algorithms including A3C, DDPG, DPG, TRPO, and PPO with their mechanisms and characteristics.</p><div class=article-meta><time datetime=2021-03-22T11:35:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
March 22, 2021
</time><time datetime=2026-02-14T00:31:41+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 14, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
3 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/reinforcement-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Reinforcement Learning
</a><a href=/en/tags/deep-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Deep Learning</a></div></header><div class=article-content itemprop=articleBody><p>Methods that apply deep neural networks (DNN) to reinforcement learning are broadly classified into three categories: &ldquo;value-based,&rdquo; &ldquo;policy-based,&rdquo; and &ldquo;Actor-Critic,&rdquo; which combines both.</p><p><img src=.././DNN.png alt="Deep RL methods using DNNs"></p><h2 id=actor-critic-algorithms>Actor-Critic Algorithms</h2><p>The Actor-Critic method simultaneously learns a policy (Actor) and a value function (Critic).</p><h3 id=asynchronous-advantage-actor-critic-a3c>Asynchronous Advantage Actor-Critic (A3C)</h3><p>A3C is an asynchronous extension of A2C (Advantage Actor-Critic). Multiple agents collect experience and learn in parallel across different environments.</p><ul><li><strong>Asynchronous Learning</strong>: Each agent copies parameters from the central global network, collects experience in its own environment while computing gradients, and then asynchronously applies the computed gradients to update the global network.</li><li><strong>Comparison with Experience Replay</strong>: While Experience Replay stores experience collected by a single agent in a buffer for learning, A3C ensures experience diversity by having multiple agents simultaneously collect experience in different environments. This reduces correlations between experiences and stabilizes learning.</li><li><strong>Origin of A2C</strong>: After A3C was published, it was shown that asynchrony does not necessarily contribute to performance improvement, and A2C was proposed, which synchronously collects gradients from multiple agents for updates. A2C is simpler to implement than A3C and often performs equally well or better.</li></ul><h3 id=deep-deterministic-policy-gradient-ddpg>Deep Deterministic Policy Gradient (DDPG)</h3><p>DDPG is an Actor-Critic algorithm designed for continuous action spaces. While DQN was specialized for discrete action spaces, DDPG can output continuous actions.</p><ul><li><strong>Deterministic Policy</strong>: Instead of outputting an action probability distribution, the Actor directly outputs a deterministic action from the state.</li><li><strong>Use of Experience Replay</strong>: Like DQN, Experience Replay is used to stabilize learning.</li><li><strong>Target Networks</strong>: Like DQN, target networks (for both Actor and Critic) are used for learning stabilization.</li><li><strong>TD Error</strong>: The Critic updates the value function using TD error, and this information is used for the Actor&rsquo;s policy update.</li></ul><h3 id=deterministic-policy-gradient-dpg>Deterministic Policy Gradient (DPG)</h3><p>DPG is the foundational algorithm for DDPG, which computes the gradient of a deterministic policy. DDPG combines DPG with deep learning and DQN&rsquo;s stabilization techniques (Experience Replay, target networks).</p><h2 id=policy-gradient-algorithms>Policy Gradient Algorithms</h2><p>Policy gradient methods directly update policy parameters to maximize expected reward. However, they face the challenge of unstable learning.</p><h3 id=trust-region-policy-optimization-trpo>Trust Region Policy Optimization (TRPO)</h3><p>TRPO is a method to improve the stability of policy gradient learning. When updating the policy, it imposes a <strong>trust region</strong> constraint to prevent the updated policy from deviating too far from the previous policy.</p><ul><li><strong>Constraint</strong>: The KL divergence between the updated policy $\pi_\theta$ and the previous policy $\pi_{\theta_{old}}$ is constrained to be below a threshold $\delta$.
$$ \mathbb{E}<em>{s \sim d^{\pi</em>{\theta*{old}}}}[KL[\pi*{\theta*{old}}(\cdot|s),\pi*\theta(\cdot|s)]] ] \le \delta $$</li><li><strong>Objective Function</strong>: Under this constraint, it maximizes an objective function using the advantage function $A_t$.
$$ \max*\theta \mathbb{E}<em>{s</em>t, a_t \sim \pi*{\theta*{old}}}\left[\frac{\pi*\theta(a<em>t|s_t)}{\pi</em>{\theta*{old}}(a_t|s_t)} A_t\right] $$
Here, $\frac{\pi*\theta(a<em>t|s_t)}{\pi</em>{\theta_{old}}(a_t|s_t)}$ is called the <strong>probability ratio</strong>, representing the ratio of action selection probabilities before and after the update.</li></ul><p>TRPO has theoretical guarantees but faces the challenge of complex implementation.</p><h3 id=proximal-policy-optimization-ppo>Proximal Policy Optimization (PPO)</h3><p>PPO is an algorithm that maintains TRPO&rsquo;s performance while simplifying the implementation. Instead of solving TRPO&rsquo;s complex constrained optimization problem, it introduces <strong>clipping</strong> to the objective function.</p><ul><li><strong>Clipping</strong>: The probability ratio $\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ is restricted to a certain range (e.g., $[1-\epsilon, 1+\epsilon]$). If it exceeds this range, the value is clipped.
$$ L^{CLIP}(\theta) = \mathbb{E}<em>{s_t, a_t \sim \pi</em>{\theta*{old}}}\left[\min\left(\frac{\pi*\theta(a<em>t|s_t)}{\pi</em>{\theta*{old}}(a_t|s_t)} A_t, \text{clip}\left(\frac{\pi*\theta(a<em>t|s_t)}{\pi</em>{\theta_{old}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon\right) A_t\right)\right] $$
Maximizing this objective function stabilizes policy updates and has been shown to achieve performance equal to or better than TRPO. PPO is one of the most widely used reinforcement learning algorithms today due to its ease of implementation and high performance.</li></ul><h2 id=references>References</h2><ul><li>Takahiro Kubo, &ldquo;Introduction to Reinforcement Learning with Python: From Basics to Practice&rdquo;, Shoeisha (2019)</li></ul></div><footer class=article-footer></footer><meta itemprop=wordCount content="579"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20210319_rl/7/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20210319_rl/6/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Advantage Actor-Critic (A2C)</span>
</a><a href=/en/posts/20210329_cem/1/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Cross-Entropy Method: A Practical Monte Carlo Optimization Technique</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/4/>Reinforcement Learning with Neural Networks</a></h3><div class=card-meta><time datetime=2021-03-22>March 22, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/6/>Advantage Actor-Critic (A2C)</a></h3><div class=card-meta><time datetime=2021-03-22>March 22, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/5/>Policy Gradient Methods</a></h3><div class=card-meta><time datetime=2021-03-22>March 22, 2021</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Key Algorithms in Deep Reinforcement Learning</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy;
2026
tomato blog.
All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Key Algorithms in Deep Reinforcement Learning","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/7\/","description":"A comprehensive overview of deep reinforcement learning algorithms including A3C, DDPG, DPG, TRPO, and PPO with their mechanisms and characteristics.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>