<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no"><meta name=robots content="
    index
  
  ,follow"><meta name=description content="A guide to model-free reinforcement learning covering exploration-exploitation trade-off, Monte Carlo methods, TD learning, Q-learning, SARSA, and Actor-Critic."><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20210319_rl/3/><meta property="og:type" content="
    article
  "><meta property="og:title" content="
    Model-Free Reinforcement Learning
  "><meta property="og:description" content="A guide to model-free reinforcement learning covering exploration-exploitation trade-off, Monte Carlo methods, TD learning, Q-learning, SARSA, and Actor-Critic."><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20210319_rl/3/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-03-19T12:00:23+09:00"><meta property="article:modified_time" content="2026-02-14T01:34:22+09:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="
    Model-Free Reinforcement Learning
  "><meta name=twitter:description content="A guide to model-free reinforcement learning covering exploration-exploitation trade-off, Monte Carlo methods, TD learning, Q-learning, SARSA, and Actor-Critic."><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Model-Free Reinforcement Learning |
tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20210319_rl/3/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20210319_rl/3/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/en/posts/20210319_rl/3/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Model-Free Reinforcement Learning","description":"A guide to model-free reinforcement learning covering exploration-exploitation trade-off, Monte Carlo methods, TD learning, Q-learning, SARSA, and Actor-Critic.","author":{"@type":"Person","name":""},"publisher":{"@type":"Organization","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-03-19T12:00:23\u002b09:00","dateModified":"2026-02-14T01:34:22\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/3\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/3\/","wordCount":639,"keywords":["Reinforcement Learning"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT3M"}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.de039aff98fb649abaf5fe23d3d4981f460573d7728709826e219c496c8e5be2.css integrity="sha256-3gOa/5j7ZJq69f4j09SYH0YFc9dyhwmCbiGcSWyOW+I=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c50b1500452fdb66f687cdcbf45a7774123c0111e1054e26db51fee604557b28.css integrity="sha256-xQsVAEUv22b2h83L9Fp3dBI8ARHhBU4m21H+5gRVeyg=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script async src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LN6QP6VVM3")</script><script data-ad-client=ca-pub-9558545098866170 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Model-Free Reinforcement Learning</h1><p class="lead article-description" itemprop=description>A guide to model-free reinforcement learning covering exploration-exploitation trade-off, Monte Carlo methods, TD learning, Q-learning, SARSA, and Actor-Critic.</p><div class=article-meta><time datetime=2021-03-19T12:00:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
March 19, 2021
</time><time datetime=2026-02-14T01:34:22+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 14, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
3 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/reinforcement-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Reinforcement Learning</a></div></header><div class=article-content itemprop=articleBody><p>Model-free reinforcement learning assumes that the environment model (transition function $T(s&rsquo;|s,a)$ and reward function $R(s,s&rsquo;)$) is <strong>unknown</strong>. The agent accumulates experience by interacting with the environment and learns optimal policies directly from that experience.</p><h2 id=exploration-exploitation-trade-off>Exploration-Exploitation Trade-off</h2><p>In reinforcement learning, the agent must balance between &ldquo;maximizing rewards using knowledge gained so far (<strong>exploitation</strong>)&rdquo; and &ldquo;trying unknown actions or states to gain new knowledge (<strong>exploration</strong>).&rdquo; This is called the &ldquo;exploration-exploitation trade-off.&rdquo;</p><p>If infinite actions were possible, one could explore sufficiently and then focus on exploitation. However, in most cases, the number of actions is limited. A common method for balancing this trade-off is the <strong>epsilon-Greedy method</strong>.</p><ul><li><strong>Epsilon-Greedy method</strong>:<ul><li>With probability epsilon, a random action is selected (exploration).</li><li>With probability 1-epsilon, the action considered best based on current knowledge is selected (exploitation).
It is common to set epsilon large in the early stages of learning to encourage exploration and gradually decrease it as learning progresses to emphasize exploitation.</li></ul></li></ul><h2 id=plan-correction-monte-carlo-and-td-methods>Plan Correction: Monte Carlo and TD Methods</h2><p>There are several methods for how and when the agent updates its value function and policy from experience.</p><h3 id=monte-carlo-method>Monte Carlo Method</h3><p>The Monte Carlo method updates values based on the total reward (return) actually obtained in an episode <strong>after one episode has ended</strong>.</p><p>$$ V(s<em>t) \leftarrow V(s_t) + \alpha (G_t - V(s_t)) $$
Here, $G_t = r</em>{t+1} + \gamma r*{t+2} + \gamma^2 r*{t+3} + \dots + \gamma^{T-t-1} r_T$ is the discounted return from time $t$ onward.</p><ul><li><strong>Characteristics</strong>: Since it uses actually obtained rewards, there is no bias. However, since updates cannot be performed until an episode ends, it is not suitable for problems with long episodes.</li></ul><h3 id=td-learning-temporal-difference-learning>TD Learning (Temporal-Difference Learning)</h3><p>TD learning updates values <strong>at each step</strong>. Updates are based on the difference (<strong>TD error</strong>) between the current value estimate and the one-step-ahead reward plus value estimate (TD target).</p><p>$$ V(s<em>t) \leftarrow V(s_t) + \alpha (r</em>{t+1} + \gamma V(s*{t+1}) - V(s_t)) $$
Here, $r*{t+1} + \gamma V(s_{t+1})$ is the TD target.</p><ul><li><strong>Characteristics</strong>: Since updates can be performed during an episode, learning can progress faster than the Monte Carlo method. However, since it uses estimates of future values, bias may arise from bootstrapping (self-reference).</li></ul><h3 id=tdlambda>TD($\lambda$)</h3><p>TD($\lambda$) is an intermediate method between the Monte Carlo method and TD learning. It calculates the TD error considering not only one step ahead but multiple steps ahead (n-step TD targets). By adjusting the parameter $\lambda$, it can continuously vary from the Monte Carlo method ($\lambda=1$) to TD learning ($\lambda=0$).</p><h2 id=value-based-and-policy-based>Value-Based and Policy-Based</h2><p>Model-free reinforcement learning, like model-based reinforcement learning, is classified by whether it focuses on updating value estimation or policy. The action value is denoted by $Q(s,a)$.</p><h3 id=value-based>Value-Based</h3><p>Learns the value function (especially the action-value function $Q(s,a)$) and selects actions based on that value function.</p><ul><li><strong>Q-Learning</strong>:
$$ Q(s<em>t, a_t) \leftarrow Q(s_t, a_t) + \alpha (r</em>{t+1} + \gamma \max*{a&rsquo;} Q(s*{t+1}, a&rsquo;) - Q(s_t, a_t)) $$
Q-learning uses the maximum action value at the next state, making it an <strong>off-policy</strong> method. This means the policy used for action selection during learning (e.g., epsilon-Greedy) differs from the optimal policy obtained through learning.</li></ul><h3 id=policy-based>Policy-Based</h3><p>Directly learns the policy itself (action probabilities at each state).</p><ul><li><strong>SARSA</strong>:
$$ Q(s<em>t, a_t) \leftarrow Q(s_t, a_t) + \alpha (r</em>{t+1} + \gamma Q(s*{t+1}, a*{t+1}) - Q(s<em>t, a_t)) $$
SARSA uses the action value of the action $a</em>{t+1}$ actually selected at the next state $s_{t+1}$, making it an <strong>on-policy</strong> method. This means the policy used for action selection during learning is the same as the policy obtained through learning.</li></ul><h3 id=actor-critic-method>Actor-Critic Method</h3><p>A method that combines value-based and policy-based approaches.</p><ul><li><strong>Actor</strong>: The part that learns the policy and selects actions.</li><li><strong>Critic</strong>: The part that learns the value function and evaluates the actions selected by the actor.</li></ul><p>The actor and critic learn by mutually updating information with each other.</p><h2 id=references>References</h2><ul><li>Takahiro Kubo, &ldquo;Introduction to Reinforcement Learning with Python: From Basics to Practice&rdquo;, Shoeisha (2019)</li></ul></div><footer class=article-footer></footer><meta itemprop=wordCount content="639"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20210319_rl/3/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20210319_rl/2/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Model-Based Reinforcement Learning</span>
</a><a href=/en/posts/20210319_rl/4/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Reinforcement Learning with Neural Networks</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/2/>Model-Based Reinforcement Learning</a></h3><div class=card-meta><time datetime=2021-03-18>March 18, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/1/>Reinforcement Learning Basics: Overview and Markov Decision Process</a></h3><div class=card-meta><time datetime=2021-03-18>March 18, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20200831_rl_map/1/>A Comprehensive Overview of Reinforcement Learning</a></h3><div class=card-meta><time datetime=2020-08-31>August 31, 2020</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Model-Free Reinforcement Learning</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy;
2026
tomato blog.
All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Model-Free Reinforcement Learning","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/3\/","description":"A guide to model-free reinforcement learning covering exploration-exploitation trade-off, Monte Carlo methods, TD learning, Q-learning, SARSA, and Actor-Critic.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>