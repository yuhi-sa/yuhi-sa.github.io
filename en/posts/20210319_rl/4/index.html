<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no"><meta name=robots content="
    index
  
  ,follow"><meta name=description content="Deep reinforcement learning techniques including Experience Replay, Fixed Target Q-Network, DQN, and its improvements like Double DQN, PER, and Rainbow."><meta name=author content="yuhi-sa"><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20210319_rl/4/><meta property="og:type" content="
    article
  "><meta property="og:title" content="
    Reinforcement Learning with Neural Networks
  "><meta property="og:description" content="Deep reinforcement learning techniques including Experience Replay, Fixed Target Q-Network, DQN, and its improvements like Double DQN, PER, and Rainbow."><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20210319_rl/4/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-03-22T10:00:23+09:00"><meta property="article:modified_time" content="2026-02-14T01:34:22+09:00"><meta property="article:tag" content="Reinforcement Learning"><meta property="article:tag" content="Deep Learning"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="
    Reinforcement Learning with Neural Networks
  "><meta name=twitter:description content="Deep reinforcement learning techniques including Experience Replay, Fixed Target Q-Network, DQN, and its improvements like Double DQN, PER, and Rainbow."><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Reinforcement Learning with Neural Networks |
tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://pagead2.googlesyndication.com crossorigin><link rel=dns-prefetch href=https://pagead2.googlesyndication.com><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20210319_rl/4/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20210319_rl/4/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/en/posts/20210319_rl/4/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Reinforcement Learning with Neural Networks","description":"Deep reinforcement learning techniques including Experience Replay, Fixed Target Q-Network, DQN, and its improvements like Double DQN, PER, and Rainbow.","author":{"@type":"Person","name":"yuhi-sa"},"publisher":{"@type":"Organization","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-03-22T10:00:23\u002b09:00","dateModified":"2026-02-14T01:34:22\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/4\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/4\/","wordCount":557,"keywords":["Reinforcement Learning","Deep Learning"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT3M"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"tomato blog","item":"https:\/\/yuhi-sa.github.io\/en\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/yuhi-sa.github.io\/en\/posts\/"},{"@type":"ListItem","position":3,"name":"Reinforcement Learning with Neural Networks"}]}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.de039aff98fb649abaf5fe23d3d4981f460573d7728709826e219c496c8e5be2.css integrity="sha256-3gOa/5j7ZJq69f4j09SYH0YFc9dyhwmCbiGcSWyOW+I=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.c50b1500452fdb66f687cdcbf45a7774123c0111e1054e26db51fee604557b28.css integrity="sha256-xQsVAEUv22b2h83L9Fp3dBI8ARHhBU4m21H+5gRVeyg=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script data-ad-client=ca-pub-9558545098866170 async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Reinforcement Learning with Neural Networks</h1><p class="lead article-description" itemprop=description>Deep reinforcement learning techniques including Experience Replay, Fixed Target Q-Network, DQN, and its improvements like Double DQN, PER, and Rainbow.</p><div class=article-meta><time datetime=2021-03-22T10:00:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
March 22, 2021
</time><time datetime=2026-02-14T01:34:22+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 14, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
3 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/reinforcement-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Reinforcement Learning
</a><a href=/en/tags/deep-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Deep Learning</a></div></header><div class=article-content itemprop=articleBody><p>In reinforcement learning, when the state space or action space is very large, it becomes difficult to represent the value function or policy in tabular form. In such cases, neural networks (NN) are used to approximate value functions and policies.</p><p>However, applying NNs to reinforcement learning introduces the challenge of unstable learning. Various techniques have been proposed to address this issue.</p><h2 id=key-techniques-for-stabilizing-learning>Key Techniques for Stabilizing Learning</h2><h3 id=1-experience-replay>1. Experience Replay</h3><p>The experience (state, action, reward, next state) obtained through the agent&rsquo;s interaction with the environment is stored in a memory called a <strong>replay buffer</strong>. During learning, mini-batches of experience are randomly sampled from this replay buffer to update the NN.</p><ul><li><strong>Benefits</strong>:<ul><li>Reduces correlations between experiences, improving learning stability.</li><li>Since the same experience can be used multiple times, data utilization efficiency is improved.</li></ul></li></ul><h3 id=2-fixed-target-q-network>2. Fixed Target Q-Network</h3><p>In Q-learning, this technique fixes the parameters of the Q-network used for calculating target values (target Q-values) for a certain period.</p><ul><li><strong>Problem</strong>: In standard Q-learning, the same network is used for both target Q-value calculation and Q-network updates, causing the target values to constantly fluctuate and destabilize learning.</li><li><strong>Benefit</strong>: By fixing the target Q-network parameters, the target values become stable, improving learning convergence. The target Q-network parameters are updated with the main Q-network parameters at regular intervals.</li></ul><h3 id=3-reward-clipping>3. Reward Clipping</h3><p>When the reward scale is very large or the reward distribution is skewed, this technique clips rewards to a fixed range (e.g., [-1, 1]).</p><ul><li><strong>Benefit</strong>: Prevents gradient instability caused by excessively large reward scales and stabilizes learning.</li></ul><h2 id=deep-q-network-dqn-and-its-improvements>Deep Q-Network (DQN) and Its Improvements</h2><p><strong>Deep Q-Network (DQN)</strong> is a groundbreaking method that combines Q-learning with deep learning (neural networks) and the above stabilization techniques (Experience Replay, Fixed Target Q-Network). Developed by Google DeepMind, it demonstrated superhuman performance in Atari games.</p><p>Since DQN&rsquo;s publication, various improvement methods have been proposed to further enhance performance. DeepMind also published <strong>Rainbow</strong>, a model that combines several of these improvements.</p><h3 id=main-dqn-improvements>Main DQN Improvements</h3><ol><li><p><strong>Double DQN (DDQN)</strong></p><ul><li><strong>Purpose</strong>: Suppresses overestimation of Q-values and improves value estimation accuracy.</li><li><strong>Mechanism</strong>: Separates the networks used for action selection and target Q-value calculation. Action selection is performed by the main Q-network, and the target Q-value of that action is calculated by the target Q-network.</li></ul></li><li><p><strong>Prioritized Experience Replay (PER)</strong></p><ul><li><strong>Purpose</strong>: Improves learning efficiency.</li><li><strong>Mechanism</strong>: Instead of randomly sampling experiences in Experience Replay, experiences with large TD errors (i.e., high learning potential) are sampled preferentially.</li></ul></li><li><p><strong>Dueling Network Architectures (Dueling DQN)</strong></p><ul><li><strong>Purpose</strong>: Improves value estimation accuracy.</li><li><strong>Mechanism</strong>: Adopts a network architecture that decomposes Q-values into &ldquo;State-Value&rdquo; and &ldquo;Advantage&rdquo; components. This enables more accurate evaluation of each action&rsquo;s value.</li></ul></li><li><p><strong>Multi-step Learning (N-step TD)</strong></p><ul><li><strong>Purpose</strong>: Improves value estimation accuracy.</li><li><strong>Mechanism</strong>: Instead of 1-step TD updates like Q-learning or SARSA, updates are performed using rewards and value estimates from n steps ahead. It has properties intermediate between Monte Carlo and TD methods.</li></ul></li><li><p><strong>Distributional RL (C51, QR-DQN, etc.)</strong></p><ul><li><strong>Purpose</strong>: Improves value estimation accuracy.</li><li><strong>Mechanism</strong>: Instead of estimating Q-values as single values, the distribution of rewards itself is learned. This allows action decisions based on richer information.</li></ul></li><li><p><strong>Noisy Nets</strong></p><ul><li><strong>Purpose</strong>: Improves exploration efficiency.</li><li><strong>Mechanism</strong>: Instead of manually setting epsilon as in epsilon-Greedy, noise is added to the network weights, allowing the agent itself to learn the degree of exploration. This automates the balance adjustment of exploration.</li></ul></li></ol><h2 id=references>References</h2><ul><li>Takahiro Kubo, &ldquo;Introduction to Reinforcement Learning with Python: From Basics to Practice&rdquo;, Shoeisha (2019)</li></ul></div><div class="ad-slot in-content my-3"><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9558545098866170></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=article-footer></footer><meta itemprop=wordCount content="557"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20210319_rl/4/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20210319_rl/3/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Model-Free Reinforcement Learning</span>
</a><a href=/en/posts/20210319_rl/5/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Policy Gradient Methods</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/3/>Model-Free Reinforcement Learning</a></h3><div class=card-meta><time datetime=2021-03-19>March 19, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/2/>Model-Based Reinforcement Learning</a></h3><div class=card-meta><time datetime=2021-03-18>March 18, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20210319_rl/1/>Reinforcement Learning Basics: Overview and Markov Decision Process</a></h3><div class=card-meta><time datetime=2021-03-18>March 18, 2021</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Reinforcement Learning with Neural Networks</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy;
2026
yuhi-sa.
All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Reinforcement Learning with Neural Networks","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20210319_rl\/4\/","description":"Deep reinforcement learning techniques including Experience Replay, Fixed Target Q-Network, DQN, and its improvements like Double DQN, PER, and Rainbow.","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>