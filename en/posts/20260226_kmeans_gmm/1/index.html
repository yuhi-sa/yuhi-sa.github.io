<!doctype html><html lang=en dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#e74c3c"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="From k-means convergence guarantees to GMM with EM algorithm, scikit-learn implementation, and cluster number selection (elbow, silhouette, BIC)."><meta name=author content="yuhi-sa"><link rel=canonical href=https://yuhi-sa.github.io/en/posts/20260226_kmeans_gmm/1/><meta property="og:type" content="article"><meta property="og:title" content="k-means and GMM: Clustering Theory and Python Implementation"><meta property="og:description" content="From k-means convergence guarantees to GMM with EM algorithm, scikit-learn implementation, and cluster number selection (elbow, silhouette, BIC)."><meta property="og:url" content="https://yuhi-sa.github.io/en/posts/20260226_kmeans_gmm/1/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="en"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2026-02-26T20:00:00+09:00"><meta property="article:modified_time" content="2026-02-26T23:47:37+09:00"><meta property="article:tag" content="Machine Learning"><meta property="article:tag" content="Python"><meta property="article:tag" content="Algorithm"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="k-means and GMM: Clustering Theory and Python Implementation"><meta name=twitter:description content="From k-means convergence guarantees to GMM with EM algorithm, scikit-learn implementation, and cluster number selection (elbow, silhouette, BIC)."><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>k-means and GMM: Clustering Theory and Python Implementation | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://pagead2.googlesyndication.com crossorigin><link rel=dns-prefetch href=https://pagead2.googlesyndication.com><link rel=icon href=https://yuhi-sa.github.io/favicon.ico><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20260226_kmeans_gmm/1/><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20260226_kmeans_gmm/1/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/en/posts/20260226_kmeans_gmm/1/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"k-means and GMM: Clustering Theory and Python Implementation","description":"From k-means convergence guarantees to GMM with EM algorithm, scikit-learn implementation, and cluster number selection (elbow, silhouette, BIC).","author":{"@type":"Person","name":"yuhi-sa","url":"https:\/\/yuhi-sa.github.io\/"},"publisher":{"@type":"Organization","name":"tomato blog","logo":{"@type":"ImageObject","url":"https:\/\/yuhi-sa.github.io\/ogp.jpeg"},"url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2026-02-26T20:00:00\u002b09:00","dateModified":"2026-02-26T23:47:37\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/en\/posts\/20260226_kmeans_gmm\/1\/"},"url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20260226_kmeans_gmm\/1\/","wordCount":568,"keywords":["Machine Learning","Python","Algorithm"],"articleSection":"Posts","inLanguage":"en","timeRequired":"PT3M"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"tomato blog","item":"https:\/\/yuhi-sa.github.io\/en\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/yuhi-sa.github.io\/en\/posts\/"},{"@type":"ListItem","position":3,"name":"k-means and GMM: Clustering Theory and Python Implementation"}]}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.72a177faa7b12de55dcc39c4ab6b6392116718d5e2735dab0214511354ecb973.css integrity="sha256-cqF3+qexLeVdzDnEq2tjkhFnGNXic12rAhRRE1TsuXM=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.403a82b29e94ee6e7a39204b5082cdf8b4966cfb44115390da7d8867a5006acd.css integrity="sha256-QDqCsp6U7m56OSBLUILN+LSWbPtEEVOQ2n2IZ6UAas0=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>window.MathJax={tex:{inlineMath:[["\\(","\\)"]],displayMath:[["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/en/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script data-ad-client=ca-pub-9558545098866170 async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>k-means and GMM: Clustering Theory and Python Implementation</h1><p class="lead article-description" itemprop=description>From k-means convergence guarantees to GMM with EM algorithm, scikit-learn implementation, and cluster number selection (elbow, silhouette, BIC).</p><div class=article-meta><time datetime=2026-02-26T20:00:00+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
February 26, 2026
</time><time datetime=2026-02-26T23:47:37+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 26, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
3 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/en/tags/machine-learning/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Machine Learning
</a><a href=/en/tags/python/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Python
</a><a href=/en/tags/algorithm/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Algorithm</a></div></header><div class=article-content itemprop=articleBody><h2 id=introduction>Introduction</h2><p>Clustering is a fundamental unsupervised learning task that partitions unlabeled data into groups. This article covers <strong>k-means</strong>, the most widely used method, and <strong>Gaussian Mixture Models (GMM)</strong>, a probabilistic approach.</p><h2 id=k-means-algorithm>k-means Algorithm</h2><h3 id=algorithm>Algorithm</h3><ol><li>Initialize \(k\) centroids \(\boldsymbol{\mu}_1, \ldots, \boldsymbol{\mu}_k\) randomly</li><li><strong>Assignment step</strong>: Assign each point \(\mathbf{x}_i\) to the nearest centroid</li></ol>\[c_i = \arg\min_{j} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|^2 \tag{1}\]<ol start=3><li><strong>Update step</strong>: Recompute centroids</li></ol>\[\boldsymbol{\mu}_j = \frac{1}{|C_j|} \sum_{i \in C_j} \mathbf{x}_i \tag{2}\]<ol start=4><li>Repeat 2-3 until convergence</li></ol><h3 id=objective-function>Objective Function</h3><p>k-means minimizes the inertia:</p>\[J = \sum_{j=1}^{k} \sum_{i \in C_j} \|\mathbf{x}_i - \boldsymbol{\mu}_j\|^2 \tag{3}\]<p>\(J\) monotonically decreases at each step, guaranteeing convergence in finite iterations. However, global optimality is not guaranteed due to initialization dependence.</p><h3 id=k-means>k-means++</h3><p>Selects initial centroids with probability proportional to <strong>squared distance</strong> from existing centroids, mitigating the initialization problem. This is scikit-learn&rsquo;s default.</p><h2 id=gaussian-mixture-model-gmm>Gaussian Mixture Model (GMM)</h2><h3 id=model-definition>Model Definition</h3><p>A mixture of \(k\) Gaussian distributions:</p>\[p(\mathbf{x}) = \sum_{j=1}^{k} \pi_j \mathcal{N}(\mathbf{x} | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j) \tag{4}\]<p>where \(\pi_j\) are mixing coefficients (\(\sum_j \pi_j = 1\)) and \(\boldsymbol{\Sigma}_j\) are covariance matrices.</p><h3 id=em-algorithm>EM Algorithm</h3><p><strong>E-step</strong>: Compute responsibilities</p>\[\gamma_{ij} = \frac{\pi_j \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)}{\sum_{l=1}^{k} \pi_l \mathcal{N}(\mathbf{x}_i | \boldsymbol{\mu}_l, \boldsymbol{\Sigma}_l)} \tag{5}\]<p><strong>M-step</strong>: Update parameters</p>\[\boldsymbol{\mu}_j = \frac{\sum_i \gamma_{ij} \mathbf{x}_i}{\sum_i \gamma_{ij}}, \quad \boldsymbol{\Sigma}_j = \frac{\sum_i \gamma_{ij} (\mathbf{x}_i - \boldsymbol{\mu}_j)(\mathbf{x}_i - \boldsymbol{\mu}_j)^T}{\sum_i \gamma_{ij}} \tag{6}\]\[\pi_j = \frac{\sum_i \gamma_{ij}}{n} \tag{7}\]<h3 id=k-means-vs-gmm>k-means vs GMM</h3><table><thead><tr><th>Feature</th><th>k-means</th><th>GMM</th></tr></thead><tbody><tr><td>Assignment</td><td>Hard (0 or 1)</td><td>Soft (probability)</td></tr><tr><td>Cluster shape</td><td>Spherical</td><td>Ellipsoidal (arbitrary covariance)</td></tr><tr><td>Objective</td><td>Inertia</td><td>Log-likelihood</td></tr><tr><td>Computation</td><td>Fast</td><td>Slower</td></tr></tbody></table><h2 id=python-implementation>Python Implementation</h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.datasets</span> <span class=kn>import</span> <span class=n>make_blobs</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.cluster</span> <span class=kn>import</span> <span class=n>KMeans</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.mixture</span> <span class=kn>import</span> <span class=n>GaussianMixture</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.preprocessing</span> <span class=kn>import</span> <span class=n>StandardScaler</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>y_true</span> <span class=o>=</span> <span class=n>make_blobs</span><span class=p>(</span><span class=n>n_samples</span><span class=o>=</span><span class=mi>500</span><span class=p>,</span> <span class=n>centers</span><span class=o>=</span><span class=p>[[</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>4</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>                        <span class=n>cluster_std</span><span class=o>=</span><span class=p>[</span><span class=mf>0.8</span><span class=p>,</span> <span class=mf>1.2</span><span class=p>,</span> <span class=mf>0.6</span><span class=p>],</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>scaler</span> <span class=o>=</span> <span class=n>StandardScaler</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>scaler</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- k-means ---</span>
</span></span><span class=line><span class=cl><span class=n>kmeans</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>init</span><span class=o>=</span><span class=s1>&#39;k-means++&#39;</span><span class=p>,</span> <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>km_labels</span> <span class=o>=</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>fit_predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- GMM ---</span>
</span></span><span class=line><span class=cl><span class=n>gmm</span> <span class=o>=</span> <span class=n>GaussianMixture</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>3</span><span class=p>,</span> <span class=n>covariance_type</span><span class=o>=</span><span class=s1>&#39;full&#39;</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gmm</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gmm_labels</span> <span class=o>=</span> <span class=n>gmm</span><span class=o>.</span><span class=n>predict</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>gmm_probs</span> <span class=o>=</span> <span class=n>gmm</span><span class=o>.</span><span class=n>predict_proba</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- Visualization ---</span>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>15</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>km_labels</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>15</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>kmeans</span><span class=o>.</span><span class=n>cluster_centers_</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>kmeans</span><span class=o>.</span><span class=n>cluster_centers_</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>                <span class=n>c</span><span class=o>=</span><span class=s1>&#39;red&#39;</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;X&#39;</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>200</span><span class=p>,</span> <span class=n>edgecolors</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;k-means&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>gmm_labels</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>,</span> <span class=n>s</span><span class=o>=</span><span class=mi>15</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.6</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;GMM (hard)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>uncertainty</span> <span class=o>=</span> <span class=mi>1</span> <span class=o>-</span> <span class=n>gmm_probs</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>gmm_labels</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;viridis&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>s</span><span class=o>=</span><span class=mi>15</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mi>1</span> <span class=o>-</span> <span class=n>uncertainty</span> <span class=o>*</span> <span class=mf>0.8</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>axes</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;GMM (soft: uncertainty)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>ax</span> <span class=ow>in</span> <span class=n>axes</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;Feature 1&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;Feature 2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=cluster-number-selection>Cluster Number Selection</h2><h3 id=elbow-method>Elbow Method</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>inertias</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>K_range</span> <span class=o>=</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>K_range</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>km</span> <span class=o>=</span> <span class=n>KMeans</span><span class=p>(</span><span class=n>n_clusters</span><span class=o>=</span><span class=n>k</span><span class=p>,</span> <span class=n>n_init</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>km</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>inertias</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>km</span><span class=o>.</span><span class=n>inertia_</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>K_range</span><span class=p>,</span> <span class=n>inertias</span><span class=p>,</span> <span class=s1>&#39;bo-&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;k&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Inertia&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;Elbow Method&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><h3 id=silhouette-score>Silhouette Score</h3>\[s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))} \tag{8}\]<p>where \(a(i)\) is the mean intra-cluster distance and \(b(i)\) is the mean nearest-cluster distance. \(s(i) \in [-1, 1]\); closer to 1 indicates better clustering.</p><h3 id=bic-for-gmm>BIC (for GMM)</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>bics</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl><span class=n>K_range</span> <span class=o>=</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>K_range</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>gm</span> <span class=o>=</span> <span class=n>GaussianMixture</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=n>k</span><span class=p>,</span> <span class=n>random_state</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>gm</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>bics</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>gm</span><span class=o>.</span><span class=n>bic</span><span class=p>(</span><span class=n>X</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>K_range</span><span class=p>,</span> <span class=n>bics</span><span class=p>,</span> <span class=s1>&#39;ro-&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;k&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;BIC&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;BIC for GMM&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>grid</span><span class=p>(</span><span class=kc>True</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><p>Select \(k\) that minimizes BIC.</p><h2 id=related-articles>Related Articles</h2><ul><li><a href=https://yuhi-sa.github.io/en/posts/20260226_svm/1/>Support Vector Machines (SVM)</a> - Comparison with supervised classification; SVM finds boundaries while clustering discovers data structure.</li><li><a href=https://yuhi-sa.github.io/en/posts/20210108_bayes/1/>Bayesian Linear Regression Fundamentals</a> - Bayesian foundations underlying GMM.</li><li><a href=https://yuhi-sa.github.io/en/posts/20260226_mcmc/1/>Markov Chain Monte Carlo (MCMC)</a> - MCMC can also estimate GMM parameters.</li><li><a href=https://yuhi-sa.github.io/en/posts/20260226_sgd_adam/1/>SGD to Adam: Gradient Descent Theory and Implementation</a> - Understanding the relationship between EM and gradient methods.</li></ul><h2 id=references>References</h2><ul><li>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer. Chapters 9.</li><li>MacQueen, J. (1967). &ldquo;Some methods for classification and analysis of multivariate observations&rdquo;. <em>Proceedings of the 5th Berkeley Symposium</em>.</li><li><a href=https://scikit-learn.org/stable/modules/clustering.html>scikit-learn Clustering documentation</a></li></ul></div><div class="ad-slot in-content my-3"><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9558545098866170></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=article-footer></footer><meta itemprop=wordCount content="568"><meta itemprop=url content="https://yuhi-sa.github.io/en/posts/20260226_kmeans_gmm/1/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/en/posts/20260226_ensemble_learning/1/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>Ensemble Learning: From Decision Trees to Random Forest and Gradient Boosting</span>
</a><a href=/en/posts/20260226_sgd_adam/1/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>From SGD to Adam: Evolution of Gradient-Based Optimization</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260226_svm/1/>Support Vector Machines (SVM): Kernel Methods and Nonlinear Classification with Python</a></h3><div class=card-meta><time datetime=2026-02-26>February 26, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260226_ensemble_learning/1/>Ensemble Learning: From Decision Trees to Random Forest and Gradient Boosting</a></h3><div class=card-meta><time datetime=2026-02-26>February 26, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/en/posts/20260228_self_attention/1/>Understanding Self-Attention from Scratch: Math and Python Implementation</a></h3><div class=card-meta><time datetime=2026-02-28>February 28, 2026</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/en/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/en/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/en/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>k-means and GMM: Clustering Theory and Python Implementation</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2026 yuhi-sa. All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"k-means and GMM: Clustering Theory and Python Implementation","url":"https:\/\/yuhi-sa.github.io\/en\/posts\/20260226_kmeans_gmm\/1\/","description":"From k-means convergence guarantees to GMM with EM algorithm, scikit-learn implementation, and cluster number selection (elbow, silhouette, BIC).","inLanguage":"en","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>