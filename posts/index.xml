<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on とまとまとブログ</title>
    <link>https://yuhi-sa.github.io/posts/</link>
    <description>Recent content in Posts on とまとまとブログ</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 25 Jan 2021 15:17:23 +0900</lastBuildDate><atom:link href="https://yuhi-sa.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Unscented Transformation(アンセンテッド変換,U変換)：非線形変換後の確率変数の推定</title>
      <link>https://yuhi-sa.github.io/posts/20210125/</link>
      <pubDate>Mon, 25 Jan 2021 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20210125/</guid>
      <description>はじめに Unscented Transformation(アンセンテッド変換,U変換)は，Unscented Kalman Filterの中で出てくるけど，U変換単体での説明がなくて困ってるので今の理解をまとめました． 間違ってる所など教えていただけるととてもありがたいです．
Unscented Transformationの概要 U変換は，標準正規分布に従う確率変数$x$の平均$\bar{x}$と分散共分散行列$P_x$が既知であるとき，$x$の非線形変換$y=f(x)$で変換される確率変数$y$の$\bar{y}$と$P_y$を推定する方法である．
まず，この場合，モンテカロ的に
$$ \bar{y}\simeq\frac{1}{N}\sum_{i=1}^Nf(x_i) $$
$$ P_y \simeq \frac{1}{N}\sum_{i=1}^N(f(x_i)-\bar{y})(f(x_i)-\bar{y})^T $$
のように計算することが思いつくが，精度をよく計算するにはNを大きくする必要があり，実用上問題がある．
線形近似をすることなく，モンテカロ法のよいところを利用できるように，できるだけ少ないサンプル点を用いて，変換後の確率変数の統計的性質を推定する方法がU変換である．
まず，確率変数$x$からサンプルする値(シグマ点)を決め，シグマ点を非線形変換し，変換した値から$y$の$\bar{y}$と$P_y$を求める．
参考1：UKF （Unscented Kalman Filter）っ て何 ？
import matplotlib.pyplot as plt import numpy as np import random import math import scipy.linalg 入力には$X=(X_1,X_2)$を用いる．
平均ベクトルは，
$$ \mu=(E[X_1],E[X_2])=(\bar{x}_1,\bar{x}_2) $$ 分散共分散ベクトルは，
$$ P_x = [ \begin{array}{cc} var[X_1] &amp;amp; cov[X_1,X_2] \
cov[X_2,X_1] &amp;amp; var[X_2] \end{array} ] = [ \begin{array}{cc} \sigma_1^2 &amp;amp; \sigma_1\sigma_2 \
\sigma_1\sigma_2 &amp;amp; \sigma_2^2 \end{array} ] $$</description>
    </item>
    
    <item>
      <title>遺伝的アルゴリズム(GA)を用いたニューラルネットワークの学習</title>
      <link>https://yuhi-sa.github.io/posts/20210109/</link>
      <pubDate>Sat, 09 Jan 2021 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20210109/</guid>
      <description>はじめに $$ f(x,y)=\frac{(\frac{\sin x^2}{cos y}+x^2-5y+30)}{80} $$ の関数を遺伝的アルゴリズムを用いてニューラルネットワークを学習させる． (学部のときの課題で，できなかったのでリベンジしてみた)
遺伝的アルゴリズム(GA:Genetic Algorithms)とは GAはHollandによって開発された，生物の進化のメカニズムを模倣した最適解探索のプログラムである．有性生殖をする生物の進化の課程の中で，環境に適応できる個体ほど次世代に自分の遺伝子を残すことができ，2個体の交叉により子をつくる，また稀に突然変異がおこるという特徴に着目をしている．決定的な優れた厳密解法が発見されておらず，全探索が不可能と考えられるほど広大な解空間をもつ問題に有効とされており，様々な最適化問題に応用可能であり，今後もさらなる発展が期待される．
GAのアルゴリズム GAのアルゴリズムを以下に示す．
 個体の初期生成： 初期の個体をランダムに生成する． 適合度(評価値)の算出 各個体の適合度(評価値)決定する． 再生 各個体の適合度に依存した個体の再生を行う．適合度の高い個体は増殖し，低い個体は淘汰される． 交叉 Step3で選択された個体群からランダムに選択された個体のペアから，新しい個体を生成する．これを既定回数繰り返す． 突然変異 突然変異確率に基づいて，各個体の遺伝子の一部をランダムに書き換える． 終了条件判定 終了条件を満たせば終了，そうでなければStep2に戻る．  GAアルゴリズムの性質 GAでは再生により評価値の高い粒子を重点的に探索すると同時に，交叉と突然変異により広範囲に解を探索するため，これらの遺伝的操作が有効に動作すれば良好な解が発見されることが期待される．しかし，遺伝的操作により，最良解の情報を失い，局所解に陥ってしまうことがあり最適解をうまく発見できない場合がしばしばある．特に，制御パラメータの解空間は多数の局所解が存在しているため，交叉の操作は不向きであるといえる．また，GAは決定変数が離散な値であることを前提とした解探索手法であるが，制御パラメータの最適化のための解情報は連続値であることから解の探索に最適ではないことも考えられる．
実装 遺伝子はニューラルネットワークの重みと閾値から構成されるものとして，1つのニューラルネットワークが1つの個体に対応するように設計を行い，上記のGAの手法を用いて更新を行った． コードはこちら
パラメータなどを設定 # 世代 GEN = 100 # NNの個数 In = 2 Hidden = 2 Out = 1 # NNの個体数 Number = 1000 # 教師信号の数 Num = 1000 # 交叉確率 kousa = 0.8 # 突然変異確率 change = 0.05 # 学習する関数 def kansu(x): return((math.</description>
    </item>
    
    <item>
      <title>ベイズ推定に基づく線形回帰(最小二乗推定，最尤推定，MAP推定，ベイズ推定)</title>
      <link>https://yuhi-sa.github.io/posts/20210108/</link>
      <pubDate>Fri, 08 Jan 2021 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20210108/</guid>
      <description>#背景 回帰問題の目的は，$N$個の観測値と対応する目標値からなる訓練データ集合が与えられたとき，新しい観測値に対する目標値の値を予測することである．今回扱う線形回帰モデルは，多項式は調節可能なパラメータの線形結合という特徴を利用した最も単純なモデルである．固定された基底関数の入力変数に関して非線型な関数の固定された集合結合をとることにより，有用な関数のクラスが得られる．
観測されたデータ$D={(x_i,y_i);i=1,2,&amp;hellip;,n}$に対して，基底関数の線形結合に基づく回帰関数モデルを以下のように定義する．ここで$\Phi$を$x$の基底関数，$\epsilon$を誤差項とする．
$$ y_i= \Phi w+ \epsilon $$
ベイズ線形回帰について 最小二乗推定 最小二乗推定は，回帰モデルによる予測誤差の二乗和$S(w)$を最小化する$\hat{w}$を求める手法である．$S(w)$を$w$で偏微分し，$\hat{w}$を求める．
$$ S(w)=\epsilon^{T}\epsilon=(y-\Phi w)^T(y-\Phi w) $$
$$ \frac{dS(w)}{dw}=-\Phi^{T}y+\Phi^T\Phi w $$ $\frac{dS(w)}{dw}=0$のときを考えると，
$$ \hat{w}=(\Phi^T\Phi)^{-1}\Phi^{T}y $$ 従って，最小二乗推定による予測モデルは以下のようになる．
$$ \hat{y}=\Phi\hat{w}=\Phi(\Phi^T\Phi)^{-1}y $$
最尤推定 最尤推定は，尤度$P(y,w)$を最大化する$\hat{w}$を求める手法である．誤差項に正規分布を仮定したモデルを考える．このとき観測値$y$は平均$\Phi w$，分散行列$\sigma^2I_n$のn次元正規分布に従う．よって尤度は，以下のように与えられる．
$$ y= \Phi w+ \epsilon,\epsilon \sim \mathcal{N}(0,\sigma^2I_n) $$ $$ P(y\mid w,\sigma^2)=\mathcal{N}(\Phi w,\sigma^2I_n) =\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}exp{-\frac{1}{2\sigma^2}(y-\Phi w)^T(y-\Phi w)} $$
$P(y\mid w)$の対数を$w$で偏微分し，$\hat{w}$を求める．
$$ \log P(y\mid w) = -\frac{n}{2}\log(2\pi\sigma^2)-\frac{(y-\Phi w)^T(y-\Phi w)}{2\sigma^2} $$
$$ \frac{1}{P(y\mid w)}\frac{P(y\mid w)}{dw}=-(\Phi^{T}y+\Phi^{T}\Phi w) $$ $\frac{dP(y\mid w)}{dw}=0$のときを考えると．
$$ \hat{w}=(\Phi^T\Phi)^{-1}\Phi^{T}y $$</description>
    </item>
    
    <item>
      <title>情報理論(エントロピーから相互情報量, PRML1.6)</title>
      <link>https://yuhi-sa.github.io/posts/20210107_2/</link>
      <pubDate>Thu, 07 Jan 2021 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20210107_2/</guid>
      <description>情報量の表し方 情報量を測る尺度は確率分布$p(x)$に依存しており，確率$p(x)$の単調な関数$h(x)$で表されるものを考える． 情報量として用いる場合，以下の2点を満たしている必要がある．
 $h(x,y)=h(x)+h(y)$ 2つの事象$x,y$が無関係なら，両方を観測したときの情報が，それぞれを別々に観測した情報の和となる． $p(x,y)=p(x)p(y)$ 2つの無関係な事象は統計的に独立である．  この2つの関係から，対数を用いる．$p(x)$は1以下となるためマイナス記号を加え情報が0以上であることを保証する．
$$ h(x)=-log_2p(x) $$ 情報理論では一般的に底2が用いられる．
エントロピー ある送信者が確率変数を受信者に送りたいと考えた時，送られる情報の平均量は，分布$p(x)$の期待値を撮ったものとなり，これを確率変数$x$のエントロピーと呼ぶ．
$$ H[x]=-\sum_x p(x) \log_{2}p(x) $$
ビット数の下限 ある確率変数$x$が8個の{$a,b,c,d,e,f,g,h$}を送信する場合を考える．
 8個それぞれの確率が等確率であるとする．  $$ H[x]= -8*\frac{1}{8}\log_{2}\frac{1}{8}=3ビット $$
 8個それぞれの確率が{$\frac{1}{2},\frac{1}{4},\frac{1}{8},\frac{1}{16},\frac{1}{64},\frac{1}{64},\frac{1}{64},\frac{1}{64},$}で与えられるとする．  $$ H[x]=-\frac{1}{2}\log_2\frac{1}{2}-\frac{1}{4}\log_2\frac{1}{4}-\frac{1}{8}\log_2\frac{1}{8}-\frac{1}{16}\log_2\frac{1}{16}-\frac{4}{64}\log_2\frac{1}{64}=2ビット $$
上記の例2のように非一様な場合は，よく起きる事象に短い符号を使い，あまり起きない事象に長い符号を割り当てることにより効率よく通信を行うことができる． ノイズなし符号化定理では，確率変数の状態を送るために必要なビット数の下限がエントロピーであることを主張している．
エントロピーの高低 少ない値で鋭いピークを持つようば分布でエントロピーは低く，薄く広がってる分布はエントロピーが高い．
最大のエントロピーを持つ確率分布 まず，エントロピーの二階微分二階微分を計算する．
$$ \frac{\delta H[x]}{\delta p(x_i)\delta p(x_j)}=-I_{ij}\frac{1}{p_i} $$ $I$は単位行列である．二階微分が負であるため上に凸なグラフとなり．停留点が最大値であることがわかる．
微分エントロピー $x$を等間隔の区間$\Delta$にわけることを考えると，平均値の定理より，以下の式を満たす$x_i$が存在する．
$$ \int_{i\Delta}^{(i+1)\Delta}p(x)dx = p(x_i)\Delta $$ $x_i$の値を観測する確率は，$p(x_i)\Delta$となる．よって離散分布のエントロピーは，以下のようになる．
$$ H_{\Delta}=-\sum_ip(x_i)\Delta\ln(p(x_i)\Delta)= -\sum_i p(x_i)\Delta\ln p(x_i)- \ln \Delta $$ 第二項を無視して$\Delta\rightarrow 0$の極限を考える．
$$ \lim_{\Delta\rightarrow 0}{ -\sum_i p(x_i)\Delta\ln p(x_i) } = -\int p(x)\ln p(x)dx $$ 右辺が微分エントロピーとなる．</description>
    </item>
    
    <item>
      <title>粒子群最適化(PSO)とTCPSO</title>
      <link>https://yuhi-sa.github.io/posts/20210107/</link>
      <pubDate>Thu, 07 Jan 2021 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20210107/</guid>
      <description>粒子群最適化(PSO:Particle Swarm Optimization) PSOは，Kennedy and Eberhartによって開発された，魚や鳥などにみられる群行動の餌の在り処の情報が群全体に伝達され，他の個体の行動に合わせて速度を調整しながらすべての個体が一斉に行動しているという点に着目して提案された最適解探索のアルゴリズムである．
PSOでは，位置情報と速度情報を持った粒子(個体)が最適解を求めて探索空間を探索する．各粒子は自分自身の過去の情報と周りの粒子の過去の情報を互いに共有しながら解を探索する．粒子の更新が単純な概念から容易に実装でき，さらに多様な改良が可能である．
粒子の更新 粒子$i,i=1,2,…,n$は時刻$t$における位置情報ベクトル$x_i(t)=(x_i1,x_i2,…,x_m)$と速度ベクトル$v_i (t)=(v_i1,v_i2,…,v_m)$の情報を持っている．
このとき，$n$は粒子数，$m$は次元数である．粒子$i$が各時刻まで探索した解のうち最良の位置情報ベクトルを$P_i(t)$として保持し，粒子群の中で最良の粒子の位置情報ベクトルを$P_G(t)$として群全体で共有する．
以下に位置情報ベクトルと速度ベクトルの更新式を示す．
$$ v_i(t+1)=w{v}_i(t)+C_1r_1(P_i(t)-m{x}_i(t))+C_2r_2(P_G(t)-x_i(t)) $$
$$ x_i(t+1)=x_i(t)+{v}_i(t+1) $$
$w$は慣性項に対する重み，$C_1$は自身の最良位置に対する重み，$C_2$は全体の最良位置に対する重みである．
また，$r_1,r_2∈[0,1]$は各次元ごとに毎期生成される一様分布に従ってランダムに決定される実数値である．
更新の概念図を以下に示す． アルゴリズム PSOのアルゴリズムを以下に示す．
 粒子の初期生成 初期の粒子の位置情報ベクトル$x_i$と速度ベクトル$v_i$を乱数に基づいて生成する． 評価値の算出 各粒子の位置から評価値を決定する． 探索開始($t=0$)から現在までの各粒子$i$の最良位置 情報$P_G (t)$の更新，各粒子ごとに，現在までの最良の評価値を記録する． 群全体の最良位置情報$P_G (t)$の更新 全粒子の中から，最良の評価値を記憶する． 各粒子の速度を計算 各粒子の位置を計算 終了条件判定 終了条件を満たすまで，Step2からStep6を繰り返す．  アルゴリズムの性質 単純な算術演算により構成され，パラメータ$w，C_1，C_2$への依存性が高いことや，各粒子が群全体の最良位置情報$P_G (t)$の位置に集中してしまうことで局所解に陥り，十分な探索ができないといった問題が存在する．収束速度と多様性の維持のバランスが難しい．
以下に，粒子数100個をランダムに生成し，$(x,y)=(0,0)$からの距離の逆数を評価値としてPSOを行った例を示す． コードはこちら
 $t=0$のとき  $t=250$のとき  $t=500$のとき   収束は早いが，局所解に陥っていることがわかる．また，慣性項の影響で集中的に探索することが難しくなっている．
TCPSO(Two-swarm Cooperative Particle Swarm Optimization) TCPSOでは大域的な探索を行うマスター粒子群と集中的な探索を行うスレーブ粒子群を用いることで，PSOの収束速度と多様性の問題を改善する．2つの粒子群がお互いに情報を共有しながら各粒子が位置と速度の更新を行う．性質の異なる粒子群を用いることで大域探索と集中探索を行うことが可能になり，効率的な解探索が可能になる．
粒子の更新 スレーブ粒子を添字$S$，マスター粒子を添字$M$で表す．以下に位置情報ベクトルと速度ベクトルの更新式を示す．
$$ v_i^S(t+1)=C_1^Sr_1(P_i^S(t)-x_i^S(t))+C_2^Sr_2(P_G(t)-x_i^S(t)) $$ $$ x_i^S(t+1)=x_i^S(t)+v_i^S(t+1) $$ $$ v_i^M(t+1)=w^Mv_i^M(t)C_1^Mr_1(P_i^M(t)-x_i^M(t)) +C_2^Mr_2(P_G^S(t)-x_i^M(t))+C_3^Mr_3(P_G(t)-x_i^M(t)) $$ $$ x_i^M(t+1)=x_i^M(t)+v_i^M(t+1) $$ マスター粒子の速度の更新には，スレーブ粒子群の全体の最良位置$P_G^S (t)$が追加されており，スレーブ群の情報を更新することで多様性を保持しながら大域的な探索をすることが可能になっている． スレーブ粒子とマスター粒子の更新の概念を以下に示す．</description>
    </item>
    
    <item>
      <title>Pythonでslackに実験結果(テキストと画像)を送る</title>
      <link>https://yuhi-sa.github.io/posts/20201223/</link>
      <pubDate>Wed, 23 Dec 2020 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20201223/</guid>
      <description>テキストを送る slack側のAPIを設定  slackの「設定と管理」から「アプリを管理する」を開く．  AppディレクトリIncoming Webhookを検索して追加する．  チャンネルを選択して，「Incoming Webhookインテグレーションの追加」  「Webhook URL」をコピーする．  botのアイコンと名前をここで変更できる．   コード import slackweb slack = slackweb.Slack(url=&amp;#34;コピーした Webhook URL&amp;#34;) def notify(title, text, color): attachments = [{&amp;#34;title&amp;#34;: title, &amp;#34;text&amp;#34;: text, &amp;#34;color&amp;#34;: color, #good, warning, danger &amp;#34;footer&amp;#34;: &amp;#34;Send from Python&amp;#34;, }] slack.notify(text=None, attachments=attachments) notify(&amp;#34;テスト&amp;#34;,&amp;#34;おはよう&amp;#34;,&amp;#34;good&amp;#34;) notify(&amp;#34;テスト&amp;#34;,&amp;#34;こんにちは&amp;#34;,&amp;#34;warning&amp;#34;) notify(&amp;#34;テスト&amp;#34;,&amp;#34;こんばんは&amp;#34;,&amp;#34;danger&amp;#34;) attachmentsの構文は，以下を参考
 Creating rich message layouts Slack API attachmentsチートシート  実行結果 画像を送る  slack側のAPIを設定   上記の1から2と同様にして「Bots」を追加する． API トークンをコピーする．  コード import requests import json def notifyImg(title, imageURL): files = {&amp;#39;file&amp;#39;: open(imageURL, &amp;#39;rb&amp;#39;)} param = { &amp;#39;token&amp;#39;: &amp;#34;コピーした APIトークン&amp;#34;, &amp;#39;channels&amp;#39;:&amp;#39;投稿したいチャンネル名&amp;#39;, &amp;#39;filename&amp;#39;:&amp;#34;filename&amp;#34;, &amp;#39;title&amp;#39;: title, } requests.</description>
    </item>
    
    <item>
      <title>楕円曲線上のElGamal暗号</title>
      <link>https://yuhi-sa.github.io/posts/20201223_2/</link>
      <pubDate>Wed, 23 Dec 2020 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20201223_2/</guid>
      <description>鍵生成  楕円曲線$E/F_p$と位数$l$のベーズポイント$G$を生成する．($p,l$は素数) 秘密鍵$x$を乱数にて生成し，$E$上で$Y=xG$を計算する．  秘密鍵$x$，公開鍵$E,G,Y$
暗号化 $m$を送信したいメッセージとする．
 $r$wお乱数で生成し，$U=rG=(u_x,u_y)$を計算する． 公開鍵$Y$を用いて，$V = xU =(v_x,v_y)$と，$c=v_x\oplus m$ $\oplus$は排他的論理和 $(U,c)$を暗号文として，送信する．  復号化  以下のように複合する． $V=xU=(v_x,v_y)$,$m=v_x\oplus c$  コード # 乱数 r = 3 # 秘密鍵 key = 3 # ベースポイント g = [2,2] # 位数 l = 5 # y^2 = x^3 + ax +b a = 0 b = 1 def Mod(x,y): if x &amp;lt; 0: x = x+y return x%y def invMod(x,y): count = 1 while True: tmp = x*count if tmp%y == 1: return count count += 1 def Ellipse(p,r): for _ in range(r): s = Mod(Mod((3*p[0]*p[0]+a),l)*invMod((2*p[1]),l),l) x = Mod(s*s-p[0]-p[0],l) y = Mod(s*(p[0]-x)-p[1],l) return [x,y] def encrypt(G,Y, m): U =Ellipse(G,r) V =Ellipse(Y,r) # 排他的論理和をとる c = V[1] ^ m return U,c def decrypt(U, c, key): V = Ellipse(U,key) m = V[1] ^ c return m def main(): # 公開鍵の作成 Y = g Y = Ellipse(Y, key) print(&amp;#34;公開鍵：&amp;#34;,[a,b], g,Y) # 平文 message = 4 print(&amp;#34;平文：&amp;#34;, message) # 暗号化 U,c = encrypt(g, Y, message) print(&amp;#34;暗号文：&amp;#34;,U,c) # 復号化 decrypt_message = decrypt(U, c, key) print(&amp;#34;復号化メッセージ&amp;#34;, decrypt_message) if __name__ == &amp;#34;__main__&amp;#34;: main() </description>
    </item>
    
    <item>
      <title>バイナリ法</title>
      <link>https://yuhi-sa.github.io/posts/20201220/</link>
      <pubDate>Sun, 20 Dec 2020 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20201220/</guid>
      <description>バイナリ法 $x=a^k$のとき，2乗計算を$k$回行うことになる． 計算を効率よくする方法として，$a^{2^i}$を順次求めることで，計算量を$log(k)$回に抑える方法がバイナリ法である．
具体例 $5^{21}=5^{2^4}*5^{2^2}*5^{2^0}$
2進数に展開し，左から順に展開することにより計算を実行する． これにより，$g^k(mod p)$を計算する．
アルゴリズム def Binary(k, g, p): k_binary = [] while(k != 0): k_binary.append(k%2) k = k//2 if k == 1: k_binary.append(k) k = 0 y = 1 for i in reversed(range(len(k_binary))): if k_binary[i] == 1: y = (y*y%p)*g%p else: y = (y*y%p) return y </description>
    </item>
    
    <item>
      <title>ユークリッドの互除法と拡張ユークリッドの互除法</title>
      <link>https://yuhi-sa.github.io/posts/20201015/</link>
      <pubDate>Thu, 15 Oct 2020 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20201015/</guid>
      <description>ユークリッドの互除法 整数$a,b(a&amp;gt;b)$が与えられた時，$a$を$b$で割った余り$r$とすると， $a$と$b$の最大公約数と$b$と$r$の最大公約数は等しいこと(除法の原理)を利用し，割り算を繰り返すことによって$a,b$の最大公約数を求める方法．
アルゴリズム 入力　整数$a,b$ 出力　最大公約数 $d$
 $a_0 = a$,$a_1 = b$ $a_i=0$のとき，$d=a_{i-1}$とし終了 $a_{i-1}=a_iq_i+a_{i+1}$として2に戻る  コード def euclid(a,b): a_list = [] if a &amp;lt; b: a_list.append(b) a_list.append(a) if a &amp;gt;= b: a_list.append(a) a_list.append(b) i = 0 while(a_list[-1]!=0): a_list.append(a_list[i]%a_list[i+1]) i +=1 return a_list[-2] 拡張ユークリッドの互除法 以下の仕組みを用いて一次不定方程式の一つの解を求める方法． $ax+by=d$を求める場合$a_0=a,a_1=b$とおく．
$[\begin{array}{cc} a_{i-1} \
a_i \end{array}]= [\begin{array}{cc} a_iq_i+a_{i+1} \
a_i \end{array}]$ とすると， $[\begin{array}{cc} a_{i-1} \
a_i \end{array}]= [\begin{array}{cc} q_i &amp;amp; 1 \
1 &amp;amp; 0 \end{array}] [\begin{array}{cc} a_i \</description>
    </item>
    
    <item>
      <title>フィルタまとめ</title>
      <link>https://yuhi-sa.github.io/posts/20200901/</link>
      <pubDate>Tue, 01 Sep 2020 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20200901/</guid>
      <description>フィルタ(Filter)とは フィルタとは，測定された時系列データから信号成分だけを通し，ノイズ成分を除去する仕組みである．
想定するモデル フィルタリングでは，時間ステップ$t$，初期値$x_0$，観測値$z_t$，制御入力$u_{t-1}$が与えられたときの，潜在状態$x_t$を推定する．推定値$x$はプロセスモデル$f$によって，観測値$z$は観測モデル$h$によって与えられる．
$$ x_t = f(x_{t-1},u_{t-1},q_{t-1}) \tag{1} $$ $$ z_t=h(x_t,r_t) \tag{2} $$ ここで，$q$は，プロセスノイズ，$r$は観測ノイズである．
$$ q\sim N(0,Q) $$ $$ r\sim N(0,R) $$
カルマンフィルタ(Kalman Filter, EKF) 目的 時刻$t$が観測されたとき，その状態を観測データ$z(t)$と時系列の状態空間モデルを用いて推定する．
方法 状態空間モデルの係数は既知であり，時間によらず一定であると仮定する(LTI:Linear Time-Invariant,線形時不変)．Step1において，対象とする時系列をガウスノイズにより駆動された線形システムの出力とみなし，その線形システムの状態空間モデルを構築する．Step2においてStep1で得られた状態空間モデルの状態$x(t)$を時系列データ$y(t)$から推定する． $x$は，xの平均$\mu$と共分散$\Sigma$によって表される．また，プロセスノイズを$Q$，観測ノイズを$R$とする．
$$ x \sim	(\mu,\Sigma) $$
 Step1 予測ステップ
 事前状態推定値
$$ \hat{\mu}=A\mu_{t-1}+B\mu_t \tag{3} $$ 事前誤差共分散行列
$$ \hat{\Sigma}{t}=A\Sigma{t-1}A^T+Q_{t-1} \tag{4} $$
 Step2 状態推定
 カルマンゲイン
$$ K_t=\hat{\Sigma_t}H^T(\hat{\Sigma_tH^T+R_t})^{-1} \tag{5} $$ 状態推定値
$$ \mu_t=\hat{\mu_t}+K_t(z_t-H\hat{\mu_t}) \tag{6} $$ 事後誤差共分散行列
$$ \Sigma_t = (I_n-K_tH)\hat{\Sigma_t} \tag{7} $$</description>
    </item>
    
    <item>
      <title>強化学習の全体像まとめ</title>
      <link>https://yuhi-sa.github.io/posts/20200831/</link>
      <pubDate>Mon, 31 Aug 2020 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20200831/</guid>
      <description>はじめに 強化学習について，学んでいく中でごちゃごちゃしてきたので頭を整理するために自分なりに要所をまとめてみました．間違っている箇所などあれば教えていただけると勉強になります．
強化学習の全体像 プランニング問題 環境が既知の場合の逐次的意思決定問題
 価値反復法
ベルマン最適作用素を繰り返し用いて最適価値関数を求める．  $$ (B_{*}v)(s)=\max_a{\pi(a|s)(g(s,a)+\gamma \sum p_T(s&#39;|s,a)v(s&#39;)} $$ $$ V^{*}=\lim_{k\rightarrow \infty}(B_{*}^kV)(s) $$
 方策反復法
ベルマン期待作用素を繰り返し用いて最適方策を求める．  $$ (B_{\pi}v)(s)=\sum_a\pi(a|s)(g(s,a)+\gamma \sum p_T(s&#39;|s,a)v(s&#39;)) $$ $$ V^{\pi}=\lim_{k\rightarrow \infty}(B_{\pi}^kV)(s) $$ $$ \pi(s)=\arg\max_a{g(s,a)+\gamma \sum_{s&#39;}p_T(s&#39;|s,a)V^\pi(s&#39;)} $$
強化学習 環境が既知の場合の逐次的意思決定問題
報酬や次状態を観測することでデータを収集して，データから方策を学習する．
価値関数Vの推定 方策$\pi$を固定して価値関数の推定を行う．
 オフライン
ベルマン作用素を直接求められないので，まず標本近似によって近似ベルマン作用素を求める．そして，近似ベルマン作用素を価値関数用いて更新する．  $$ \hat{V}(s)=\hat{B}(\hat{V},h_T^\pi)(s) $$
 オンライン    TD法
TD誤差$\delta$を計算して価値関数を更新する．   $$ \delta=r_t+\gamma \hat{V}(s_{t+1})-\hat{V}(s_t) $$ $$ \hat{V}(s_t)=\hat{V}(s_t)+\alpha_t\delta $$
  TD($\lambda$)法
エリジビリティートレースを用いて1エピソード分の価値を一括更新する．   行動価値関数Qの推定 方策$\pi$を固定して行動価値関数の推定を行う．</description>
    </item>
    
    <item>
      <title>[ROS]PublisherとSubscriberを1つのノードに書く方法</title>
      <link>https://yuhi-sa.github.io/posts/20200816/</link>
      <pubDate>Sun, 16 Aug 2020 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/20200816/</guid>
      <description>はじめに ROSを使った際に，Subscriberとして機能しているノードからも，計測データをトピックとして通信したいなと思ったのですが，SubscriberとPublisherを一つのプログラムに書く方法が見つけられずに困ったので(Google検索力低いだけの可能性大)，ROSの理解を深めるためにまとめようと思います．最近使い始めたばかりで，間違ってることもあるかと思いますので詳しい方，間違っていたらご指摘いただきたいです．勉強になります．
ROS(Robot Operating System)とは ロボット・アプリケーション作成を支援するライブラリとツールを提供しているミドルウェアです．具体的には，ロボット同士の通信が簡易にできるようになります．
Pub &amp;amp; Sub通信 ROSでは，ROSのネットワークにつながった実行可能なものを&amp;quot;ノード&amp;quot;とよび，ノード間で&amp;quot;トピック&amp;quot;とよばれるメッセージをやり取りします．メッセージを配信するノードをPublisher(配信者)，メッセージを受信するノードをSubscriber(購読者)と呼びます． 実行環境  ROS kinetic Ubuntu 16.04 LTS Python 3.8.5  Publisherのサンプルプログラム #!/usr/bin/env python # license removed for brevity import rospy from std_msgs.msg import String #使うデータ型をインポート def talker(): #Publisherを作成(&amp;#39;トピック名&amp;#39;,型,サイズ) pub = rospy.Publisher(&amp;#39;chatter&amp;#39;, String, queue_size=10) #ノード名を宣言 rospy.init_node(&amp;#39;talker&amp;#39;, anonymous=True) #ループの周期を宣言 rate = rospy.Rate(10) # 10hz while not rospy.is_shutdown(): #パブリッシュするデータを記入 hello_str = &amp;#34;hello world %s&amp;#34; % rospy.get_time() #パブリッシュするデータをターミナルに表示 rospy.loginfo(hello_str) #データをパブリッシュ pub.publish(hello_str) rate.sleep() if __name__ == &amp;#39;__main__&amp;#39;: try: talker() except rospy.</description>
    </item>
    
    <item>
      <title>about</title>
      <link>https://yuhi-sa.github.io/posts/about/</link>
      <pubDate>Sun, 16 Aug 2020 15:17:23 +0900</pubDate>
      
      <guid>https://yuhi-sa.github.io/posts/about/</guid>
      <description>地方国立大の大学院生．
学んだことをつれづれなるままに書いています．</description>
    </item>
    
  </channel>
</rss>
