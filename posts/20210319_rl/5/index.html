<!doctype html><html lang=ja dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#e74c3c"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="方策勾配法の理論を解説。方策のパラメータ化、期待報酬の最適化、方策勾配定理、対数微分トリック、ベースラインによる分散削減について説明します。"><meta name=keywords content="強化学習"><meta name=author content="yuhi-sa"><link rel=canonical href=https://yuhi-sa.github.io/posts/20210319_rl/5/><meta property="og:type" content="article"><meta property="og:title" content="方策勾配法"><meta property="og:description" content="方策勾配法の理論を解説。方策のパラメータ化、期待報酬の最適化、方策勾配定理、対数微分トリック、ベースラインによる分散削減について説明します。"><meta property="og:url" content="https://yuhi-sa.github.io/posts/20210319_rl/5/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="ja"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-03-22T11:00:23+09:00"><meta property="article:modified_time" content="2026-02-14T10:00:21+09:00"><meta property="article:tag" content="強化学習"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="方策勾配法"><meta name=twitter:description content="方策勾配法の理論を解説。方策のパラメータ化、期待報酬の最適化、方策勾配定理、対数微分トリック、ベースラインによる分散削減について説明します。"><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>方策勾配法 | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://pagead2.googlesyndication.com crossorigin><link rel=dns-prefetch href=https://pagead2.googlesyndication.com><link rel=icon href=https://yuhi-sa.github.io/favicon.ico><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20210319_rl/5/><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20210319_rl/5/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/posts/20210319_rl/5/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"方策勾配法","description":"方策勾配法の理論を解説。方策のパラメータ化、期待報酬の最適化、方策勾配定理、対数微分トリック、ベースラインによる分散削減について説明します。","author":{"@type":"Person","name":"yuhi-sa","url":"https:\/\/yuhi-sa.github.io\/"},"publisher":{"@type":"Organization","name":"tomato blog","logo":{"@type":"ImageObject","url":"https:\/\/yuhi-sa.github.io\/ogp.jpeg"},"url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-03-22T11:00:23\u002b09:00","dateModified":"2026-02-14T10:00:21\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/posts\/20210319_rl\/5\/"},"url":"https:\/\/yuhi-sa.github.io\/posts\/20210319_rl\/5\/","wordCount":115,"keywords":["強化学習"],"articleSection":"Posts","inLanguage":"ja","timeRequired":"PT1M"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"tomato blog","item":"https:\/\/yuhi-sa.github.io\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/yuhi-sa.github.io\/posts\/"},{"@type":"ListItem","position":3,"name":"方策勾配法"}]}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.72a177faa7b12de55dcc39c4ab6b6392116718d5e2735dab0214511354ecb973.css integrity="sha256-cqF3+qexLeVdzDnEq2tjkhFnGNXic12rAhRRE1TsuXM=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.403a82b29e94ee6e7a39204b5082cdf8b4966cfb44115390da7d8867a5006acd.css integrity="sha256-QDqCsp6U7m56OSBLUILN+LSWbPtEEVOQ2n2IZ6UAas0=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>window.MathJax={tex:{inlineMath:[["\\(","\\)"]],displayMath:[["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script data-ad-client=ca-pub-9558545098866170 async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>方策勾配法</h1><p class="lead article-description" itemprop=description>方策勾配法の理論を解説。方策のパラメータ化、期待報酬の最適化、方策勾配定理、対数微分トリック、ベースラインによる分散削減について説明します。</p><div class=article-meta><time datetime=2021-03-22T11:00:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
March 22, 2021
</time><time datetime=2026-02-14T10:00:21+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 14, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
1 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/tags/%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>強化学習</a></div></header><div class=article-content itemprop=articleBody><p>方策勾配法（Policy Gradient Method）は、強化学習における方策ベースの手法の一つです。価値関数を学習するのではなく、<strong>方策そのもの</strong>をパラメータ化された関数として表現し、そのパラメータを直接最適化することで、最適な方策を見つけます。</p><p>方策は、状態 \(s\) を入力として受け取り、各行動 \(a\) を選択する確率 \(\pi_\theta(a|s)\) を出力する関数として表現されます。ここで \(\theta\) は方策のパラメータです。</p><h2 id=方策の評価と最適化>方策の評価と最適化</h2><p>方策の良さは、その方策に従って行動したときに得られる<strong>期待報酬</strong>（または期待累積報酬）によって評価されます。この期待報酬を最大化することが方策勾配法の目的です。</p><p>期待報酬 \(J(\theta)\) は、以下のように表現できます。</p>\[ J(\theta) = \sum*s d^{\pi*\theta}(s) \sum*a \pi*\theta(a|s) Q^{\pi\_\theta}(s,a) \]<p>ここで、</p><ul><li>\(d^{\pi_\theta}(s)\): 方策 \(\pi_\theta\) の下で状態 \(s\) を訪れる定常分布（または割引状態訪問頻度）です。</li><li>\(Q^{\pi_\theta}(s,a)\): 方策 \(\pi_\theta\) の下での状態 \(s\) で行動 \(a\) を取ったときの行動価値関数です。</li></ul><p>この期待報酬 \(J(\theta)\) を最大化するために、パラメータ \(\theta\) を勾配上昇法で更新します。つまり、期待報酬の勾配 \(\nabla J(\theta)\) を計算し、その方向にパラメータを少しずつ更新していきます。</p><h2 id=方策勾配定理-policy-gradient-theorem>方策勾配定理 (Policy Gradient Theorem)</h2><p>方策勾配定理は、期待報酬の勾配 \(\nabla J(\theta)\) を、以下のようにシンプルな形で表現できることを示しています。</p>\[ \nabla J(\theta) \propto \sum*s d^{\pi*\theta}(s) \sum*a \nabla \pi*\theta(a|s) Q^{\pi\_\theta}(s,a) \]<p>さらに、対数微分トリック（Log-derivative Trick）を用いると、この勾配は期待値の形で表現できます。</p>\[ \nabla J(\theta) = \mathbb{E}_{s \sim d^{\pi_\theta}, a \sim \pi*\theta}[\nabla \log \pi*\theta(a|s) Q^{\pi\_\theta}(s,a)] \]<p>この式は、方策勾配を直感的に解釈する助けとなります。</p><ul><li>\(\nabla \log \pi_\theta(a|s)\): 行動 \(a\) を選択する確率を増加させる方向（勾配）を示します。</li><li>\(Q^{\pi_\theta}(s,a)\): その行動の「良さ」を表す重みです。</li></ul><p>つまり、方策勾配法は、「良い行動（\(Q\)値が高い行動）が選択される確率を増加させ、悪い行動（\(Q\)値が低い行動）が選択される確率を減少させる」ように方策を更新します。</p><h2 id=実装上の課題と解決策>実装上の課題と解決策</h2><p>方策勾配法の実装には、いくつかの課題があります。</p><ul><li><strong>\(Q^{\pi_\theta}(s,a)\) の推定</strong>: 行動価値関数 \(Q^{\pi_\theta}(s,a)\) は未知であるため、経験から推定する必要があります。モンテカルロ法（エピソード終了後の累積報酬）やTD学習（TD誤差）を用いて近似します。</li><li><strong>分散の削減</strong>: 勾配の推定にモンテカルロ法を用いると、分散が大きくなり、学習が不安定になることがあります。これを解決するために、<strong>ベースライン</strong>（例: 状態価値関数 \(V(s)\)）を導入して、アドバンテージ関数 \(A(s,a) = Q(s,a) - V(s)\) を用いる手法が一般的です。</li></ul><p>方策勾配法は、連続行動空間の問題にも適用できるなど、柔軟性の高い手法であり、Actor-Critic法などのより高度なアルゴリズムの基礎となっています。</p><h2 id=参考>参考</h2><ul><li>久保隆宏, 『Pythonで学ぶ強化学習 入門から実践まで』, 翔泳社 (2019)</li></ul></div><div class="ad-slot in-content my-3"><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9558545098866170></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=article-footer></footer><meta itemprop=wordCount content="115"><meta itemprop=url content="https://yuhi-sa.github.io/posts/20210319_rl/5/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/posts/20210319_rl/4/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>ニューラルネットワークを適用した強化学習</span>
</a><a href=/posts/20210319_rl/6/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>Advantage Actor-Critic (A2C)</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/posts/20211102_op3/1/>強化学習によるROBOTIS OP3歩行学習：ROSパッケージ実装解説</a></h3><div class=card-meta><time datetime=2021-11-02>November 2, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/posts/20210319_rl/7/>深層強化学習の主要アルゴリズム</a></h3><div class=card-meta><time datetime=2021-03-22>March 22, 2021</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/posts/20210319_rl/6/>Advantage Actor-Critic (A2C)</a></h3><div class=card-meta><time datetime=2021-03-22>March 22, 2021</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>方策勾配法</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2026 yuhi-sa. All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"方策勾配法","url":"https:\/\/yuhi-sa.github.io\/posts\/20210319_rl\/5\/","description":"方策勾配法の理論を解説。方策のパラメータ化、期待報酬の最適化、方策勾配定理、対数微分トリック、ベースラインによる分散削減について説明します。","inLanguage":"ja","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>