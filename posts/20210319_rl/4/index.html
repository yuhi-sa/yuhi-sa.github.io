<!DOCTYPE html>
<html lang="ja" dir="ltr">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<title>ニューラルネットを適応した強化学習 | tomato blog</title>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet"
  integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM" crossorigin="anonymous">
    <link rel="stylesheet" href="/css/main.min.f0006a9d6f3625f7765522eece9b7cccee5b1dc15a581742039a67489209bcb4.css" integrity="sha256-8ABqnW82Jfd2VSLuzpt8zO5bHcFaWBdCA5pnSJIJvLQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="/css/syntax.min.aa0332253f313dc48905008b4ab314155e5a13302588d25bcf4949f7c1abdde0.css" integrity="sha256-qgMyJT8xPcSJBQCLSrMUFV5aEzAliNJbz0lJ98Gr3eA=" crossorigin="anonymous">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">


<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"></script>


<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"></script>


<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"></script>


<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>


<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>


</head>
<body>
  <header>
    <h1>tomato blog</h1>

    <nav class="navbar navbar-expand-lg navbar-light bg-light">
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav ml-auto">
      <li class="nav-item">
        <a ZgotmplZ href="https://yuhi-sa.github.io/" class="navbar-brand" > Blog </a>
      </li>
      <li class="nav-item">
        <a ZgotmplZ href="https://yuhi-sa.github.io/tags" class="navbar-brand" > Tags </a>
      </li>
      <li class="nav-item">
        <a ZgotmplZ href="https://yuhi-sa.github.io/posts/about" class="navbar-brand" > About </a>
      </li>
      <li class="nav-item">
        <a ZgotmplZ href="https://yuhi-sa.github.io/posts/privacy_policy" class="navbar-brand" > privacy policy </a>
      </li>
        </ul>
      </div>
    </nav>
  



<script async src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3"></script>
<script>
   window.dataLayer = window.dataLayer || [];
   function gtag(){dataLayer.push(arguments);}
   gtag('js', new Date());
   gtag('config', 'G-LN6QP6VVM3');
</script>


<script data-ad-client="ca-pub-9558545098866170" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>




  </header>
  <main>
    
  <h1>ニューラルネットを適応した強化学習</h1>

  
  
  <time datetime="2021-03-22T10:00:23&#43;09:00">2021年3月22日</time>

  <h1 id="学習安定化のための方法">学習安定化のための方法</h1>
<h2 id="experience-replay">Experience Replay</h2>
<p>行動履歴をプールしておき，そこからサンプリングすることで，さまざまなエピソードにおける，異なるタイムステップのデータを学習データとして使用する．これにより学習する経験の偏りを防ぎ，学習を安定化させる．</p>
<h2 id="fixed-target-q-network">Fixed Target Q-Network</h2>
<p>一定期間固定されたパラメーターから価値を算出する手法．遷移先の価値を本体のモデルで計算する場合，学習のたびにパラメーターが変わるため，値が毎回変わることとなる．これではTD誤差が安定しないため，一定期間パラメーターを固定する．</p>
<h2 id="報酬のclipping">報酬のClipping</h2>
<p>全ゲームを通じ成功は1，失敗は-1と報酬を統一する．</p>
<h1 id="deep-q-networkdqnとその改良">Deep Q-Network(DQN)とその改良</h1>
<p>価値評価に深層学習を適応する方法をDeep Q-Network(DQN)とよぶ．単純にニューラルネットを使用するだけでは学習が安定しないため，前項のような安定化の工夫が必要となる．<br>
DQNを発表したDeep Mindは，改良手法6つを組み込んだRainbowというモデルを発表している．</p>
<h2 id="double-dqn">Double DQN</h2>
<p>行動価値と行動選択のネットワークを分けることで<!-- raw HTML omitted -->価値の見積もり精度<!-- raw HTML omitted -->を上げる．</p>
<h2 id="prioritized-replay">Prioritized Replay</h2>
<p>Experience Replyから単純にランダムサンプリングするのではなく，学習効果の高い(TD誤差が大きいもの)を優先してサンプリングを行い<!-- raw HTML omitted -->学習効率<!-- raw HTML omitted -->を上げる．</p>
<h2 id="dueling-network">Dueling Network</h2>
<p>状態価値と行動価値を分けて計算することにより<!-- raw HTML omitted -->価値の見積もり精度<!-- raw HTML omitted -->を上げる．</p>
<h2 id="multi-step-learning">Multi-step Learning</h2>
<p>Q学習とMonteCarlo法の間をとる手法で，「nステップ分の報酬」と「nステップ先の状態の価値」から修正を行うことで，<!-- raw HTML omitted -->価値の見積もり精度<!-- raw HTML omitted -->を上げる．</p>
<h2 id="distributional-rl">Distributional RL</h2>
<p>報酬を分布として扱い，<!-- raw HTML omitted -->価値の見積もり精度<!-- raw HTML omitted -->を上げる．</p>
<h2 id="noisy-nets">Noisy Nets</h2>
<p>Epsilon-Greedy法においてEpsilonの設定は非常にセンシティブである．Noisy Netsではどのくらいランダムに行動したほうがよいか自体をネットワークに学習することで<!-- raw HTML omitted -->探索効率<!-- raw HTML omitted -->を上げる．</p>
<h1 id="参考">参考</h1>
<p>久保隆宏,&quot;<a href="https://amzn.to/3tA1S4W">Pythonで学ぶ強化学習 入門から実践まで</a>&quot;</p>
  
  <div>
    <div>Tags:</div>
    <ul>
        <li><a href="/tags/python%E3%81%A7%E5%AD%A6%E3%81%B6%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92/">Pythonで学ぶ強化学習</a></li>
    </ul>
  </div>


  </main>
  <footer>
    <p>Copyright 2023. All rights reserved.</p>

  </footer>
</body>
</html>
