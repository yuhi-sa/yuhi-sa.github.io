<!doctype html><html lang=ja dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="勉強したことなどをメモしています"><link rel=canonical href=https://yuhi-sa.github.io/posts/20210107_prml_ent/1/><meta property="og:type" content="article"><meta property="og:title" content="情報理論の基礎（エントロピーから相互情報量まで, PRML 1.6）"><meta property="og:description" content="勉強したことなどをメモしています"><meta property="og:url" content="https://yuhi-sa.github.io/posts/20210107_prml_ent/1/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="ja"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-01-07T15:17:23+09:00"><meta property="article:modified_time" content="2025-07-25T17:16:12+09:00"><meta property="article:tag" content="機械学習"><meta property="article:tag" content="情報理論"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="情報理論の基礎（エントロピーから相互情報量まで, PRML 1.6）"><meta name=twitter:description content="勉強したことなどをメモしています"><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>情報理論の基礎（エントロピーから相互情報量まで, PRML 1.6） | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"情報理論の基礎（エントロピーから相互情報量まで, PRML 1.6）","description":"","author":{"@type":"Person","name":""},"publisher":{"@type":"Organization","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-01-07T15:17:23\u002b09:00","dateModified":"2025-07-25T17:16:12\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/posts\/20210107_prml_ent\/1\/"},"url":"https:\/\/yuhi-sa.github.io\/posts\/20210107_prml_ent\/1\/","wordCount":250,"keywords":["機械学習","情報理論"],"articleSection":"Posts","inLanguage":"ja","timeRequired":"PT2M"}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.963335e1959a83c16c6f09e75be26e42b746222835a882f77a89d68f9883aafe.css integrity="sha256-ljM14ZWag8FsbwnnW+JuQrdGIig1qIL3eonWj5iDqv4=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.65d091264255d75817988ed1dce4adcef2482557713854f8aaa3dde137672915.css integrity="sha256-ZdCRJkJV11gXmI7R3OStzvJIJVdxOFT4qqPd4TdnKRU=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.20b142b4e47bd9042b2d9fffa65797c455e75d76c4797957d567b04a03fd8b0c.css integrity="sha256-ILFCtOR72QQrLZ//pleXxFXnXXbEeXlX1WewSgP9iww=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script async src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LN6QP6VVM3")</script><script data-ad-client=ca-pub-9558545098866170 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>情報理論の基礎（エントロピーから相互情報量まで, PRML 1.6）</h1><div class=article-meta><time datetime=2021-01-07T15:17:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
January 7, 2021
</time><time datetime=2025-07-25T17:16:12+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated July 25, 2025
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
2 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>機械学習
</a><a href=/tags/%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>情報理論</a></div></header><div class=article-content itemprop=articleBody><p>情報理論は、情報の定量化、圧縮、通信に関する数学的な理論です。機械学習では、モデルの性能評価や正則化など、様々な場面でその概念が応用されます。</p><h2 id=情報量>情報量</h2><p>ある事象 $x$ が起きたことを知ったときに得られる「情報量」 $h(x)$ は、その事象がどれだけ「珍しい」か、つまり、その事象の起こる確率 $p(x)$ がどれだけ低いかによって定義されます。</p><p>情報量は、以下の性質を満たすように定義するのが自然です。</p><ol><li><strong>加法性</strong>: 独立な2つの事象 $x, y$ を観測したときの情報量は、それぞれを個別に観測したときの情報量の和に等しい。<br>$h(x, y) = h(x) + h(y)$</li><li><strong>独立性</strong>: 独立な事象の同時確率は、それぞれの確率の積で表される。
$p(x, y) = p(x)p(y)</li></ol><p>この2つの性質から、情報量は確率の対数を用いて定義するのが合理的であることがわかります。確率 $p(x)$ は1以下の値をとるため、情報量が非負になるように負号をつけます。</p><p>$$ h(x) = -\log_2 p(x) $$</p><p>情報の単位は、対数の底として2を用いた場合、<strong>ビット (bit)</strong> となります。</p><h2 id=エントロピー>エントロピー</h2><p>エントロピーは、ある確率変数 $X$ が生成する情報の<strong>平均情報量</strong>を表します。これは、情報量 $h(x)$ を確率分布 $p(x)$ で期待値をとることで計算されます。</p><p>$$ H[X] = \mathbb{E}_{p(x)}[h(x)] = -\sum_x p(x) \log_2 p(x) $$</p><p>エントロピーは、確率変数の「不確実性」や「予測の難しさ」の度合いと解釈できます。</p><ul><li><strong>エントロピーが低い</strong>: 分布が特定の少数の値に集中している（ピークが鋭い）状態。結果が予測しやすいため、不確実性は低い。</li><li><strong>エントロピーが高い</strong>: 分布が多くの値にわたって広がっている（一様に近い）状態。結果が予測しにくいため、不確実性は高い。</li></ul><p><img src=.././fig1.png alt=fig1.png></p><p><strong>ノイズなし符号化定理</strong>によれば、エントロピーは、ある確率変数の値を誤りなく送信するために必要なビット数の下限を与えます。例えば、出現確率が不均一な文字を符号化する場合、よく出現する文字には短い符号を、稀にしか出現しない文字には長い符号を割り当てることで、平均符号長をエントロピーに近づけることができます。</p><h3 id=微分エントロピー>微分エントロピー</h3><p>連続確率変数に対してエントロピーを拡張したものが<strong>微分エントロピー</strong>です。</p><p>$$ H[X] = -\int p(x) \ln p(x) dx $$</p><p><strong>注意</strong>: 微分エントロピーは、離散の場合とは異なり、負の値をとることがあり、情報の絶対量を表すものではありません。</p><h4 id=微分エントロピーの最大化>微分エントロピーの最大化</h4><p>ある制約の下で微分エントロピーを最大化する分布を考えると、その確率変数の性質がわかります。例えば、平均 $\mu$ と分散 $\sigma^2$ が固定されているという制約の下で微分エントロピーを最大化する分布は、<strong>ガウス分布</strong>になります。</p><p>$$ p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\lbrace-\frac{(x-\mu)^2}{2\sigma^2}\rbrace $$</p><p>ガウス分布の微分エントロピーは次式で与えられます。</p><p>$$ H[X] = \frac{1}{2} {1 + \ln(2\pi\sigma^2)} $$</p><p>この式から、エントロピーは分散 $\sigma^2$ が大きいほど（分布の広がりが大きいほど）高くなることがわかります。</p><h2 id=条件付きエントロピーと相互情報量>条件付きエントロピーと相互情報量</h2><h3 id=条件付きエントロピー>条件付きエントロピー</h3><p>2つの確率変数 $X, Y$ の同時分布 $p(x, y)$ を考えます。$X=x$ であることがわかった上で、$Y$ の不確実性がどれだけ残っているかを表すのが<strong>条件付きエントロピー</strong>です。</p><p>$$ H[Y|X] = -\iint p(x, y) \ln p(y|x) dy dx $$</p><p>これらは、エントロピーの連鎖律として知られる以下の関係を満たします。</p><p>$$ H[X, Y] = H[Y|X] + H[X] $$</p><p>これは、「$X$と$Y$を特定するための情報量は、$X$を特定するための情報量と、$X$が与えられた下で$Y$を特定するために必要な追加の情報量の和に等しい」と解釈できます。</p><h3 id=相対エントロピーカルバックライブラーダイバージェンス>相対エントロピー（カルバック・ライブラー・ダイバージェンス）</h3><p>ある未知の真の分布 $p(x)$ を、モデル $q(x)$ で近似することを考えます。このとき、2つの分布の「隔たり」を測る尺度として<strong>相対エントロピー</strong>、または<strong>KLダイバージェンス</strong>が用いられます。</p><p>$$ KL(p||q) = -\int p(x) \ln \lbrace \frac{q(x)}{p(x)} \rbrace dx $$</p><p>KLダイバージェンスには以下の重要な性質があります。</p><ul><li>$KL(p||q) \ge 0$</li><li>$p(x) = q(x)$ のとき、またそのときに限り $KL(p||q) = 0$</li></ul><p>この性質から、KLダイバージェンスは2つの分布間の「距離」のような尺度として解釈できます（ただし、対称性 $KL(p||q) \neq KL(q||p)$ を満たさないため、数学的な距離ではありません）。</p><h4 id=klダイバージェンス最小化と最尤推定>KLダイバージェンス最小化と最尤推定</h4><p>データが未知の分布 $p(x)$ から生成されているとき、それをパラメータ $\theta$ を持つモデル $q(x|\theta)$ で近似する場合、両者のKLダイバージェンスを最小化する $\theta$ を見つけることが目標となります。</p><p>$KL(p||q)$ の式を展開すると、
$$ KL(p||q) = -\int p(x) \ln q(x|\theta) dx + \int p(x) \ln p(x) dx $$</p><p>第2項は真の分布のエントロピーであり、$\theta$ には依存しません。したがって、$KL(p||q)$ を最小化することは、第1項、すなわち<strong>対数尤度の期待値を最大化</strong>することと等価です。</p><p>データセット ${x_n}$ が与えられた場合、この期待値はデータの平均で近似できるため、結果的に<strong>KLダイバージェンスの最小化は、尤度の最大化（最尤推定）と等価</strong>になります。</p><h3 id=相互情報量>相互情報量</h3><p>2つの確率変数 $X, Y$ がどれだけ強く依存しているか、一方を知ることで他方の不確実性がどれだけ減少するかを表す尺度が<strong>相互情報量</strong>です。</p><p>これは、同時分布 $p(x, y)$ と、変数が独立だと仮定した場合の分布 $p(x)p(y)$ との間のKLダイバージェンスとして定義されます。</p><p>$$ I[X, Y] \equiv KL(p(x, y) || p(x)p(y)) = \iint p(x, y) \ln ( \frac{p(x, y)}{p(x)p(y)} ) dx dy $$</p><p>相互情報量は、エントロピーを用いて以下のように表現することもできます。</p><p>$$ I[X, Y] = H[X] - H[X|Y] = H[Y] - H[Y|X] $$</p><p>これは、「$Y$を知ることで減少する$X$の不確実性の量」と解釈できます。</p><h2 id=参考>参考</h2><ul><li>C.M. ビショップ, &ldquo;パターン認識と機械学習 上&rdquo;, 丸善出版 (2012)</li></ul></div><footer class=article-footer></footer><meta itemprop=wordCount content="250"><meta itemprop=url content="https://yuhi-sa.github.io/posts/20210107_prml_ent/1/"></article><nav class="article-navigation mt-5" aria-label="Article navigation"><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>情報理論の基礎（エントロピーから相互情報量まで, PRML 1.6）</span>
<meta itemprop=position content="4"></li></ol></nav><div class=terms-wrapper role=region aria-label=Tags><h6 class=terms-label id=terms-tags>Tags:</h6><ul class=terms-list role=list aria-labelledby=terms-tags aria-describedby=terms-count-tags><li class=terms-item role=listitem><a href=/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/ class="terms-link badge badge-custom text-decoration-none" rel=tag aria-label="View all posts tagged with 機械学習" title="機械学習 (0 posts)">機械学習</a></li><li class=terms-item role=listitem><a href=/tags/%E6%83%85%E5%A0%B1%E7%90%86%E8%AB%96/ class="terms-link badge badge-custom text-decoration-none" rel=tag aria-label="View all posts tagged with 情報理論" title="情報理論 (0 posts)">情報理論</a></li></ul><span id=terms-count-tags class=sr-only>2 tags total</span></div></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container py-5"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2025
tomato blog.
All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"情報理論の基礎（エントロピーから相互情報量まで, PRML 1.6）","url":"https:\/\/yuhi-sa.github.io\/posts\/20210107_prml_ent\/1\/","description":"勉強したことなどをメモしています","inLanguage":"ja","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>