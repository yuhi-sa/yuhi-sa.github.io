<!doctype html><html lang=ja dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#e74c3c"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="SGD、Momentum、RMSProp、Adamなど勾配ベース最適化アルゴリズムの数理と特性を比較し、PyTorchでの実装と収束挙動の可視化を紹介します。"><meta name=keywords content="SGD,Adam,RMSProp,Momentum,勾配降下法,最適化,深層学習,PyTorch"><meta name=author content="yuhi-sa"><link rel=canonical href=https://yuhi-sa.github.io/posts/20260226_sgd_adam/1/><meta property="og:type" content="article"><meta property="og:title" content="確率的勾配降下法からAdamまで：勾配ベース最適化の進化"><meta property="og:description" content="SGD、Momentum、RMSProp、Adamなど勾配ベース最適化アルゴリズムの数理と特性を比較し、PyTorchでの実装と収束挙動の可視化を紹介します。"><meta property="og:url" content="https://yuhi-sa.github.io/posts/20260226_sgd_adam/1/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="ja"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2026-02-26T20:00:00+09:00"><meta property="article:modified_time" content="2026-02-26T23:47:37+09:00"><meta property="article:tag" content="機械学習"><meta property="article:tag" content="最適化"><meta property="article:tag" content="Python"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="確率的勾配降下法からAdamまで：勾配ベース最適化の進化"><meta name=twitter:description content="SGD、Momentum、RMSProp、Adamなど勾配ベース最適化アルゴリズムの数理と特性を比較し、PyTorchでの実装と収束挙動の可視化を紹介します。"><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>確率的勾配降下法からAdamまで：勾配ベース最適化の進化 | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://pagead2.googlesyndication.com crossorigin><link rel=dns-prefetch href=https://pagead2.googlesyndication.com><link rel=icon href=https://yuhi-sa.github.io/favicon.ico><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20260226_sgd_adam/1/><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20260226_sgd_adam/1/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/posts/20260226_sgd_adam/1/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"確率的勾配降下法からAdamまで：勾配ベース最適化の進化","description":"SGD、Momentum、RMSProp、Adamなど勾配ベース最適化アルゴリズムの数理と特性を比較し、PyTorchでの実装と収束挙動の可視化を紹介します。","author":{"@type":"Person","name":"yuhi-sa","url":"https:\/\/yuhi-sa.github.io\/"},"publisher":{"@type":"Organization","name":"tomato blog","logo":{"@type":"ImageObject","url":"https:\/\/yuhi-sa.github.io\/ogp.jpeg"},"url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2026-02-26T20:00:00\u002b09:00","dateModified":"2026-02-26T23:47:37\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/posts\/20260226_sgd_adam\/1\/"},"url":"https:\/\/yuhi-sa.github.io\/posts\/20260226_sgd_adam\/1\/","wordCount":599,"keywords":["機械学習","最適化","Python"],"articleSection":"Posts","inLanguage":"ja","timeRequired":"PT3M"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"tomato blog","item":"https:\/\/yuhi-sa.github.io\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/yuhi-sa.github.io\/posts\/"},{"@type":"ListItem","position":3,"name":"確率的勾配降下法からAdamまで：勾配ベース最適化の進化"}]}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.72a177faa7b12de55dcc39c4ab6b6392116718d5e2735dab0214511354ecb973.css integrity="sha256-cqF3+qexLeVdzDnEq2tjkhFnGNXic12rAhRRE1TsuXM=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.403a82b29e94ee6e7a39204b5082cdf8b4966cfb44115390da7d8867a5006acd.css integrity="sha256-QDqCsp6U7m56OSBLUILN+LSWbPtEEVOQ2n2IZ6UAas0=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>window.MathJax={tex:{inlineMath:[["\\(","\\)"]],displayMath:[["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script data-ad-client=ca-pub-9558545098866170 async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>確率的勾配降下法からAdamまで：勾配ベース最適化の進化</h1><p class="lead article-description" itemprop=description>SGD、Momentum、RMSProp、Adamなど勾配ベース最適化アルゴリズムの数理と特性を比較し、PyTorchでの実装と収束挙動の可視化を紹介します。</p><div class=article-meta><time datetime=2026-02-26T20:00:00+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
February 26, 2026
</time><time datetime=2026-02-26T23:47:37+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 26, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
3 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>機械学習
</a><a href=/tags/%E6%9C%80%E9%81%A9%E5%8C%96/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>最適化
</a><a href=/tags/python/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Python</a></div></header><div class=article-content itemprop=articleBody><h2 id=はじめに>はじめに</h2><p>深層学習モデルの学習は、損失関数を最小化するパラメータを見つける最適化問題です。この最適化の中心にあるのが**勾配降下法（Gradient Descent）**とその派生手法です。</p><p>本記事では、基本的なSGDから現在最も広く使われるAdamまで、勾配ベース最適化アルゴリズムの数理的な仕組みと特性を比較し、PyTorchでの収束挙動を可視化します。</p><h2 id=勾配降下法gd>勾配降下法（GD）</h2><p>パラメータ \(\theta\) を損失関数 \(L(\theta)\) の勾配方向に更新します。</p>\[\theta_{t+1} = \theta_t - \eta \nabla L(\theta_t) \tag{1}\]<p>ここで \(\eta\) は学習率です。全データを使って勾配を計算するため、1ステップのコストが高く大規模データには不向きです。</p><h2 id=確率的勾配降下法sgd>確率的勾配降下法（SGD）</h2><p>ミニバッチ \(\mathcal{B}\) からの勾配推定を使って更新します。</p>\[\theta_{t+1} = \theta_t - \eta \nabla L_{\mathcal{B}}(\theta_t) \tag{2}\]<p>計算コストが低い反面、勾配の分散が大きく、収束が不安定になりやすい問題があります。</p><h2 id=momentum>Momentum</h2><p>過去の勾配の指数移動平均（速度項）を導入し、更新の方向を安定させます。</p>\[v_t = \beta v_{t-1} + \nabla L_{\mathcal{B}}(\theta_t) \tag{3}\]\[\theta_{t+1} = \theta_t - \eta v_t \tag{4}\]<p>\(\beta\)（典型的に0.9）が大きいほど過去の勾配の影響が強くなります。谷状の損失地形で振動を抑制し、収束を加速する効果があります。</p><h2 id=nesterov-accelerated-gradientnag>Nesterov Accelerated Gradient（NAG）</h2><p>Momentumの改良版で、「先読み」した位置での勾配を計算します。</p>\[v_t = \beta v_{t-1} + \nabla L_{\mathcal{B}}(\theta_t - \eta \beta v_{t-1}) \tag{5}\]\[\theta_{t+1} = \theta_t - \eta v_t \tag{6}\]<p>先読みにより、最適解の付近でのオーバーシュートを軽減します。</p><h2 id=adagrad>AdaGrad</h2><p>パラメータごとに異なる学習率を適用する<strong>適応的学習率</strong>手法の先駆けです。</p>\[G_t = G_{t-1} + (\nabla L_{\mathcal{B}}(\theta_t))^2 \tag{7}\]\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \varepsilon}} \nabla L_{\mathcal{B}}(\theta_t) \tag{8}\]<p>\(G_t\) は勾配の二乗の累積和です。頻繁に更新されるパラメータほど学習率が小さくなります。問題点として、\(G_t\) が単調増加するため学習率が過度に減衰し、学習が早期に停止することがあります。</p><h2 id=rmsprop>RMSProp</h2><p>AdaGradの問題を解決するため、勾配二乗の<strong>指数移動平均</strong>を使います。</p>\[s_t = \rho s_{t-1} + (1 - \rho)(\nabla L_{\mathcal{B}}(\theta_t))^2 \tag{9}\]\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{s_t + \varepsilon}} \nabla L_{\mathcal{B}}(\theta_t) \tag{10}\]<p>\(\rho\)（典型的に0.99）により古い勾配情報を忘却し、学習率の過度な減衰を防ぎます。</p><h2 id=adamadaptive-moment-estimation>Adam（Adaptive Moment Estimation）</h2><p>MomentumとRMSPropを統合した手法で、現在最も広く使われるオプティマイザです。</p>\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) \nabla L_{\mathcal{B}}(\theta_t) \tag{11}\]\[v_t = \beta_2 v_{t-1} + (1 - \beta_2) (\nabla L_{\mathcal{B}}(\theta_t))^2 \tag{12}\]\[\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t} \tag{13}\]\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \varepsilon} \hat{m}_t \tag{14}\]<ul><li>\(m_t\): 勾配の1次モーメント（平均）の推定</li><li>\(v_t\): 勾配の2次モーメント（分散）の推定</li><li>式 \((13)\) のバイアス補正は、初期ステップでの推定バイアスを修正します</li><li>推奨パラメータ: \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), \(\varepsilon = 10^{-8}\)</li></ul><h2 id=比較表>比較表</h2><table><thead><tr><th>手法</th><th>適応的学習率</th><th>モーメンタム</th><th>主な特徴</th></tr></thead><tbody><tr><td>SGD</td><td>No</td><td>No</td><td>シンプル、汎化性能が良い場合あり</td></tr><tr><td>Momentum</td><td>No</td><td>Yes</td><td>振動抑制、収束加速</td></tr><tr><td>NAG</td><td>No</td><td>Yes</td><td>先読みによるオーバーシュート軽減</td></tr><tr><td>AdaGrad</td><td>Yes</td><td>No</td><td>スパースデータに有効、学習率減衰問題</td></tr><tr><td>RMSProp</td><td>Yes</td><td>No</td><td>AdaGradの減衰問題を解決</td></tr><tr><td>Adam</td><td>Yes</td><td>Yes</td><td>最も広く使用、ロバスト</td></tr></tbody></table><h2 id=python実装収束挙動の可視化>Python実装：収束挙動の可視化</h2><p>Rosenbrock関数上での各オプティマイザの軌跡を比較します。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>rosenbrock</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Rosenbrock関数: f(x,y) = (1-x)^2 + 100(y-x^2)^2&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>x</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span> <span class=o>+</span> <span class=mi>100</span> <span class=o>*</span> <span class=p>(</span><span class=n>y</span> <span class=o>-</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>rosenbrock_grad</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Rosenbrock関数の勾配&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>dx</span> <span class=o>=</span> <span class=o>-</span><span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>x</span><span class=p>)</span> <span class=o>-</span> <span class=mi>400</span> <span class=o>*</span> <span class=n>x</span> <span class=o>*</span> <span class=p>(</span><span class=n>y</span> <span class=o>-</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>dy</span> <span class=o>=</span> <span class=mi>200</span> <span class=o>*</span> <span class=p>(</span><span class=n>y</span> <span class=o>-</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=n>dx</span><span class=p>,</span> <span class=n>dy</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>optimize</span><span class=p>(</span><span class=n>method</span><span class=p>,</span> <span class=n>x0</span><span class=p>,</span> <span class=n>lr</span><span class=p>,</span> <span class=n>steps</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;各最適化手法でRosenbrock関数を最小化&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>x0</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=nb>float</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>trajectory</span> <span class=o>=</span> <span class=p>[</span><span class=n>x</span><span class=o>.</span><span class=n>copy</span><span class=p>()]</span>
</span></span><span class=line><span class=cl>    <span class=n>m</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># 1st moment</span>
</span></span><span class=line><span class=cl>    <span class=n>v</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>  <span class=c1># 2nd moment</span>
</span></span><span class=line><span class=cl>    <span class=n>beta1</span> <span class=o>=</span> <span class=n>kwargs</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;beta1&#39;</span><span class=p>,</span> <span class=mf>0.9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>beta2</span> <span class=o>=</span> <span class=n>kwargs</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s1>&#39;beta2&#39;</span><span class=p>,</span> <span class=mf>0.999</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>eps</span> <span class=o>=</span> <span class=mf>1e-8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>steps</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>g</span> <span class=o>=</span> <span class=n>rosenbrock_grad</span><span class=p>(</span><span class=n>x</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>x</span><span class=p>[</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>method</span> <span class=o>==</span> <span class=s1>&#39;sgd&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>g</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>method</span> <span class=o>==</span> <span class=s1>&#39;momentum&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>m</span> <span class=o>=</span> <span class=n>beta1</span> <span class=o>*</span> <span class=n>m</span> <span class=o>+</span> <span class=n>g</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>m</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>method</span> <span class=o>==</span> <span class=s1>&#39;rmsprop&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>v</span> <span class=o>=</span> <span class=n>beta2</span> <span class=o>*</span> <span class=n>v</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beta2</span><span class=p>)</span> <span class=o>*</span> <span class=n>g</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>g</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>v</span><span class=p>)</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>method</span> <span class=o>==</span> <span class=s1>&#39;adam&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>m</span> <span class=o>=</span> <span class=n>beta1</span> <span class=o>*</span> <span class=n>m</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beta1</span><span class=p>)</span> <span class=o>*</span> <span class=n>g</span>
</span></span><span class=line><span class=cl>            <span class=n>v</span> <span class=o>=</span> <span class=n>beta2</span> <span class=o>*</span> <span class=n>v</span> <span class=o>+</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beta2</span><span class=p>)</span> <span class=o>*</span> <span class=n>g</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>            <span class=n>m_hat</span> <span class=o>=</span> <span class=n>m</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beta1</span><span class=o>**</span><span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>v_hat</span> <span class=o>=</span> <span class=n>v</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>beta2</span><span class=o>**</span><span class=n>t</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=n>lr</span> <span class=o>*</span> <span class=n>m_hat</span> <span class=o>/</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>v_hat</span><span class=p>)</span> <span class=o>+</span> <span class=n>eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>trajectory</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>copy</span><span class=p>())</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>trajectory</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 各手法の軌跡を計算 ---</span>
</span></span><span class=line><span class=cl><span class=n>x0</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mf>1.0</span><span class=p>,</span> <span class=mf>1.5</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>steps</span> <span class=o>=</span> <span class=mi>5000</span>
</span></span><span class=line><span class=cl><span class=n>trajectories</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;SGD&#39;</span><span class=p>:</span> <span class=n>optimize</span><span class=p>(</span><span class=s1>&#39;sgd&#39;</span><span class=p>,</span> <span class=n>x0</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.0005</span><span class=p>,</span> <span class=n>steps</span><span class=o>=</span><span class=n>steps</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Momentum&#39;</span><span class=p>:</span> <span class=n>optimize</span><span class=p>(</span><span class=s1>&#39;momentum&#39;</span><span class=p>,</span> <span class=n>x0</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.0001</span><span class=p>,</span> <span class=n>steps</span><span class=o>=</span><span class=n>steps</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;RMSProp&#39;</span><span class=p>:</span> <span class=n>optimize</span><span class=p>(</span><span class=s1>&#39;rmsprop&#39;</span><span class=p>,</span> <span class=n>x0</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.001</span><span class=p>,</span> <span class=n>steps</span><span class=o>=</span><span class=n>steps</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;Adam&#39;</span><span class=p>:</span> <span class=n>optimize</span><span class=p>(</span><span class=s1>&#39;adam&#39;</span><span class=p>,</span> <span class=n>x0</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=mf>0.005</span><span class=p>,</span> <span class=n>steps</span><span class=o>=</span><span class=n>steps</span><span class=p>),</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- 等高線プロット ---</span>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>X</span><span class=p>,</span> <span class=n>Y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>200</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>linspace</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>200</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>rosenbrock</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>contour</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>,</span> <span class=n>Z</span><span class=p>,</span> <span class=n>levels</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>logspace</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mf>3.5</span><span class=p>,</span> <span class=mi>20</span><span class=p>),</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;gray&#39;</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.3</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>colors</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;SGD&#39;</span><span class=p>:</span> <span class=s1>&#39;blue&#39;</span><span class=p>,</span> <span class=s1>&#39;Momentum&#39;</span><span class=p>:</span> <span class=s1>&#39;green&#39;</span><span class=p>,</span> <span class=s1>&#39;RMSProp&#39;</span><span class=p>:</span> <span class=s1>&#39;orange&#39;</span><span class=p>,</span> <span class=s1>&#39;Adam&#39;</span><span class=p>:</span> <span class=s1>&#39;red&#39;</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>name</span><span class=p>,</span> <span class=n>traj</span> <span class=ow>in</span> <span class=n>trajectories</span><span class=o>.</span><span class=n>items</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>traj</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>traj</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=s1>&#39;-&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=n>colors</span><span class=p>[</span><span class=n>name</span><span class=p>],</span> <span class=n>label</span><span class=o>=</span><span class=n>name</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.7</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>traj</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>traj</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=s1>&#39;o&#39;</span><span class=p>,</span> <span class=n>color</span><span class=o>=</span><span class=n>colors</span><span class=p>[</span><span class=n>name</span><span class=p>],</span> <span class=n>markersize</span><span class=o>=</span><span class=mi>6</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=s1>&#39;k*&#39;</span><span class=p>,</span> <span class=n>markersize</span><span class=o>=</span><span class=mi>15</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Optimum (1,1)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s1>&#39;x&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s1>&#39;y&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s1>&#39;Optimizer Trajectories on Rosenbrock Function&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlim</span><span class=p>(</span><span class=o>-</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylim</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=実用的な選択指針>実用的な選択指針</h2><ul><li><strong>まず Adam を試す</strong>: ほとんどのタスクで安定した性能</li><li><strong>汎化性能が重要</strong>: SGD + Momentum + 学習率スケジューリング</li><li><strong>スパースデータ（NLP等）</strong>: Adam または AdaGrad</li><li><strong>学習率調整の手間を省きたい</strong>: Adam（適応的学習率により頑健）</li></ul><h2 id=関連記事>関連記事</h2><ul><li><a href=https://yuhi-sa.github.io/posts/20210329_cem/1/>クロスエントロピー法：モンテカルロ最適化の実践的手法</a> - 勾配を使わないブラックボックス最適化手法との比較が有益です。</li><li><a href=https://yuhi-sa.github.io/posts/20260223_bayesian_optimization/1/>ベイズ最適化の基礎とPython実装</a> - ハイパーパラメータ（学習率等）の自動チューニングに使えるベイズ最適化を解説しています。</li><li><a href=https://yuhi-sa.github.io/posts/20210108_bayes/1/>ベイズ線形回帰の基礎</a> - 勾配降下法を使わないベイズ的なパラメータ推定を解説しています。</li></ul><h2 id=参考文献>参考文献</h2><ul><li>Kingma, D. P., & Ba, J. (2015). &ldquo;Adam: A Method for Stochastic Optimization&rdquo;. <em>ICLR 2015</em>.</li><li>Ruder, S. (2016). &ldquo;An overview of gradient descent optimization algorithms&rdquo;. arXiv:1609.04747.</li><li>Goodfellow, I., Bengio, Y., & Courville, A. (2016). <em>Deep Learning</em>. MIT Press. Chapter 8.</li></ul></div><div class="ad-slot in-content my-3"><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9558545098866170></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=article-footer></footer><meta itemprop=wordCount content="599"><meta itemprop=url content="https://yuhi-sa.github.io/posts/20260226_sgd_adam/1/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/posts/20260226_ensemble_learning/1/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>アンサンブル学習：決定木からランダムフォレスト・勾配ブースティングまで</span>
</a><a href=/posts/20260226_kmeans_gmm/1/ class="post-nav__item post-nav__item--next"><span class="post-nav__label text-muted">次の記事<i class="fas fa-arrow-right ms-1" aria-hidden=true></i>
</span><span class=post-nav__title>k-means法とGMM：クラスタリング手法の理論とPython実装</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/posts/20260223_bayesian_optimization/1/>ベイズ最適化の基礎とPython実装</a></h3><div class=card-meta><time datetime=2026-02-23>February 23, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/posts/20260215_mppi/1/>MPPI（Model Predictive Path Integral）の数理：クロスエントロピー法との統一的理解</a></h3><div class=card-meta><time datetime=2026-02-15>February 15, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/posts/20260226_svm/1/>サポートベクターマシン（SVM）：カーネル法と非線形分類のPython実装</a></h3><div class=card-meta><time datetime=2026-02-26>February 26, 2026</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>確率的勾配降下法からAdamまで：勾配ベース最適化の進化</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2026 yuhi-sa. All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"確率的勾配降下法からAdamまで：勾配ベース最適化の進化","url":"https:\/\/yuhi-sa.github.io\/posts\/20260226_sgd_adam\/1\/","description":"SGD、Momentum、RMSProp、Adamなど勾配ベース最適化アルゴリズムの数理と特性を比較し、PyTorchでの実装と収束挙動の可視化を紹介します。","inLanguage":"ja","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>