<!doctype html><html lang=ja dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="勉強したことなどをメモしています"><link rel=canonical href=https://yuhi-sa.github.io/posts/20210108_bayes/1/><meta property="og:type" content="article"><meta property="og:title" content="ベイズ線形回帰の基礎：最小二乗法からベイズ推定まで"><meta property="og:description" content="勉強したことなどをメモしています"><meta property="og:url" content="https://yuhi-sa.github.io/posts/20210108_bayes/1/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="ja"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2021-01-08T15:17:23+09:00"><meta property="article:modified_time" content="2025-07-25T17:16:12+09:00"><meta property="article:tag" content="ベイズ統計"><meta property="article:tag" content="機械学習"><meta property="article:tag" content="Python"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="ベイズ線形回帰の基礎：最小二乗法からベイズ推定まで"><meta name=twitter:description content="勉強したことなどをメモしています"><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>ベイズ線形回帰の基礎：最小二乗法からベイズ推定まで | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ベイズ線形回帰の基礎：最小二乗法からベイズ推定まで","description":"","author":{"@type":"Person","name":""},"publisher":{"@type":"Organization","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2021-01-08T15:17:23\u002b09:00","dateModified":"2025-07-25T17:16:12\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/posts\/20210108_bayes\/1\/"},"url":"https:\/\/yuhi-sa.github.io\/posts\/20210108_bayes\/1\/","wordCount":281,"keywords":["ベイズ統計","機械学習","Python"],"articleSection":"Posts","inLanguage":"ja","timeRequired":"PT2M"}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.963335e1959a83c16c6f09e75be26e42b746222835a882f77a89d68f9883aafe.css integrity="sha256-ljM14ZWag8FsbwnnW+JuQrdGIig1qIL3eonWj5iDqv4=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.65d091264255d75817988ed1dce4adcef2482557713854f8aaa3dde137672915.css integrity="sha256-ZdCRJkJV11gXmI7R3OStzvJIJVdxOFT4qqPd4TdnKRU=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.20b142b4e47bd9042b2d9fffa65797c455e75d76c4797957d567b04a03fd8b0c.css integrity="sha256-ILFCtOR72QQrLZ//pleXxFXnXXbEeXlX1WewSgP9iww=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script async src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-LN6QP6VVM3")</script><script data-ad-client=ca-pub-9558545098866170 async src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>ベイズ線形回帰の基礎：最小二乗法からベイズ推定まで</h1><div class=article-meta><time datetime=2021-01-08T15:17:23+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
January 8, 2021
</time><time datetime=2025-07-25T17:16:12+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated July 25, 2025
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
2 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/tags/%E3%83%99%E3%82%A4%E3%82%BA%E7%B5%B1%E8%A8%88/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>ベイズ統計
</a><a href=/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>機械学習
</a><a href=/tags/python/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Python</a></div></header><div class=article-content itemprop=articleBody><h2 id=はじめに>はじめに</h2><p>回帰問題は、入力データとそれに対応する出力データの関係を学習し、未知の入力に対する出力を予測するタスクです。本記事では、最も基本的な回帰モデルである<strong>線形回帰</strong>を題材に、パラメータの推定方法として代表的な<strong>最小二乗法</strong>、<strong>最尤推定</strong>、<strong>MAP推定</strong>、そして<strong>ベイズ推定</strong>を比較し、その違いと特徴を解説します。</p><p>モデルとして、入力 $x$ の非線形関数である<strong>基底関数</strong> $\phi(x)$ の線形結合を考えます。$w$ をモデルのパラメータ（重み）、$\epsilon$ を誤差とすると、モデルは以下のように表せます。</p><p>$$ y = \Phi w + \epsilon $$</p><p>ここで、$\Phi$ は計画行列と呼ばれ、各データ点の基底関数ベクトルを並べたものです。</p><hr><h2 id=1-最小二乗法-least-squares-estimation>1. 最小二乗法 (Least Squares Estimation)</h2><p>最小二乗法は、モデルの予測値と実際の目標値との<strong>誤差の二乗和</strong> $S(w)$ を最小化するパラメータ $\hat{w}$ を見つける手法です。</p><p>$$ S(w) = (y - \Phi w)^T (y - \Phi w) $$</p><p>$S(w)$ を $w$ で微分してゼロとおくことで、以下の解が得られます。</p><p>$$ \hat{w}_{LS} = (\Phi^T \Phi)^{-1} \Phi^T y $$</p><p>これは解析的に解けるため、計算が非常に高速です。しかし、訓練データ数が少ない場合や、モデルの自由度が高い場合に**過学習（オーバーフィッティング）**を起こしやすいという欠点があります。</p><h2 id=2-最尤推定-maximum-likelihood-estimation>2. 最尤推定 (Maximum Likelihood Estimation)</h2><p>最尤推定は、観測データが得られる確率（<strong>尤度</strong>）を最大化するパラメータ $\hat{w}$ を見つける手法です。</p><p>ここで、誤差項 $\epsilon$ が平均 $0$、分散 $\sigma^2$ のガウス分布に従うと仮定します。すると、目標値 $y$ の条件付き確率は、平均 $\Phi w$、分散 $\sigma^2$ のガウス分布となります。</p><p>$$ p(y | w, \sigma^2) = \mathcal{N}(y | \Phi w, \sigma^2 I) = \frac{1}{(2\pi\sigma^2)^{N/2}} \exp\lbrace-\frac{1}{2\sigma^2}(y-\Phi w)^T(y-\Phi w)\rbrace $$</p><p>この尤度の対数をとった<strong>対数尤度</strong>を最大化します。</p><p>$$ \ln p(y|w) = -\frac{N}{2}\ln(2\pi\sigma^2) - \frac{1}{2\sigma^2}(y-\Phi w)^T(y-\Phi w) $$</p><p>対数尤度を最大化することは、右辺第二項の二乗誤差項を最小化することと等価です。したがって、<strong>誤差にガウス分布を仮定した場合、最尤推定は最小二乗法と等価な解を与えます。</strong></p><p>$$ \hat{w}_{ML} = (\Phi^T\Phi)^{-1}\Phi^{T}y $$</p><h2 id=3-map推定-maximum-a-posteriori-estimation>3. MAP推定 (Maximum A Posteriori Estimation)</h2><p>MAP推定は、過学習を抑制するための一般的な枠組みです。パラメータ $w$ 自身も確率変数であると考え、その<strong>事前分布</strong> $p(w)$ を導入します。ベイズの定理を用いて、データが観測された後での $w$ の<strong>事後分布</strong> $p(w|y)$ を考え、この事後確率が最大となる $\hat{w}$ を求めます。</p><p>$$ p(w|y) = \frac{p(y|w)p(w)}{p(y)} \propto p(y|w)p(w) $$</p><p>$w$ の事前分布として、平均 $0$、共分散 $\alpha^{-1}I$ のガウス分布を仮定します。これは「$w$ の各要素は0に近い値をとるだろう」という事前知識をモデルに与えることに相当し、大きな値をとる重みにペナルティを課す<strong>正則化</strong>として機能します。</p><p>$$ p(w|\alpha) = \mathcal{N}(w|0, \alpha^{-1}I) $$</p><p>このとき、事後分布を最大化する $\hat{w}_{MAP}$ は、以下のようになります。</p><p>$$ \hat{w}_{MAP} = (\Phi^T\Phi + \frac{\beta}{\alpha}I)^{-1}\Phi^{T}y $$</p><p>ここで、$\beta = 1/\sigma^2$ です。この解は、<strong>リッジ回帰（L2正則化付き最小二乗法）</strong> と呼ばれる手法と同じ形をしており、正則化項 $\frac{\beta}{\alpha}I$ のおかげで、$\Phi^T\Phi$ が正則でない（逆行列を持たない）場合でも安定して解を求めることができ、過学習を抑制します。</p><h2 id=4-ベイズ推定-bayesian-estimation>4. ベイズ推定 (Bayesian Estimation)</h2><p>最小二乗法、最尤推定、MAP推定は、いずれも最適なパラメータ $w$ を一つの値（点推定）として求めました。しかし、このアプローチでは「パラメータがどのくらい確からしいか」という<strong>不確実性</strong>を表現できません。</p><p>ベイズ推定では、$w$ を点として求めるのではなく、<strong>事後分布 $p(w|y)$ そのものを求めます</strong>。この分布は、データを見た後での、あり得る全ての $w$ の値に対する確率分布を表します。</p><p>MAP推定で用いた事前分布と尤度（どちらもガウス分布）を用いると、事後分布もまたガウス分布になることが知られています（共役性）。</p><p>$$ p(w|y) = \mathcal{N}(w | \mu_N, \Sigma_N) $$</p><p>ここで、事後分布の平均 $\mu_N$ と共分散 $\Sigma_N$ は以下で与えられます。</p><p>$$ \mu_N = (\Phi^T\Phi + \frac{\beta}{\alpha}I)^{-1}\Phi^{T}y $$
$$ \Sigma_N = (\beta\Phi^T\Phi + \alpha I)^{-1} $$</p><p>新しい入力 $x_*$ に対する予測を行うには、あり得る全ての $w$ で予測を行い、それを事後確率で重み付けして平均します。これを<strong>予測分布</strong>と呼びます。</p><p>$$ p(y_* | x_<em>, y) = \int p(y_</em> | x_*, w) p(w|y) dw $$</p><p>この予測分布は、予測の平均値だけでなく、その予測がどの程度不確かであるかを示す**分散（信頼区間）**も与えてくれます。</p><h2 id=実験結果>実験結果</h2><ul><li><strong>データ</strong>: $y = \sin(2\pi x)$ からノイズを加えて生成した15点の訓練データ</li><li><strong>基底関数</strong>: 9次の多項式 ($f_j(x) = x^j, j=0, &mldr;, 9$)</li><li><strong>ハイパーパラメータ</strong>: $\alpha=1.0, \beta=10.0$</li></ul><h4 id=最小二乗推定--最尤推定>最小二乗推定 / 最尤推定</h4><p>訓練データに強く適合しようとするため、データのない領域で予測が大きく振動し、過学習を起こしています。
<img src=.././fig1.png alt=1.png></p><h4 id=map推定>MAP推定</h4><p>事前分布（正則化）の効果により、過学習が抑制され、より滑らかな予測曲線が得られています。
<img src=.././fig2.png alt=3.png></p><h4 id=ベイズ推定>ベイズ推定</h4><p>MAP推定と同様に滑らかな予測（平均値、実線）が得られると同時に、訓練データが少ない領域では予測の不確実性が増大し、信頼区間（青い影の領域）が広がっていることがわかります。
<img src=.././fig3.png alt=4.png></p><h4 id=モデルの評価決定係数-r2>モデルの評価（決定係数 $R^2$）</h4><p>決定係数は、モデルの当てはまりの良さを示す指標（1に近いほど良い）です。MAP推定とベイズ推定（の平均）が、最小二乗法よりも高いスコアを示しています。
<img src=.././fig4.png alt=fig8.png></p><h4 id=ハイパーパラメータ-beta-の影響>ハイパーパラメータ $\beta$ の影響</h4><p>ベイズ推定において、尤度の精度パラメータ $\beta$（ノイズの逆分散）を大きくすると、モデルは訓練データにより強く適合しようとします。値を大きくしすぎると、信頼区間が狭まり、過学習に近づいていく様子がわかります。</p><ul><li>$\beta=50$
<img src=.././fig5.png alt=50.png></li><li>$\beta=100$
<img src=.././fig6.png alt=100.png></li><li>$\beta=1000$
<img src=.././fig7.png alt=1000.png></li></ul><h2 id=まとめ>まとめ</h2><ul><li><strong>最小二乗法・最尤推定</strong>: シンプルで高速だが、過学習しやすい。</li><li><strong>MAP推定</strong>: 事前分布（正則化）を導入することで、過学習を抑制できる。</li><li><strong>ベイズ推定</strong>: パラメータの不確実性を考慮し、予測の信頼区間を得ることができる。データが少ない領域で不確実性が増大するなど、よりリッチな情報を提供してくれる。</li></ul><h2 id=参考文献>参考文献</h2><ul><li>C.M. ビショップ, 『パターン認識と機械学習 上』, 丸善出版 (2012)</li><li>須山 敦志, 『ベイズ推論による機械学習入門』, 講談社 (2017)</li></ul></div><footer class=article-footer></footer><meta itemprop=wordCount content="281"><meta itemprop=url content="https://yuhi-sa.github.io/posts/20210108_bayes/1/"></article><nav class="article-navigation mt-5" aria-label="Article navigation"><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>ベイズ線形回帰の基礎：最小二乗法からベイズ推定まで</span>
<meta itemprop=position content="4"></li></ol></nav><div class=terms-wrapper role=region aria-label=Tags><h6 class=terms-label id=terms-tags>Tags:</h6><ul class=terms-list role=list aria-labelledby=terms-tags aria-describedby=terms-count-tags><li class=terms-item role=listitem><a href=/tags/%E3%83%99%E3%82%A4%E3%82%BA%E7%B5%B1%E8%A8%88/ class="terms-link badge badge-custom text-decoration-none" rel=tag aria-label="View all posts tagged with ベイズ統計" title="ベイズ統計 (0 posts)">ベイズ統計</a></li><li class=terms-item role=listitem><a href=/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/ class="terms-link badge badge-custom text-decoration-none" rel=tag aria-label="View all posts tagged with 機械学習" title="機械学習 (0 posts)">機械学習</a></li><li class=terms-item role=listitem><a href=/tags/python/ class="terms-link badge badge-custom text-decoration-none" rel=tag aria-label="View all posts tagged with Python" title="Python (14 posts)">Python</a></li></ul><span id=terms-count-tags class=sr-only>3 tags total</span></div></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container py-5"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2025
tomato blog.
All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>"serviceWorker"in navigator&&window.addEventListener("load",function(){navigator.serviceWorker.register("/sw.js").then(function(e){console.log("SW registered: ",e)}).catch(function(e){console.log("SW registration failed: ",e)})})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"ベイズ線形回帰の基礎：最小二乗法からベイズ推定まで","url":"https:\/\/yuhi-sa.github.io\/posts\/20210108_bayes\/1\/","description":"勉強したことなどをメモしています","inLanguage":"ja","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>