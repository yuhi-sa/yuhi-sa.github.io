<!doctype html><html lang=ja dir=ltr data-theme=light><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=theme-color content="#e74c3c"><meta name=format-detection content="telephone=no"><meta name=robots content="index,follow"><meta name=description content="TransformerのコアであるSelf-Attention機構を数式から丁寧に解説し、NumPyによるスクラッチ実装とPyTorchのnn.MultiheadAttentionとの比較を通じて仕組みを理解します。"><meta name=keywords content="Self-Attention,Transformer,Attention,Query Key Value,Multi-Head Attention,Python,NumPy,PyTorch,深層学習,自然言語処理"><meta name=author content="yuhi-sa"><link rel=canonical href=https://yuhi-sa.github.io/posts/20260228_self_attention/1/><meta property="og:type" content="article"><meta property="og:title" content="Pythonで理解するSelf-Attentionの仕組み：数式からスクラッチ実装まで"><meta property="og:description" content="TransformerのコアであるSelf-Attention機構を数式から丁寧に解説し、NumPyによるスクラッチ実装とPyTorchのnn.MultiheadAttentionとの比較を通じて仕組みを理解します。"><meta property="og:url" content="https://yuhi-sa.github.io/posts/20260228_self_attention/1/"><meta property="og:site_name" content="tomato blog"><meta property="og:locale" content="ja"><meta property="og:image" content="https://yuhi-sa.github.io/ogp.jpeg"><meta property="og:image:width" content="1200"><meta property="og:image:height" content="630"><meta property="article:published_time" content="2026-02-28T13:00:00+09:00"><meta property="article:modified_time" content="2026-02-28T19:16:58+09:00"><meta property="article:tag" content="深層学習"><meta property="article:tag" content="機械学習"><meta property="article:tag" content="Python"><meta property="article:section" content="posts"><meta name=twitter:card content="summary_large_image"><meta name=twitter:title content="Pythonで理解するSelf-Attentionの仕組み：数式からスクラッチ実装まで"><meta name=twitter:description content="TransformerのコアであるSelf-Attention機構を数式から丁寧に解説し、NumPyによるスクラッチ実装とPyTorchのnn.MultiheadAttentionとの比較を通じて仕組みを理解します。"><meta name=twitter:image content="https://yuhi-sa.github.io/ogp.jpeg"><title>Pythonで理解するSelf-Attentionの仕組み：数式からスクラッチ実装まで | tomato blog</title><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preconnect href=https://cdn.jsdelivr.net crossorigin><link rel=preconnect href=https://pagead2.googlesyndication.com crossorigin><link rel=dns-prefetch href=https://pagead2.googlesyndication.com><link rel=icon href=https://yuhi-sa.github.io/favicon.ico><link rel=alternate hreflang=en href=https://yuhi-sa.github.io/en/posts/20260228_self_attention/1/><link rel=alternate hreflang=ja href=https://yuhi-sa.github.io/posts/20260228_self_attention/1/><link rel=alternate hreflang=x-default href=https://yuhi-sa.github.io/posts/20260228_self_attention/1/><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Pythonで理解するSelf-Attentionの仕組み：数式からスクラッチ実装まで","description":"TransformerのコアであるSelf-Attention機構を数式から丁寧に解説し、NumPyによるスクラッチ実装とPyTorchのnn.MultiheadAttentionとの比較を通じて仕組みを理解します。","author":{"@type":"Person","name":"yuhi-sa","url":"https:\/\/yuhi-sa.github.io\/"},"publisher":{"@type":"Organization","name":"tomato blog","logo":{"@type":"ImageObject","url":"https:\/\/yuhi-sa.github.io\/ogp.jpeg"},"url":"https:\/\/yuhi-sa.github.io\/"},"datePublished":"2026-02-28T13:00:00\u002b09:00","dateModified":"2026-02-28T19:16:58\u002b09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/yuhi-sa.github.io\/posts\/20260228_self_attention\/1\/"},"url":"https:\/\/yuhi-sa.github.io\/posts\/20260228_self_attention\/1\/","wordCount":869,"keywords":["深層学習","機械学習","Python"],"articleSection":"Posts","inLanguage":"ja","timeRequired":"PT5M"}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"tomato blog","item":"https:\/\/yuhi-sa.github.io\/"},{"@type":"ListItem","position":2,"name":"Posts","item":"https:\/\/yuhi-sa.github.io\/posts\/"},{"@type":"ListItem","position":3,"name":"Pythonで理解するSelf-Attentionの仕組み：数式からスクラッチ実装まで"}]}</script><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin=anonymous referrerpolicy=no-referrer><link href=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css rel=stylesheet integrity=sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH crossorigin=anonymous><link rel=stylesheet href=/css/variables.min.72a177faa7b12de55dcc39c4ab6b6392116718d5e2735dab0214511354ecb973.css integrity="sha256-cqF3+qexLeVdzDnEq2tjkhFnGNXic12rAhRRE1TsuXM=" crossorigin=anonymous><link rel=stylesheet href=/css/main.min.403a82b29e94ee6e7a39204b5082cdf8b4966cfb44115390da7d8867a5006acd.css integrity="sha256-QDqCsp6U7m56OSBLUILN+LSWbPtEEVOQ2n2IZ6UAas0=" crossorigin=anonymous><link rel=stylesheet href=/css/syntax.min.e379066489e20d5433ca35ac1f468fd9e8859705a62d77a79bb7379ac3613848.css integrity="sha256-43kGZIniDVQzyjWsH0aP2eiFlwWmLXenm7c3msNhOEg=" crossorigin=anonymous><style>body{font-family:-apple-system,BlinkMacSystemFont,inter,segoe ui,Roboto,sans-serif;line-height:1.5;color:#000;background:#fff}[data-theme=dark] body{color:#fff;background:#000}.container{max-width:768px;margin:0 auto;padding:0 1rem}</style><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script>window.MathJax={tex:{inlineMath:[["\\(","\\)"]],displayMath:[["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre","code"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}}</script><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js id=mathjax-script async></script></head><body itemscope itemtype=https://schema.org/WebPage class=theme-tomatohugo><a href=#main-content class="skip-link sr-only sr-only-focusable" aria-label="Skip to main content">Skip to main content</a><header role=banner class=site-header><nav class="navbar navbar-expand-lg navbar-light bg-light" role=navigation aria-label="Main navigation"><div class=container><a class=navbar-brand href=https://yuhi-sa.github.io/ aria-label="Return to tomato blog homepage">tomato blog
</a><button class="navbar-toggler d-lg-none" type=button data-bs-toggle=collapse data-bs-target=#navbarNav aria-controls=navbarNav aria-expanded=false aria-label="Toggle navigation menu">
<span class=navbar-toggler-icon></span></button><div class="collapse navbar-collapse" id=navbarNav><ul class="navbar-nav ms-auto" role=menubar><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/ role=menuitem aria-label="Navigate to Blog">Blog</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/tags/ role=menuitem aria-label="Navigate to Tags">Tags</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/posts/about/ role=menuitem aria-label="Navigate to About">About</a></li><li class=nav-item role=none><a class=nav-link href=https://yuhi-sa.github.io/posts/privacy_policy/ role=menuitem aria-label="Navigate to Privacy policy">Privacy policy</a></li><li class=nav-item role=none><button id=darkModeToggle class="nav-link btn btn-link border-0" type=button role=menuitem aria-label="Toggle dark mode" title="Switch between light and dark themes">
<i class="fas fa-moon" id=darkModeIcon aria-hidden=true></i>
<span class="d-lg-none ms-2">ダークモード</span></button></li></ul></div></div></nav><script data-ad-client=ca-pub-9558545098866170 async crossorigin=anonymous src=https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js></script></header><main id=main-content role=main class=site-main aria-label="Main content"><div class="container mt-4"><div class="row justify-content-center"><div class=col-lg-8><article itemscope itemtype=https://schema.org/Article><header class=article-header><h1 itemprop=headline>Pythonで理解するSelf-Attentionの仕組み：数式からスクラッチ実装まで</h1><p class="lead article-description" itemprop=description>TransformerのコアであるSelf-Attention機構を数式から丁寧に解説し、NumPyによるスクラッチ実装とPyTorchのnn.MultiheadAttentionとの比較を通じて仕組みを理解します。</p><div class=article-meta><time datetime=2026-02-28T13:00:00+09:00 itemprop=datePublished><i class="far fa-calendar-alt me-1" aria-hidden=true></i>
February 28, 2026
</time><time datetime=2026-02-28T19:16:58+09:00 itemprop=dateModified class=ms-3><i class="far fa-edit me-1" aria-hidden=true></i>
Updated
February 28, 2026
</time><span aria-label="Reading time" class=ms-3><i class="far fa-clock me-1" aria-hidden=true></i>
5 min read</span></div><div class=article-tags role=group aria-label="Article tags"><a href=/tags/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>深層学習
</a><a href=/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>機械学習
</a><a href=/tags/python/ class="badge badge-custom text-decoration-none me-1" rel=tag itemprop=keywords>Python</a></div></header><div class=article-content itemprop=articleBody><h2 id=はじめに>はじめに</h2><p>Transformerは自然言語処理（NLP）を起点に、画像認識や音声処理など幅広い分野で標準的なアーキテクチャとなっています。その中核を担うのが<strong>Self-Attention</strong>機構です。</p><p>Self-Attentionは、入力系列の各要素が他のすべての要素との関連度を動的に計算し、文脈に応じた表現を獲得する仕組みです。固定的な重みで信号を処理する従来のフィルタ（たとえば<a href=https://yuhi-sa.github.io/posts/20220206_ema/1/>指数移動平均</a>）とは異なり、入力データに依存して重みが変化する点が大きな特徴です。</p><p>本記事では、Scaled Dot-Product AttentionとMulti-Head Attentionの数式を導出し、NumPyでスクラッチ実装した上で、PyTorchの<code>nn.MultiheadAttention</code>との比較を通じて仕組みを理解します。</p><h2 id=なぜattentionが必要なのか>なぜAttentionが必要なのか</h2><p>RNN（再帰型ニューラルネットワーク）は系列データの処理に広く使われてきましたが、2つの根本的な課題があります。</p><ol><li><strong>逐次処理</strong>: 時刻 \(t\) の計算が時刻 \(t-1\) の結果に依存するため、並列化が困難</li><li><strong>長距離依存性の学習困難</strong>: 系列が長くなると勾配消失・爆発により、離れた位置間の関係を学習しにくい</li></ol><p>Attention機構はこれらの問題を解決します。各位置が他のすべての位置に直接アクセスでき、系列長に依存する逐次計算が不要です。さらに、Attention重みは入力から動的に計算されるため、固定的な構造に縛られません。</p><h2 id=scaled-dot-product-attention>Scaled Dot-Product Attention</h2><h3 id=query-key-valueの導出>Query, Key, Valueの導出</h3><p>入力系列 \(X \in \mathbb{R}^{n \times d_{\text{model}}}\)（\(n\) はトークン数、\(d_{\text{model}}\) はモデルの次元数）に対し、3つの線形変換を適用してQuery、Key、Valueを生成します。</p>\[Q = XW_Q, \quad K = XW_K, \quad V = XW_V \tag{1}\]<p>ここで \(W_Q, W_K \in \mathbb{R}^{d_{\text{model}} \times d_k}\)、\(W_V \in \mathbb{R}^{d_{\text{model}} \times d_v}\) は学習可能な重み行列です。直感的には、Queryは「何を探しているか」、Keyは「何を持っているか」、Valueは「実際の情報」に対応します。</p><h3 id=attention計算>Attention計算</h3><p>Attention関数は以下で定義されます。</p>\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \tag{2}\]<p>この計算を分解して理解します。</p><p><strong>ステップ1: 類似度の計算</strong></p>\[S = QK^T \in \mathbb{R}^{n \times n} \tag{3}\]<p>\(S_{ij}\) はトークン \(i\) のQueryとトークン \(j\) のKeyの内積であり、2つのトークン間の類似度を表します。</p><p><strong>ステップ2: スケーリング</strong></p>\[S_{\text{scaled}} = \frac{S}{\sqrt{d_k}} \tag{4}\]<p>\(d_k\) が大きいとき、内積の値も大きくなります。\(q\) と \(k\) が平均0、分散1の独立な成分を持つとき、内積 \(q \cdot k = \sum_{i=1}^{d_k} q_i k_i\) の分散は \(d_k\) になります。大きな値はsoftmaxを飽和領域に押し込み、勾配が極端に小さくなります。\(\sqrt{d_k}\) で割ることで分散を1に正規化し、この問題を回避します。</p><p><strong>ステップ3: Attention重みの計算</strong></p>\[A = \text{softmax}(S_{\text{scaled}}) \tag{5}\]<p>softmaxにより各行が確率分布（合計1）になります。\(A_{ij}\) はトークン \(i\) がトークン \(j\) にどれだけ注目するかを表します。</p><p><strong>ステップ4: 重み付き和</strong></p>\[\text{Output} = AV \tag{6}\]<p>各トークンの出力は、すべてのValueベクトルのAttention重みによる加重和です。これはソフトな辞書検索とみなせます。Queryで検索し、Keyとのマッチ度に応じてValueを取り出す操作です。</p><h2 id=numpyによるスクラッチ実装>NumPyによるスクラッチ実装</h2><h3 id=scaled-dot-product-attention-1>Scaled Dot-Product Attention</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>softmax</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;数値的に安定なsoftmax&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>e_x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span> <span class=o>-</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=n>axis</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>e_x</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>e_x</span><span class=p>,</span> <span class=n>axis</span><span class=o>=</span><span class=n>axis</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>scaled_dot_product_attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Scaled Dot-Product Attention（式2）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters:
</span></span></span><span class=line><span class=cl><span class=s2>        Q: Query行列 (n, d_k)
</span></span></span><span class=line><span class=cl><span class=s2>        K: Key行列   (n, d_k)
</span></span></span><span class=line><span class=cl><span class=s2>        V: Value行列  (n, d_v)
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        output: Attention出力 (n, d_v)
</span></span></span><span class=line><span class=cl><span class=s2>        weights: Attention重み (n, n)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>d_k</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># ステップ1-2: 類似度計算とスケーリング</span>
</span></span><span class=line><span class=cl>    <span class=n>scores</span> <span class=o>=</span> <span class=n>Q</span> <span class=o>@</span> <span class=n>K</span><span class=o>.</span><span class=n>T</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>d_k</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># ステップ3: Attention重み</span>
</span></span><span class=line><span class=cl>    <span class=n>weights</span> <span class=o>=</span> <span class=n>softmax</span><span class=p>(</span><span class=n>scores</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=c1># ステップ4: 重み付き和</span>
</span></span><span class=line><span class=cl>    <span class=n>output</span> <span class=o>=</span> <span class=n>weights</span> <span class=o>@</span> <span class=n>V</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>output</span><span class=p>,</span> <span class=n>weights</span>
</span></span></code></pre></div><h3 id=動作確認と可視化>動作確認と可視化</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 入力: 4トークン、モデル次元8</span>
</span></span><span class=line><span class=cl><span class=n>n_tokens</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_v</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># ランダムな入力系列</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 重み行列（通常は学習で獲得される）</span>
</span></span><span class=line><span class=cl><span class=n>W_Q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=n>W_K</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_k</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl><span class=n>W_V</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>d_v</span><span class=p>)</span> <span class=o>*</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Q, K, V の計算（式1）</span>
</span></span><span class=line><span class=cl><span class=n>Q</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=n>W_Q</span>
</span></span><span class=line><span class=cl><span class=n>K</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=n>W_K</span>
</span></span><span class=line><span class=cl><span class=n>V</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=n>W_V</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Attention計算</span>
</span></span><span class=line><span class=cl><span class=n>output</span><span class=p>,</span> <span class=n>weights</span> <span class=o>=</span> <span class=n>scaled_dot_product_attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;入力形状:&#34;</span><span class=p>,</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;出力形状:&#34;</span><span class=p>,</span> <span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Attention重み:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Attention重みのヒートマップ</span>
</span></span><span class=line><span class=cl><span class=n>token_labels</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;Token 0&#34;</span><span class=p>,</span> <span class=s2>&#34;Token 1&#34;</span><span class=p>,</span> <span class=s2>&#34;Token 2&#34;</span><span class=p>,</span> <span class=s2>&#34;Token 3&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span> <span class=mi>5</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>im</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>weights</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;Blues&#34;</span><span class=p>,</span> <span class=n>vmin</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>vmax</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xticks</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_yticks</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xticklabels</span><span class=p>(</span><span class=n>token_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_yticklabels</span><span class=p>(</span><span class=n>token_labels</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s2>&#34;Key position&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s2>&#34;Query position&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s2>&#34;Attention Weights&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>ax</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>j</span><span class=p>,</span> <span class=n>i</span><span class=p>,</span> <span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>weights</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>]</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>ha</span><span class=o>=</span><span class=s2>&#34;center&#34;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s2>&#34;center&#34;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>11</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>im</span><span class=p>,</span> <span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=multi-head-attention>Multi-Head Attention</h2><h3 id=動機と定式化>動機と定式化</h3><p>単一のAttentionヘッドでは、1つの表現空間でしか類似度を計算できません。Multi-Head Attentionは、Q, K, Vを \(h\) 個の異なる部分空間に射影し、それぞれで独立にAttentionを計算することで、多様な関係性を同時に捉えます。</p>\[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W_O \tag{7}\]\[\text{head}_i = \text{Attention}(QW_Q^i, KW_K^i, VW_V^i) \tag{8}\]<p>各ヘッドの次元は \(d_k = d_v = d_{\text{model}} / h\) とするのが標準的です。\(W_O \in \mathbb{R}^{hd_v \times d_{\text{model}}}\) は出力射影行列です。パラメータ数は単一ヘッドのAttentionとほぼ同じですが、異なる部分空間での表現を獲得できます。</p><h3 id=numpy実装>NumPy実装</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>MultiHeadAttention</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;Multi-Head Attention（式7, 8）&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>0</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>assert</span> <span class=n>d_model</span> <span class=o>%</span> <span class=n>n_heads</span> <span class=o>==</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_model</span> <span class=o>=</span> <span class=n>d_model</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span> <span class=o>=</span> <span class=n>n_heads</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>n_heads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>rng</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>RandomState</span><span class=p>(</span><span class=n>seed</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>scale</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 各ヘッドの射影行列</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_Q</span> <span class=o>=</span> <span class=n>rng</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_heads</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span> <span class=o>*</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_K</span> <span class=o>=</span> <span class=n>rng</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_heads</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span> <span class=o>*</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_V</span> <span class=o>=</span> <span class=n>rng</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_heads</span><span class=p>,</span> <span class=n>d_model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>)</span> <span class=o>*</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>        <span class=c1># 出力射影行列</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>W_O</span> <span class=o>=</span> <span class=n>rng</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_heads</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>d_k</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span> <span class=o>*</span> <span class=n>scale</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>        Parameters:
</span></span></span><span class=line><span class=cl><span class=s2>            X: 入力系列 (n, d_model)
</span></span></span><span class=line><span class=cl><span class=s2>        Returns:
</span></span></span><span class=line><span class=cl><span class=s2>            output: Multi-Head Attention出力 (n, d_model)
</span></span></span><span class=line><span class=cl><span class=s2>            all_weights: 各ヘッドのAttention重み (n_heads, n, n)
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>head_outputs</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=n>all_weights</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>n_heads</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>Q</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_Q</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>K</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_K</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>V</span> <span class=o>=</span> <span class=n>X</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_V</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>head_out</span><span class=p>,</span> <span class=n>weights</span> <span class=o>=</span> <span class=n>scaled_dot_product_attention</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>K</span><span class=p>,</span> <span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>head_outputs</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>head_out</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>all_weights</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>weights</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># ヘッドの結合と出力射影（式7）</span>
</span></span><span class=line><span class=cl>        <span class=n>concat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>(</span><span class=n>head_outputs</span><span class=p>,</span> <span class=n>axis</span><span class=o>=-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>output</span> <span class=o>=</span> <span class=n>concat</span> <span class=o>@</span> <span class=bp>self</span><span class=o>.</span><span class=n>W_O</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>all_weights</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=multi-head-attentionの可視化>Multi-Head Attentionの可視化</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>n_tokens</span> <span class=o>=</span> <span class=mi>6</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>16</span>
</span></span><span class=line><span class=cl><span class=n>n_heads</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mha</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>output</span><span class=p>,</span> <span class=n>all_weights</span> <span class=o>=</span> <span class=n>mha</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;入力形状:&#34;</span><span class=p>,</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;出力形状:&#34;</span><span class=p>,</span> <span class=n>output</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Attention重み形状:&#34;</span><span class=p>,</span> <span class=n>all_weights</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 各ヘッドのAttention重みを可視化</span>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>axes</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>16</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>h</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_heads</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>im</span> <span class=o>=</span> <span class=n>axes</span><span class=p>[</span><span class=n>h</span><span class=p>]</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>all_weights</span><span class=p>[</span><span class=n>h</span><span class=p>],</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;Blues&#34;</span><span class=p>,</span> <span class=n>vmin</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>vmax</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>axes</span><span class=p>[</span><span class=n>h</span><span class=p>]</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;Head </span><span class=si>{</span><span class=n>h</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>axes</span><span class=p>[</span><span class=n>h</span><span class=p>]</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s2>&#34;Key&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>axes</span><span class=p>[</span><span class=n>h</span><span class=p>]</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s2>&#34;Query&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>suptitle</span><span class=p>(</span><span class=s2>&#34;Multi-Head Attention Weights&#34;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>14</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=位置エンコーディング>位置エンコーディング</h2><p>Self-Attentionは入力の順序に対して不変（permutation-invariant）です。つまり、トークンの並びを変えても出力は同じ集合になります。系列中の位置情報を注入するために、位置エンコーディング（Positional Encoding）が必要です。</p><p>Transformerの原論文では正弦波ベースの位置エンコーディングが提案されています。</p>\[PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \tag{9}\]\[PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \tag{10}\]<p>ここで \(pos\) は系列中の位置、\(i\) は次元のインデックスです。各次元で異なる周波数の正弦波を使うことで、位置ごとにユニークなパターンを生成します。</p><h3 id=実装と可視化>実装と可視化</h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>positional_encoding</span><span class=p>(</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    正弦波位置エンコーディング（式9, 10）
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Parameters:
</span></span></span><span class=line><span class=cl><span class=s2>        max_len: 最大系列長
</span></span></span><span class=line><span class=cl><span class=s2>        d_model: モデル次元
</span></span></span><span class=line><span class=cl><span class=s2>    Returns:
</span></span></span><span class=line><span class=cl><span class=s2>        PE: 位置エンコーディング行列 (max_len, d_model)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>PE</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>position</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>max_len</span><span class=p>)[:,</span> <span class=n>np</span><span class=o>.</span><span class=n>newaxis</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>div_term</span> <span class=o>=</span> <span class=mi>10000</span> <span class=o>**</span> <span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>d_model</span> <span class=o>//</span> <span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>PE</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sin</span><span class=p>(</span><span class=n>position</span> <span class=o>/</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>PE</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>::</span><span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>cos</span><span class=p>(</span><span class=n>position</span> <span class=o>/</span> <span class=n>div_term</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>PE</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 位置エンコーディングの可視化</span>
</span></span><span class=line><span class=cl><span class=n>max_len</span> <span class=o>=</span> <span class=mi>50</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>64</span>
</span></span><span class=line><span class=cl><span class=n>PE</span> <span class=o>=</span> <span class=n>positional_encoding</span><span class=p>(</span><span class=n>max_len</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>fig</span><span class=p>,</span> <span class=n>ax</span> <span class=o>=</span> <span class=n>plt</span><span class=o>.</span><span class=n>subplots</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>6</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>im</span> <span class=o>=</span> <span class=n>ax</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>PE</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s2>&#34;RdBu&#34;</span><span class=p>,</span> <span class=n>aspect</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_xlabel</span><span class=p>(</span><span class=s2>&#34;Dimension&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_ylabel</span><span class=p>(</span><span class=s2>&#34;Position&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>ax</span><span class=o>.</span><span class=n>set_title</span><span class=p>(</span><span class=s2>&#34;Sinusoidal Positional Encoding&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>colorbar</span><span class=p>(</span><span class=n>im</span><span class=p>,</span> <span class=n>ax</span><span class=o>=</span><span class=n>ax</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>tight_layout</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div><h2 id=pytorchのnnmultiheadattentionとの比較>PyTorchのnn.MultiheadAttentionとの比較</h2><p>PyTorchには<code>nn.MultiheadAttention</code>が実装されています。ここではスクラッチ実装と同一の重みを設定し、出力が一致することを確認します。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch.nn</span> <span class=k>as</span> <span class=nn>nn</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>42</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>n_tokens</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>d_model</span> <span class=o>=</span> <span class=mi>8</span>
</span></span><span class=line><span class=cl><span class=n>n_heads</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>d_k</span> <span class=o>=</span> <span class=n>d_model</span> <span class=o>//</span> <span class=n>n_heads</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 入力データ</span>
</span></span><span class=line><span class=cl><span class=n>X_np</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=n>n_tokens</span><span class=p>,</span> <span class=n>d_model</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- スクラッチ実装 ---</span>
</span></span><span class=line><span class=cl><span class=n>mha_np</span> <span class=o>=</span> <span class=n>MultiHeadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_np</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>mha_np</span><span class=o>.</span><span class=n>forward</span><span class=p>(</span><span class=n>X_np</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># --- PyTorch実装 ---</span>
</span></span><span class=line><span class=cl><span class=n>mha_pt</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>MultiheadAttention</span><span class=p>(</span><span class=n>d_model</span><span class=p>,</span> <span class=n>n_heads</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>batch_first</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># PyTorchのin_proj_weightにスクラッチ実装の重みをコピー</span>
</span></span><span class=line><span class=cl><span class=c1># PyTorchは[W_Q; W_K; W_V]を結合した形で保持 (3*d_model, d_model)</span>
</span></span><span class=line><span class=cl><span class=n>W_Q_cat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>mha_np</span><span class=o>.</span><span class=n>W_Q</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_heads</span><span class=p>)],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl><span class=n>W_K_cat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>mha_np</span><span class=o>.</span><span class=n>W_K</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_heads</span><span class=p>)],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl><span class=n>W_V_cat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>mha_np</span><span class=o>.</span><span class=n>W_V</span><span class=p>[</span><span class=n>i</span><span class=p>]</span> <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_heads</span><span class=p>)],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>T</span>
</span></span><span class=line><span class=cl><span class=n>in_proj_weight</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>W_Q_cat</span><span class=p>,</span> <span class=n>W_K_cat</span><span class=p>,</span> <span class=n>W_V_cat</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=n>mha_pt</span><span class=o>.</span><span class=n>in_proj_weight</span><span class=o>.</span><span class=n>copy_</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>in_proj_weight</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>mha_pt</span><span class=o>.</span><span class=n>out_proj</span><span class=o>.</span><span class=n>weight</span><span class=o>.</span><span class=n>copy_</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>mha_np</span><span class=o>.</span><span class=n>W_O</span><span class=o>.</span><span class=n>T</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># PyTorchはデフォルトで (seq_len, batch, d_model) の形式</span>
</span></span><span class=line><span class=cl><span class=n>X_pt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>from_numpy</span><span class=p>(</span><span class=n>X_np</span><span class=p>)</span><span class=o>.</span><span class=n>unsqueeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># (n_tokens, 1, d_model)</span>
</span></span><span class=line><span class=cl><span class=n>out_pt</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>mha_pt</span><span class=p>(</span><span class=n>X_pt</span><span class=p>,</span> <span class=n>X_pt</span><span class=p>,</span> <span class=n>X_pt</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>out_pt</span> <span class=o>=</span> <span class=n>out_pt</span><span class=o>.</span><span class=n>squeeze</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 比較</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;スクラッチ実装の出力:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>out_np</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;PyTorchの出力:</span><span class=se>\n</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>out_pt</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;最大誤差:&#34;</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>out_np</span> <span class=o>-</span> <span class=n>out_pt</span><span class=p>)))</span>
</span></span></code></pre></div><p>同一の重みを設定することで、両者の出力が数値誤差の範囲で一致することを確認できます。</p><h2 id=attentionの直感的な理解>Attentionの直感的な理解</h2><p>Self-Attentionの本質は、<strong>データに依存した重み付き平均</strong>です。</p><p>通常のフィルタ（<a href=https://yuhi-sa.github.io/posts/20260225_moving_average/1/>移動平均</a>や<a href=https://yuhi-sa.github.io/posts/20220206_ema/1/>EMA</a>など）では、重みは入力に関係なく事前に決められています。一方、Self-Attentionでは重み（Attention重み行列 \(A\)）が入力 \(X\) 自身から計算されます。</p><table><thead><tr><th>特性</th><th>固定重みフィルタ</th><th>Self-Attention</th></tr></thead><tbody><tr><td>重みの決定</td><td>事前に固定</td><td>入力から動的に計算</td></tr><tr><td>参照範囲</td><td>局所（窓幅に依存）</td><td>系列全体（大域的）</td></tr><tr><td>適応性</td><td>なし</td><td>入力ごとに重みが変化</td></tr><tr><td>計算量</td><td>\(O(n)\)</td><td>\(O(n^2)\)（系列長の2乗）</td></tr><tr><td>用途</td><td>信号平滑化、ノイズ除去</td><td>系列間の関係性モデリング</td></tr></tbody></table><p>この「入力依存の動的な重み付け」が、Attentionの強力さの源泉です。各トークンが文脈に応じて注目すべき相手を選択的に決定できるため、長距離依存性や複雑な構造を柔軟に捉えることができます。</p><h2 id=まとめ>まとめ</h2><ul><li><strong>Self-Attention</strong>は入力系列の各要素間の関連度を動的に計算し、文脈に応じた表現を獲得する機構</li><li><strong>Scaled Dot-Product Attention</strong>はQuery-Key間の内積を \(\sqrt{d_k}\) でスケーリングし、softmaxでAttention重みを求め、Valueの加重和を計算する</li><li><strong>Multi-Head Attention</strong>は複数の部分空間で独立にAttentionを計算することで、多様なパターンを同時に捉える</li><li><strong>位置エンコーディング</strong>は順序不変なSelf-Attentionに位置情報を注入する</li><li>Attentionの本質は<strong>データ依存の重み付き平均</strong>であり、固定重みフィルタとは根本的に異なる</li></ul><h2 id=関連記事>関連記事</h2><ul><li><a href=https://yuhi-sa.github.io/posts/20260226_sgd_adam/1/>確率的勾配降下法からAdamまで</a> - Transformerの学習に使われる最適化手法を解説しています。</li><li><a href=https://yuhi-sa.github.io/posts/20220206_ema/1/>指数移動平均（EMA）フィルタの周波数特性</a> - 固定重みフィルタとAttentionの動的重み付けの対比を理解するための参考記事です。</li><li><a href=https://yuhi-sa.github.io/posts/20260226_ensemble_learning/1/>アンサンブル学習の手法と比較</a> - 機械学習の別のアプローチであるアンサンブル手法を解説しています。</li><li><a href=https://yuhi-sa.github.io/posts/20260226_svm/1/>SVM（サポートベクターマシン）とカーネル法</a> - カーネルトリックによる非線形変換とAttentionの類似性について理解を深められます。</li><li><a href=https://yuhi-sa.github.io/posts/20260225_moving_average/1/>移動平均フィルタの種類と比較</a> - Attentionを固定重みフィルタと対比するための参考記事です。</li></ul><h2 id=参考文献>参考文献</h2><ul><li>Vaswani, A., et al. (2017). &ldquo;Attention Is All You Need.&rdquo; <em>NeurIPS 2017</em>.</li><li>PyTorch Documentation: <code>nn.MultiheadAttention</code>. <a href=https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html>https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a></li><li>Alammar, J. (2018). &ldquo;The Illustrated Transformer.&rdquo; <a href=https://jalammar.github.io/illustrated-transformer/>https://jalammar.github.io/illustrated-transformer/</a></li></ul></div><div class="ad-slot in-content my-3"><ins class=adsbygoogle style=display:block;text-align:center data-ad-layout=in-article data-ad-format=fluid data-ad-client=ca-pub-9558545098866170></ins><script>(adsbygoogle=window.adsbygoogle||[]).push({})</script></div><footer class=article-footer></footer><meta itemprop=wordCount content="869"><meta itemprop=url content="https://yuhi-sa.github.io/posts/20260228_self_attention/1/"></article><nav class="post-nav mt-5" aria-label="Post navigation"><a href=/posts/20260228_notch_filter/1/ class="post-nav__item post-nav__item--prev"><span class="post-nav__label text-muted"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>前の記事
</span><span class=post-nav__title>ノッチフィルタの設計とPython実装：電源ノイズ除去の実践</span></a></nav><section class="related-posts mt-5" aria-label="Related posts"><h2 class=related-posts__title>関連記事</h2><div class=related-posts__grid><article class=related-posts__item><h3 class=related-posts__item-title><a href=/posts/20260228_timeseries_anomaly/1/>時系列データの異常検知：統計的手法からカルマンフィルタまでPython実装</a></h3><div class=card-meta><time datetime=2026-02-28>February 28, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/posts/20260226_svm/1/>サポートベクターマシン（SVM）：カーネル法と非線形分類のPython実装</a></h3><div class=card-meta><time datetime=2026-02-26>February 26, 2026</time></div></article><article class=related-posts__item><h3 class=related-posts__item-title><a href=/posts/20260226_kmeans_gmm/1/>k-means法とGMM：クラスタリング手法の理論とPython実装</a></h3><div class=card-meta><time datetime=2026-02-26>February 26, 2026</time></div></article></div></section><nav class="article-navigation mt-4" aria-label="Article navigation"><a href=/posts/ class="btn btn-outline-secondary btn-sm mb-3"><i class="fas fa-arrow-left me-1" aria-hidden=true></i>
Back to posts</a><nav aria-label="Breadcrumb navigation" class=breadcrumb-nav role=navigation><ol class=breadcrumb itemscope itemtype=https://schema.org/BreadcrumbList><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=/ itemprop=item aria-label="Navigate to homepage"><i class="fas fa-home" aria-hidden=true></i>
<span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="1"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/ itemprop=item aria-label="Navigate to tomato blog"><span itemprop=name>tomato blog</span>
</a><meta itemprop=position content="2"></li><li class=breadcrumb-item itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><a href=https://yuhi-sa.github.io/posts/ itemprop=item aria-label="Navigate to Posts"><span itemprop=name>Posts</span>
</a><meta itemprop=position content="3"></li><li class="breadcrumb-item active" aria-current=page itemprop=itemListElement itemscope itemtype=https://schema.org/ListItem><span itemprop=name>Pythonで理解するSelf-Attentionの仕組み：数式からスクラッチ実装まで</span>
<meta itemprop=position content="4"></li></ol></nav></nav></div></div></div></main><footer role=contentinfo class=site-footer><div class="container pt-4 pb-3" style="border-top:2px solid var(--accent,#e54d2e)"><div class="row justify-content-center"><div class="col-md-8 text-center"><p class="copyright-text text-muted mb-2">&copy; 2026 yuhi-sa. All rights reserved.</p><p class="theme-attribution text-muted small mt-2 mb-0">Powered by
<a href=https://gohugo.io/ target=_blank rel=noopener class=text-decoration-none>Hugo</a>
with
<a href=https://github.com/yuhi-sa/tomatohugo target=_blank rel=noopener class=text-decoration-none>TomatoHugo</a></p></div></div></div></footer><script src=https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js integrity=sha384-YvpcrYf0tY3lHB60NNkmXc5s9fDVZLESaAA55NDzOxhy9GkcIdslK1eN7N6jIeHz crossorigin=anonymous defer></script><script src=/js/dark-mode.min.3e457dc8346f064bee795e6f9b73e1516dcd059e750c521fe4b445f9ea9a7821.js integrity="sha256-PkV9yDRvBkvueV5vm3PhUW3NBZ51DFIf5LRF+eqaeCE=" defer></script><script>window.addEventListener("load",function(){const e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-LN6QP6VVM3",document.head.appendChild(e),e.onload=function(){window.dataLayer=window.dataLayer||[];function e(){dataLayer.push(arguments)}e("js",new Date),e("config","G-LN6QP6VVM3")}})</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"WebPage","name":"Pythonで理解するSelf-Attentionの仕組み：数式からスクラッチ実装まで","url":"https:\/\/yuhi-sa.github.io\/posts\/20260228_self_attention\/1\/","description":"TransformerのコアであるSelf-Attention機構を数式から丁寧に解説し、NumPyによるスクラッチ実装とPyTorchのnn.MultiheadAttentionとの比較を通じて仕組みを理解します。","inLanguage":"ja","isPartOf":{"@type":"WebSite","name":"tomato blog","url":"https:\/\/yuhi-sa.github.io\/"}}</script></body></html>